@misc{moummad2024selfsupervisedlearningfewshotbird,
      title={Self-Supervised Learning for Few-Shot Bird Sound Classification}, 
      author={Ilyass Moummad and Romain Serizel and Nicolas Farrugia},
      year={2024},
      eprint={2312.15824},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2312.15824}, 
}

@article{TRIPATHI2021108183,
title = {Self-supervised learning for Environmental Sound Classification},
journal = {Applied Acoustics},
volume = {182},
pages = {108183},
year = {2021},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2021.108183},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X21002772},
author = {Achyut Mani Tripathi and Aakansha Mishra},
keywords = {Environmental Sound Classification, Data Augmentation, Residual Network, Self-Supervised Learning, Sound, Transfer Learning},
abstract = {Environmental Sound Classification (ESC) is one of the most challenging tasks in signal processing, digital forensic and machine learning. Numerous methods have been proposed to perform ESC. The conventional models’ training depends on an enormous amount of annotated data, specifically while training the deep models. This paper presents a self-supervised learning (SSL)-based deep classifier for ESC, which is an under-explored method in the field of ESC. SSL mechanism directs the model to effectively learn prototypical features from the data itself by solving a pretext task. The model proposed in this paper takes spectrogram images as input. A pretext or an auxiliary task is defined as identification of the type of data augmentation applied to the signal. The model learned by solving the pretext task is further fine-tuned for developing the deep model for ESC. The model’s performance is evaluated on two benchmark sound classification datasets, i.e. ESC-10 and DCASE 2019 Task-1(A) datasets. The experiments and results show that the SSL model attains an improvement of 12.59% and 11.17% in accuracy compared to the baseline models of the DCASE 2019 Task-1(A) and ESC-10 datasets respectively. Moreover, the model also shows competitive performance to state-of-the-art methods.}
}

@misc{wilkinghoff2023selfsupervisedlearninganomaloussound,
      title={Self-Supervised Learning for Anomalous Sound Detection}, 
      author={Kevin Wilkinghoff},
      year={2023},
      eprint={2312.09578},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2312.09578}, 
}

@misc{abdoli2019endtoendenvironmentalsoundclassification,
      title={End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network}, 
      author={Sajjad Abdoli and Patrick Cardinal and Alessandro Lameiras Koerich},
      year={2019},
      eprint={1904.08990},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1904.08990}, 
}

@misc{guzhov2021audioclipextendingclipimage,
      title={AudioCLIP: Extending CLIP to Image, Text and Audio}, 
      author={Andrey Guzhov and Federico Raue and Jörn Hees and Andreas Dengel},
      year={2021},
      eprint={2106.13043},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2106.13043}, 
}

@inproceedings{Bae_2023, series={interspeech_2023},
   title={Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification},
   url={http://dx.doi.org/10.21437/Interspeech.2023-1426},
   DOI={10.21437/interspeech.2023-1426},
   booktitle={INTERSPEECH 2023},
   publisher={ISCA},
   author={Bae, Sangmin and Kim, June-Woo and Cho, Won-Yang and Baek, Hyerim and Son, Soyoun and Lee, Byungjo and Ha, Changwan and Tae, Kyongpil and Kim, Sungnyun and Yun, Se-Young},
   year={2023},
   month=aug, collection={interspeech_2023} }

@article{CHEN2025110593,
title = {Evaluating metric and contrastive learning in pretrained models for environmental sound classification},
journal = {Applied Acoustics},
volume = {232},
pages = {110593},
year = {2025},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2025.110593},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X25000659},
author = {Feilong Chen and Zhenjun Zhu and Chengli Sun and Linqing Xia},
keywords = {Environmental sound classification, Metric learning, Contrastive learning, Lightweight pretrained model, Unmanned aerial vehicle},
abstract = {Environmental Sound Classification (ESC) has advanced significantly with the advent of deep learning techniques. This study conducts a comprehensive evaluation of contrastive and metric learning approaches in ESC, introducing the ESC51 dataset, an extension of the ESC50 benchmark that incorporates noise samples from quadrotor Unmanned Aerial Vehicles (UAVs). To enhance classification performance and the discriminative power of embedding spaces, we propose a novel metric learning-based approach, SoundMLR, which employs a hybrid loss function emphasizing metric learning principles. Experimental results demonstrate that SoundMLR consistently outperforms contrastive learning methods in terms of classification accuracy and inference latency, particularly when applied to the lightweight MobileNetV2 pretrained model across ESC50, ESC51, and UrbanSound8K (US8K) datasets. Analyses of confusion matrices and t-SNE visualizations further highlight SoundMLR’s ability to generate compact, distinct feature clusters, enabling more robust discrimination between sound classes. Additionally, we introduce two innovative modules, Spectral Pooling Attention (SPA) and the Feature Pooling Layer (FPL), designed to optimize the MobileNetV2 backbone. Notably, the MobileNetV2 + FPL model, equipped with SoundMLR, achieves an impressive 92.16 % classification accuracy on the ESC51 dataset while reducing computational complexity by 24.5 %. Similarly, the MobileNetV2 + SPA model achieves a peak accuracy of 91.75 % on the ESC50 dataset, showcasing the complementary strengths of these modules. These findings offer valuable insights for the future development of efficient, scalable, and robust ESC systems. The source code for this study is publicly available at https://github.com/flchenwhu/ESC-SoundMLR.}
}


@Article{app14219711,
AUTHOR = {Chen, Xu and Wang, Mei and Kan, Ruixiang and Qiu, Hongbing},
TITLE = {Improved Patch-Mix Transformer and Contrastive Learning Method for Sound Classification in Noisy Environments},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {21},
ARTICLE-NUMBER = {9711},
URL = {https://www.mdpi.com/2076-3417/14/21/9711},
ISSN = {2076-3417},
ABSTRACT = {In urban environments, noise significantly impacts daily life and presents challenges for Environmental Sound Classification (ESC). The structural influence of urban noise on audio signals complicates feature extraction and audio classification for environmental sound classification methods. To address these challenges, this paper proposes a Contrastive Learning-based Audio Spectrogram Transformer (CL-Transformer) that incorporates a Patch-Mix mechanism and adaptive contrastive learning strategies while simultaneously improving and utilizing adaptive data augmentation techniques for model training. Firstly, a combination of data augmentation techniques is introduced to enrich environmental sounds. Then, the Patch-Mix feature fusion scheme randomly mixes patches of the enhanced and noisy spectrograms during the Transformer’s patch embedding. Furthermore, a novel contrastive learning scheme is introduced to quantify loss and improve model performance, synergizing well with the Transformer model. Finally, experiments on the ESC-50 and UrbanSound8K public datasets achieved accuracies of 97.75% and 92.95%, respectively. To simulate the impact of noise in real urban environments, the model is evaluated using the UrbanSound8K dataset with added background noise at different signal-to-noise ratios (SNR). Experimental results demonstrate that the proposed framework performs well in noisy environments.},
DOI = {10.3390/app14219711}
}

@article{Venkatesh_2022,
   title={You Only Hear Once: A YOLO-like Algorithm for Audio Segmentation and Sound Event Detection},
   volume={12},
   ISSN={2076-3417},
   url={http://dx.doi.org/10.3390/app12073293},
   DOI={10.3390/app12073293},
   number={7},
   journal={Applied Sciences},
   publisher={MDPI AG},
   author={Venkatesh, Satvik and Moffat, David and Miranda, Eduardo Reck},
   year={2022},
   month=mar, pages={3293} }

@misc{vu2024endtoendinterpretableconvolutionalneural,
      title={Toward end-to-end interpretable convolutional neural networks for waveform signals}, 
      author={Linh Vu and Thu Tran and Wern-Han Lim and Raphael Phan},
      year={2024},
      eprint={2405.01815},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2405.01815}, 
}

@misc{kadandale2020multichannelunetmusicsource,
      title={Multi-channel U-Net for Music Source Separation}, 
      author={Venkatesh S. Kadandale and Juan F. Montesinos and Gloria Haro and Emilia Gómez},
      year={2020},
      eprint={2003.10414},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2003.10414}, 
}

@INPROCEEDINGS{8995448,
  author={Nasiri, Alireza and Cui, Yuxin and Liu, Zhonghao and Jin, Jing and Zhao, Yong and Hu, Jianjun},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={AudioMask: Robust Sound Event Detection Using Mask R-CNN and Frame-Level Classifier}, 
  year={2019},
  volume={},
  number={},
  pages={485-492},
  keywords={Sound Event Detection;Mask R-CNN;Audio Analysis;Audio Classifier},
  doi={10.1109/ICTAI.2019.00074}}

@article{https://doi.org/10.1049/cit2.12007,
author = {Ntalampiras, Stavros and Potamitis, Ilyas},
title = {Acoustic detection of unknown bird species and individuals},
journal = {CAAI Transactions on Intelligence Technology},
volume = {6},
number = {3},
pages = {291-300},
keywords = {acoustic signal detection, acoustic signal processing, bioacoustics, decoding, probability, convolutional neural nets},
doi = {https://doi.org/10.1049/cit2.12007},
url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12007},
eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12007},
abstract = {Abstract Computational bioacoustics is a relatively young research area, yet it has increasingly received attention over the last decade because it can be used in a wide range of applications in a cost-effective manner. This work focuses on the problem of detecting the novel bird calls and songs associated with various species and individual birds. To this end, variational autoencoders, consisting of deep encoding–decoding networks, are employed. The encoder encompasses a series of convolutional layers leading to a smooth high-level abstraction of log-Mel spectrograms that characterise bird vocalisations. The decoder operates on this latent representation to generate each respective original observation. Novel species/individual detection is carried out by monitoring and thresholding the expected reconstruction probability. We thoroughly evaluate the proposed method on two different data sets, including the vocalisations of 11 North American bird species and 16 Athene noctua individuals.},
year = {2021}
}

@misc{zhao2024universalsoundseparationselfsupervised,
      title={Universal Sound Separation with Self-Supervised Audio Masked Autoencoder}, 
      author={Junqi Zhao and Xubo Liu and Jinzheng Zhao and Yi Yuan and Qiuqiang Kong and Mark D. Plumbley and Wenwu Wang},
      year={2024},
      eprint={2407.11745},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2407.11745}, 
}

