{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a7d0774-b070-4b9c-a7ec-d42beec76056",
      "metadata": {
        "id": "9a7d0774-b070-4b9c-a7ec-d42beec76056"
      },
      "source": [
        "# Enhanced SimCLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoRtkQmGmLs7",
        "outputId": "9291188c-694c-40da-a24a-b40f631540ad"
      },
      "id": "aoRtkQmGmLs7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ec326472-5abf-4e6f-8303-e0e1eea36e67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ec326472-5abf-4e6f-8303-e0e1eea36e67",
        "outputId": "ef91cc12-2737-4fec-800a-58e8e2e9018b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "=== Fold 6/10 ====================\n",
            "========================================\n",
            "\n",
            "\n",
            "========================================\n",
            "=== Fold 7/10 ====================\n",
            "========================================\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "  Batch 010/898 | Loss: 0.6353 | CLoss: 0.4222 | FLoss: 0.4263 | LR: 3.00e-04\n",
            "  Batch 020/898 | Loss: 0.6061 | CLoss: 0.4200 | FLoss: 0.3723 | LR: 3.00e-04\n",
            "  Batch 030/898 | Loss: 1.0534 | CLoss: 0.8360 | FLoss: 0.4349 | LR: 3.00e-04\n",
            "  Batch 040/898 | Loss: 1.4819 | CLoss: 1.2143 | FLoss: 0.5351 | LR: 3.00e-04\n",
            "  Batch 050/898 | Loss: 1.1093 | CLoss: 0.8156 | FLoss: 0.5874 | LR: 3.00e-04\n",
            "  Batch 060/898 | Loss: 0.5918 | CLoss: 0.4491 | FLoss: 0.2854 | LR: 3.00e-04\n",
            "  Batch 070/898 | Loss: 0.9009 | CLoss: 0.6663 | FLoss: 0.4692 | LR: 3.00e-04\n",
            "  Batch 080/898 | Loss: 1.0772 | CLoss: 0.7979 | FLoss: 0.5586 | LR: 3.00e-04\n",
            "  Batch 090/898 | Loss: 0.7491 | CLoss: 0.6050 | FLoss: 0.2882 | LR: 3.00e-04\n",
            "  Batch 100/898 | Loss: 0.8057 | CLoss: 0.6129 | FLoss: 0.3856 | LR: 3.00e-04\n",
            "  Batch 110/898 | Loss: 0.9331 | CLoss: 0.7025 | FLoss: 0.4611 | LR: 3.00e-04\n",
            "  Batch 120/898 | Loss: 0.6231 | CLoss: 0.4802 | FLoss: 0.2857 | LR: 3.00e-04\n",
            "  Batch 130/898 | Loss: 0.6591 | CLoss: 0.4919 | FLoss: 0.3344 | LR: 3.00e-04\n",
            "  Batch 140/898 | Loss: 0.9641 | CLoss: 0.7407 | FLoss: 0.4467 | LR: 3.00e-04\n",
            "  Batch 150/898 | Loss: 1.0106 | CLoss: 0.7968 | FLoss: 0.4275 | LR: 3.00e-04\n",
            "  Batch 160/898 | Loss: 0.9148 | CLoss: 0.7012 | FLoss: 0.4271 | LR: 3.00e-04\n",
            "  Batch 170/898 | Loss: 0.8131 | CLoss: 0.5908 | FLoss: 0.4445 | LR: 3.00e-04\n",
            "  Batch 180/898 | Loss: 0.8227 | CLoss: 0.5364 | FLoss: 0.5727 | LR: 3.00e-04\n",
            "  Batch 190/898 | Loss: 1.1165 | CLoss: 0.9078 | FLoss: 0.4174 | LR: 3.00e-04\n",
            "  Batch 200/898 | Loss: 1.0605 | CLoss: 0.7799 | FLoss: 0.5611 | LR: 3.00e-04\n",
            "  Batch 210/898 | Loss: 0.7526 | CLoss: 0.5176 | FLoss: 0.4700 | LR: 3.00e-04\n",
            "  Batch 220/898 | Loss: 1.0127 | CLoss: 0.7673 | FLoss: 0.4909 | LR: 3.00e-04\n",
            "  Batch 230/898 | Loss: 0.8239 | CLoss: 0.6815 | FLoss: 0.2849 | LR: 3.00e-04\n",
            "  Batch 240/898 | Loss: 0.7793 | CLoss: 0.6298 | FLoss: 0.2990 | LR: 3.00e-04\n",
            "  Batch 250/898 | Loss: 1.1768 | CLoss: 0.8812 | FLoss: 0.5913 | LR: 3.00e-04\n",
            "  Batch 260/898 | Loss: 0.8446 | CLoss: 0.5795 | FLoss: 0.5302 | LR: 3.00e-04\n",
            "  Batch 270/898 | Loss: 1.1938 | CLoss: 0.9318 | FLoss: 0.5241 | LR: 3.00e-04\n",
            "  Batch 280/898 | Loss: 1.0617 | CLoss: 0.7298 | FLoss: 0.6639 | LR: 3.00e-04\n",
            "  Batch 290/898 | Loss: 0.6619 | CLoss: 0.4520 | FLoss: 0.4198 | LR: 3.00e-04\n",
            "  Batch 300/898 | Loss: 0.9714 | CLoss: 0.7713 | FLoss: 0.4002 | LR: 3.00e-04\n",
            "  Batch 310/898 | Loss: 1.0225 | CLoss: 0.8436 | FLoss: 0.3578 | LR: 3.00e-04\n",
            "  Batch 320/898 | Loss: 0.7857 | CLoss: 0.6169 | FLoss: 0.3376 | LR: 3.00e-04\n",
            "  Batch 330/898 | Loss: 0.7266 | CLoss: 0.6302 | FLoss: 0.1927 | LR: 3.00e-04\n",
            "  Batch 340/898 | Loss: 1.0168 | CLoss: 0.8668 | FLoss: 0.3000 | LR: 3.00e-04\n",
            "  Batch 350/898 | Loss: 1.1987 | CLoss: 0.9389 | FLoss: 0.5196 | LR: 3.00e-04\n",
            "  Batch 360/898 | Loss: 0.9399 | CLoss: 0.7388 | FLoss: 0.4022 | LR: 3.00e-04\n",
            "  Batch 370/898 | Loss: 1.0892 | CLoss: 0.7355 | FLoss: 0.7074 | LR: 3.00e-04\n",
            "  Batch 380/898 | Loss: 0.5940 | CLoss: 0.3708 | FLoss: 0.4466 | LR: 3.00e-04\n",
            "  Batch 390/898 | Loss: 0.9746 | CLoss: 0.7402 | FLoss: 0.4687 | LR: 3.00e-04\n",
            "  Batch 400/898 | Loss: 1.0083 | CLoss: 0.7573 | FLoss: 0.5020 | LR: 3.00e-04\n",
            "  Batch 410/898 | Loss: 0.7831 | CLoss: 0.5925 | FLoss: 0.3811 | LR: 3.00e-04\n",
            "  Batch 420/898 | Loss: 1.1668 | CLoss: 0.8726 | FLoss: 0.5884 | LR: 3.00e-04\n",
            "  Batch 430/898 | Loss: 0.7006 | CLoss: 0.4945 | FLoss: 0.4123 | LR: 3.00e-04\n",
            "  Batch 440/898 | Loss: 0.9625 | CLoss: 0.7329 | FLoss: 0.4593 | LR: 3.00e-04\n",
            "  Batch 450/898 | Loss: 1.1438 | CLoss: 0.9487 | FLoss: 0.3902 | LR: 3.00e-04\n",
            "  Batch 460/898 | Loss: 0.4732 | CLoss: 0.3336 | FLoss: 0.2793 | LR: 3.00e-04\n",
            "  Batch 470/898 | Loss: 0.7540 | CLoss: 0.5516 | FLoss: 0.4047 | LR: 3.00e-04\n",
            "  Batch 480/898 | Loss: 1.0549 | CLoss: 0.8599 | FLoss: 0.3901 | LR: 3.00e-04\n",
            "  Batch 490/898 | Loss: 1.2566 | CLoss: 0.9015 | FLoss: 0.7103 | LR: 3.00e-04\n",
            "  Batch 500/898 | Loss: 1.1839 | CLoss: 1.0153 | FLoss: 0.3371 | LR: 3.00e-04\n",
            "  Batch 510/898 | Loss: 0.8980 | CLoss: 0.6490 | FLoss: 0.4979 | LR: 3.00e-04\n",
            "  Batch 520/898 | Loss: 0.6760 | CLoss: 0.5331 | FLoss: 0.2857 | LR: 3.00e-04\n",
            "  Batch 530/898 | Loss: 1.1050 | CLoss: 0.8216 | FLoss: 0.5667 | LR: 3.00e-04\n",
            "  Batch 540/898 | Loss: 0.5144 | CLoss: 0.3452 | FLoss: 0.3383 | LR: 3.00e-04\n",
            "  Batch 550/898 | Loss: 0.5860 | CLoss: 0.4121 | FLoss: 0.3477 | LR: 3.00e-04\n",
            "  Batch 560/898 | Loss: 1.0317 | CLoss: 0.7791 | FLoss: 0.5052 | LR: 3.00e-04\n",
            "  Batch 570/898 | Loss: 0.5909 | CLoss: 0.3986 | FLoss: 0.3844 | LR: 3.00e-04\n",
            "  Batch 580/898 | Loss: 0.6658 | CLoss: 0.4940 | FLoss: 0.3435 | LR: 3.00e-04\n",
            "  Batch 590/898 | Loss: 0.8398 | CLoss: 0.5659 | FLoss: 0.5479 | LR: 3.00e-04\n",
            "  Batch 600/898 | Loss: 0.6810 | CLoss: 0.5854 | FLoss: 0.1914 | LR: 3.00e-04\n",
            "  Batch 610/898 | Loss: 0.9152 | CLoss: 0.7051 | FLoss: 0.4202 | LR: 3.00e-04\n",
            "  Batch 620/898 | Loss: 1.1189 | CLoss: 0.8463 | FLoss: 0.5452 | LR: 3.00e-04\n",
            "  Batch 630/898 | Loss: 0.6143 | CLoss: 0.4890 | FLoss: 0.2504 | LR: 3.00e-04\n",
            "  Batch 640/898 | Loss: 0.7767 | CLoss: 0.5959 | FLoss: 0.3616 | LR: 3.00e-04\n",
            "  Batch 650/898 | Loss: 0.8709 | CLoss: 0.6496 | FLoss: 0.4427 | LR: 3.00e-04\n",
            "  Batch 660/898 | Loss: 0.7377 | CLoss: 0.5656 | FLoss: 0.3442 | LR: 3.00e-04\n",
            "  Batch 670/898 | Loss: 0.7366 | CLoss: 0.5503 | FLoss: 0.3725 | LR: 3.00e-04\n",
            "  Batch 680/898 | Loss: 1.1697 | CLoss: 0.9409 | FLoss: 0.4577 | LR: 3.00e-04\n",
            "  Batch 690/898 | Loss: 0.5867 | CLoss: 0.4467 | FLoss: 0.2800 | LR: 3.00e-04\n",
            "  Batch 700/898 | Loss: 0.3857 | CLoss: 0.2662 | FLoss: 0.2390 | LR: 3.00e-04\n",
            "  Batch 710/898 | Loss: 1.3235 | CLoss: 1.0222 | FLoss: 0.6026 | LR: 3.00e-04\n",
            "  Batch 720/898 | Loss: 0.6902 | CLoss: 0.5211 | FLoss: 0.3383 | LR: 3.00e-04\n",
            "  Batch 730/898 | Loss: 1.2794 | CLoss: 1.0499 | FLoss: 0.4589 | LR: 3.00e-04\n",
            "  Batch 740/898 | Loss: 0.4304 | CLoss: 0.1818 | FLoss: 0.4974 | LR: 3.00e-04\n",
            "  Batch 750/898 | Loss: 0.7772 | CLoss: 0.4661 | FLoss: 0.6221 | LR: 3.00e-04\n",
            "  Batch 760/898 | Loss: 0.9770 | CLoss: 0.6916 | FLoss: 0.5706 | LR: 3.00e-04\n",
            "  Batch 770/898 | Loss: 0.7119 | CLoss: 0.4703 | FLoss: 0.4831 | LR: 3.00e-04\n",
            "  Batch 780/898 | Loss: 1.0232 | CLoss: 0.8003 | FLoss: 0.4459 | LR: 3.00e-04\n",
            "  Batch 790/898 | Loss: 1.0497 | CLoss: 0.7847 | FLoss: 0.5301 | LR: 3.00e-04\n",
            "  Batch 800/898 | Loss: 1.0413 | CLoss: 0.8185 | FLoss: 0.4456 | LR: 3.00e-04\n",
            "  Batch 810/898 | Loss: 1.2176 | CLoss: 0.9191 | FLoss: 0.5969 | LR: 3.00e-04\n",
            "  Batch 820/898 | Loss: 0.6303 | CLoss: 0.4701 | FLoss: 0.3204 | LR: 3.00e-04\n",
            "  Batch 830/898 | Loss: 0.9577 | CLoss: 0.8178 | FLoss: 0.2798 | LR: 3.00e-04\n",
            "  Batch 840/898 | Loss: 0.6267 | CLoss: 0.4674 | FLoss: 0.3185 | LR: 3.00e-04\n",
            "  Batch 850/898 | Loss: 1.4290 | CLoss: 1.0867 | FLoss: 0.6846 | LR: 3.00e-04\n",
            "  Batch 860/898 | Loss: 0.9674 | CLoss: 0.7341 | FLoss: 0.4665 | LR: 3.00e-04\n",
            "  Batch 870/898 | Loss: 0.9792 | CLoss: 0.8333 | FLoss: 0.2918 | LR: 3.00e-04\n",
            "  Batch 880/898 | Loss: 1.1611 | CLoss: 0.8091 | FLoss: 0.7041 | LR: 3.00e-04\n",
            "  Batch 890/898 | Loss: 0.6278 | CLoss: 0.4361 | FLoss: 0.3835 | LR: 3.00e-04\n",
            "  Batch 898/898 | Loss: 0.1299 | CLoss: 0.0001 | FLoss: 0.2597 | LR: 3.00e-04\n",
            "\n",
            "  Training Summary | Epoch 1\n",
            "  Avg Loss: 0.8868\n",
            "  Last Batch Loss: 0.1299\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/102 | Loss: 0.9276 | Batch Acc: 65.52%\n",
            "    Val Batch 010/102 | Loss: 0.6571 | Batch Acc: 74.14%\n",
            "    Val Batch 015/102 | Loss: 0.6118 | Batch Acc: 81.03%\n",
            "    Val Batch 020/102 | Loss: 0.5150 | Batch Acc: 93.10%\n",
            "    Val Batch 025/102 | Loss: 0.2103 | Batch Acc: 89.66%\n",
            "    Val Batch 030/102 | Loss: 0.0811 | Batch Acc: 96.55%\n",
            "    Val Batch 035/102 | Loss: 0.0947 | Batch Acc: 96.55%\n",
            "    Val Batch 040/102 | Loss: 0.6422 | Batch Acc: 72.41%\n",
            "    Val Batch 045/102 | Loss: 1.0372 | Batch Acc: 25.86%\n",
            "    Val Batch 050/102 | Loss: 0.3732 | Batch Acc: 86.21%\n",
            "    Val Batch 055/102 | Loss: 0.5145 | Batch Acc: 84.48%\n",
            "    Val Batch 060/102 | Loss: 1.1049 | Batch Acc: 70.69%\n",
            "    Val Batch 065/102 | Loss: 0.3168 | Batch Acc: 87.93%\n",
            "    Val Batch 070/102 | Loss: 0.3248 | Batch Acc: 86.21%\n",
            "    Val Batch 075/102 | Loss: 0.2490 | Batch Acc: 91.38%\n",
            "    Val Batch 080/102 | Loss: 0.3787 | Batch Acc: 84.48%\n",
            "    Val Batch 085/102 | Loss: 0.3446 | Batch Acc: 91.38%\n",
            "    Val Batch 090/102 | Loss: 0.3006 | Batch Acc: 93.10%\n",
            "    Val Batch 095/102 | Loss: 0.1857 | Batch Acc: 94.83%\n",
            "    Val Batch 100/102 | Loss: 0.3481 | Batch Acc: 93.10%\n",
            "    Val Batch 102/102 | Loss: 0.3821 | Batch Acc: 93.10%\n",
            "\n",
            "  Validation Summary | Epoch 1\n",
            "  Avg Loss: 0.4528 | Accuracy: 83.73%\n",
            "  Current Best Acc: 83.73%\n",
            "\n",
            "Epoch 2/5\n",
            "  Batch 010/898 | Loss: 1.0891 | CLoss: 0.9103 | FLoss: 0.3576 | LR: 2.71e-04\n",
            "  Batch 020/898 | Loss: 0.6460 | CLoss: 0.4365 | FLoss: 0.4191 | LR: 2.71e-04\n",
            "  Batch 030/898 | Loss: 0.7431 | CLoss: 0.5839 | FLoss: 0.3186 | LR: 2.71e-04\n",
            "  Batch 040/898 | Loss: 0.6823 | CLoss: 0.5229 | FLoss: 0.3187 | LR: 2.71e-04\n",
            "  Batch 050/898 | Loss: 1.0523 | CLoss: 0.8586 | FLoss: 0.3874 | LR: 2.71e-04\n",
            "  Batch 060/898 | Loss: 0.7399 | CLoss: 0.6422 | FLoss: 0.1956 | LR: 2.71e-04\n",
            "  Batch 070/898 | Loss: 1.1012 | CLoss: 0.9069 | FLoss: 0.3886 | LR: 2.71e-04\n",
            "  Batch 080/898 | Loss: 0.7299 | CLoss: 0.5676 | FLoss: 0.3246 | LR: 2.71e-04\n",
            "  Batch 090/898 | Loss: 0.9705 | CLoss: 0.7888 | FLoss: 0.3634 | LR: 2.71e-04\n",
            "  Batch 100/898 | Loss: 1.2024 | CLoss: 0.9747 | FLoss: 0.4554 | LR: 2.71e-04\n",
            "  Batch 110/898 | Loss: 0.9761 | CLoss: 0.7357 | FLoss: 0.4810 | LR: 2.71e-04\n",
            "  Batch 120/898 | Loss: 0.8658 | CLoss: 0.6666 | FLoss: 0.3985 | LR: 2.71e-04\n",
            "  Batch 130/898 | Loss: 1.1430 | CLoss: 0.9082 | FLoss: 0.4696 | LR: 2.71e-04\n",
            "  Batch 140/898 | Loss: 0.6864 | CLoss: 0.5310 | FLoss: 0.3107 | LR: 2.71e-04\n",
            "  Batch 150/898 | Loss: 0.8692 | CLoss: 0.7306 | FLoss: 0.2772 | LR: 2.71e-04\n",
            "  Batch 160/898 | Loss: 1.2283 | CLoss: 0.8933 | FLoss: 0.6699 | LR: 2.71e-04\n",
            "  Batch 170/898 | Loss: 0.8391 | CLoss: 0.6934 | FLoss: 0.2913 | LR: 2.71e-04\n",
            "  Batch 180/898 | Loss: 1.0105 | CLoss: 0.7227 | FLoss: 0.5756 | LR: 2.71e-04\n",
            "  Batch 190/898 | Loss: 0.5764 | CLoss: 0.4476 | FLoss: 0.2576 | LR: 2.71e-04\n",
            "  Batch 200/898 | Loss: 0.7739 | CLoss: 0.6533 | FLoss: 0.2412 | LR: 2.71e-04\n",
            "  Batch 210/898 | Loss: 0.9326 | CLoss: 0.7463 | FLoss: 0.3726 | LR: 2.71e-04\n",
            "  Batch 220/898 | Loss: 1.0300 | CLoss: 0.7961 | FLoss: 0.4679 | LR: 2.71e-04\n",
            "  Batch 230/898 | Loss: 0.7010 | CLoss: 0.5520 | FLoss: 0.2980 | LR: 2.71e-04\n",
            "  Batch 240/898 | Loss: 0.9395 | CLoss: 0.6621 | FLoss: 0.5548 | LR: 2.71e-04\n",
            "  Batch 250/898 | Loss: 1.1113 | CLoss: 0.8729 | FLoss: 0.4767 | LR: 2.71e-04\n",
            "  Batch 260/898 | Loss: 0.9469 | CLoss: 0.7515 | FLoss: 0.3909 | LR: 2.71e-04\n",
            "  Batch 270/898 | Loss: 0.6778 | CLoss: 0.5160 | FLoss: 0.3237 | LR: 2.71e-04\n",
            "  Batch 280/898 | Loss: 1.2794 | CLoss: 1.0182 | FLoss: 0.5224 | LR: 2.71e-04\n",
            "  Batch 290/898 | Loss: 0.9945 | CLoss: 0.8550 | FLoss: 0.2790 | LR: 2.71e-04\n",
            "  Batch 300/898 | Loss: 1.1175 | CLoss: 0.8489 | FLoss: 0.5373 | LR: 2.71e-04\n",
            "  Batch 310/898 | Loss: 0.9557 | CLoss: 0.6405 | FLoss: 0.6305 | LR: 2.71e-04\n",
            "  Batch 320/898 | Loss: 0.6023 | CLoss: 0.4332 | FLoss: 0.3384 | LR: 2.71e-04\n",
            "  Batch 330/898 | Loss: 0.6825 | CLoss: 0.4907 | FLoss: 0.3836 | LR: 2.71e-04\n",
            "  Batch 340/898 | Loss: 1.4898 | CLoss: 1.2334 | FLoss: 0.5128 | LR: 2.71e-04\n",
            "  Batch 350/898 | Loss: 0.8069 | CLoss: 0.6407 | FLoss: 0.3324 | LR: 2.71e-04\n",
            "  Batch 360/898 | Loss: 0.4426 | CLoss: 0.3607 | FLoss: 0.1638 | LR: 2.71e-04\n",
            "  Batch 370/898 | Loss: 0.6002 | CLoss: 0.4403 | FLoss: 0.3198 | LR: 2.71e-04\n",
            "  Batch 380/898 | Loss: 1.0109 | CLoss: 0.7816 | FLoss: 0.4587 | LR: 2.71e-04\n",
            "  Batch 390/898 | Loss: 0.9086 | CLoss: 0.6913 | FLoss: 0.4346 | LR: 2.71e-04\n",
            "  Batch 400/898 | Loss: 0.7447 | CLoss: 0.5666 | FLoss: 0.3563 | LR: 2.71e-04\n",
            "  Batch 410/898 | Loss: 1.2360 | CLoss: 0.9313 | FLoss: 0.6092 | LR: 2.71e-04\n",
            "  Batch 420/898 | Loss: 0.9415 | CLoss: 0.7707 | FLoss: 0.3416 | LR: 2.71e-04\n",
            "  Batch 430/898 | Loss: 0.7484 | CLoss: 0.5488 | FLoss: 0.3993 | LR: 2.71e-04\n",
            "  Batch 440/898 | Loss: 1.1029 | CLoss: 0.9315 | FLoss: 0.3427 | LR: 2.71e-04\n",
            "  Batch 450/898 | Loss: 0.7929 | CLoss: 0.5730 | FLoss: 0.4398 | LR: 2.71e-04\n",
            "  Batch 460/898 | Loss: 0.8936 | CLoss: 0.7065 | FLoss: 0.3741 | LR: 2.71e-04\n",
            "  Batch 470/898 | Loss: 0.8777 | CLoss: 0.6467 | FLoss: 0.4620 | LR: 2.71e-04\n",
            "  Batch 480/898 | Loss: 1.0916 | CLoss: 0.9113 | FLoss: 0.3605 | LR: 2.71e-04\n",
            "  Batch 490/898 | Loss: 0.8356 | CLoss: 0.6739 | FLoss: 0.3235 | LR: 2.71e-04\n",
            "  Batch 500/898 | Loss: 0.5912 | CLoss: 0.3888 | FLoss: 0.4048 | LR: 2.71e-04\n",
            "  Batch 510/898 | Loss: 0.8252 | CLoss: 0.6702 | FLoss: 0.3102 | LR: 2.71e-04\n",
            "  Batch 520/898 | Loss: 0.9957 | CLoss: 0.6889 | FLoss: 0.6137 | LR: 2.71e-04\n",
            "  Batch 530/898 | Loss: 0.5430 | CLoss: 0.3867 | FLoss: 0.3126 | LR: 2.71e-04\n",
            "  Batch 540/898 | Loss: 0.7172 | CLoss: 0.5991 | FLoss: 0.2361 | LR: 2.71e-04\n",
            "  Batch 550/898 | Loss: 0.5631 | CLoss: 0.3952 | FLoss: 0.3359 | LR: 2.71e-04\n",
            "  Batch 560/898 | Loss: 0.7420 | CLoss: 0.5331 | FLoss: 0.4178 | LR: 2.71e-04\n",
            "  Batch 570/898 | Loss: 0.6893 | CLoss: 0.5128 | FLoss: 0.3531 | LR: 2.71e-04\n",
            "  Batch 580/898 | Loss: 0.6897 | CLoss: 0.5164 | FLoss: 0.3467 | LR: 2.71e-04\n",
            "  Batch 590/898 | Loss: 0.7708 | CLoss: 0.6109 | FLoss: 0.3196 | LR: 2.71e-04\n",
            "  Batch 600/898 | Loss: 0.7212 | CLoss: 0.5670 | FLoss: 0.3083 | LR: 2.71e-04\n",
            "  Batch 610/898 | Loss: 0.6342 | CLoss: 0.4859 | FLoss: 0.2966 | LR: 2.71e-04\n",
            "  Batch 620/898 | Loss: 0.8214 | CLoss: 0.5605 | FLoss: 0.5218 | LR: 2.71e-04\n",
            "  Batch 630/898 | Loss: 0.6820 | CLoss: 0.5373 | FLoss: 0.2893 | LR: 2.71e-04\n",
            "  Batch 640/898 | Loss: 0.9165 | CLoss: 0.7313 | FLoss: 0.3702 | LR: 2.71e-04\n",
            "  Batch 650/898 | Loss: 0.6487 | CLoss: 0.4842 | FLoss: 0.3289 | LR: 2.71e-04\n",
            "  Batch 660/898 | Loss: 0.9184 | CLoss: 0.6838 | FLoss: 0.4693 | LR: 2.71e-04\n",
            "  Batch 670/898 | Loss: 0.7951 | CLoss: 0.5415 | FLoss: 0.5071 | LR: 2.71e-04\n",
            "  Batch 680/898 | Loss: 0.4924 | CLoss: 0.3914 | FLoss: 0.2019 | LR: 2.71e-04\n",
            "  Batch 690/898 | Loss: 1.0399 | CLoss: 0.8294 | FLoss: 0.4211 | LR: 2.71e-04\n",
            "  Batch 700/898 | Loss: 1.0177 | CLoss: 0.7353 | FLoss: 0.5648 | LR: 2.71e-04\n",
            "  Batch 710/898 | Loss: 0.6880 | CLoss: 0.5233 | FLoss: 0.3294 | LR: 2.71e-04\n",
            "  Batch 720/898 | Loss: 0.5544 | CLoss: 0.4036 | FLoss: 0.3016 | LR: 2.71e-04\n",
            "  Batch 730/898 | Loss: 1.2935 | CLoss: 1.0443 | FLoss: 0.4985 | LR: 2.71e-04\n",
            "  Batch 740/898 | Loss: 0.6632 | CLoss: 0.5373 | FLoss: 0.2518 | LR: 2.71e-04\n",
            "  Batch 750/898 | Loss: 0.5537 | CLoss: 0.4481 | FLoss: 0.2112 | LR: 2.71e-04\n",
            "  Batch 760/898 | Loss: 0.4945 | CLoss: 0.3845 | FLoss: 0.2200 | LR: 2.71e-04\n",
            "  Batch 770/898 | Loss: 0.6839 | CLoss: 0.5581 | FLoss: 0.2515 | LR: 2.71e-04\n",
            "  Batch 780/898 | Loss: 0.8531 | CLoss: 0.7318 | FLoss: 0.2426 | LR: 2.71e-04\n",
            "  Batch 790/898 | Loss: 1.0752 | CLoss: 0.7182 | FLoss: 0.7138 | LR: 2.71e-04\n",
            "  Batch 800/898 | Loss: 0.7457 | CLoss: 0.5963 | FLoss: 0.2986 | LR: 2.71e-04\n",
            "  Batch 810/898 | Loss: 0.9312 | CLoss: 0.6135 | FLoss: 0.6354 | LR: 2.71e-04\n",
            "  Batch 820/898 | Loss: 0.6238 | CLoss: 0.4312 | FLoss: 0.3852 | LR: 2.71e-04\n",
            "  Batch 830/898 | Loss: 0.4902 | CLoss: 0.2873 | FLoss: 0.4059 | LR: 2.71e-04\n",
            "  Batch 840/898 | Loss: 0.7295 | CLoss: 0.5530 | FLoss: 0.3530 | LR: 2.71e-04\n",
            "  Batch 850/898 | Loss: 0.9269 | CLoss: 0.7411 | FLoss: 0.3715 | LR: 2.71e-04\n",
            "  Batch 860/898 | Loss: 0.6339 | CLoss: 0.4808 | FLoss: 0.3062 | LR: 2.71e-04\n",
            "  Batch 870/898 | Loss: 0.7651 | CLoss: 0.5153 | FLoss: 0.4996 | LR: 2.71e-04\n",
            "  Batch 880/898 | Loss: 0.8543 | CLoss: 0.6233 | FLoss: 0.4620 | LR: 2.71e-04\n",
            "  Batch 890/898 | Loss: 0.5221 | CLoss: 0.3898 | FLoss: 0.2647 | LR: 2.71e-04\n",
            "  Batch 898/898 | Loss: 0.3450 | CLoss: 0.0032 | FLoss: 0.6836 | LR: 2.71e-04\n",
            "\n",
            "  Training Summary | Epoch 2\n",
            "  Avg Loss: 0.8459\n",
            "  Last Batch Loss: 0.3450\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/102 | Loss: 0.7190 | Batch Acc: 68.97%\n",
            "    Val Batch 010/102 | Loss: 0.4817 | Batch Acc: 87.93%\n",
            "    Val Batch 015/102 | Loss: 0.4533 | Batch Acc: 87.93%\n",
            "    Val Batch 020/102 | Loss: 0.6390 | Batch Acc: 74.14%\n",
            "    Val Batch 025/102 | Loss: 0.1753 | Batch Acc: 98.28%\n",
            "    Val Batch 030/102 | Loss: 0.1261 | Batch Acc: 94.83%\n",
            "    Val Batch 035/102 | Loss: 0.6580 | Batch Acc: 72.41%\n",
            "    Val Batch 040/102 | Loss: 0.4863 | Batch Acc: 79.31%\n",
            "    Val Batch 045/102 | Loss: 0.9033 | Batch Acc: 55.17%\n",
            "    Val Batch 050/102 | Loss: 0.5672 | Batch Acc: 81.03%\n",
            "    Val Batch 055/102 | Loss: 0.7915 | Batch Acc: 74.14%\n",
            "    Val Batch 060/102 | Loss: 1.9561 | Batch Acc: 51.72%\n",
            "    Val Batch 065/102 | Loss: 0.4770 | Batch Acc: 86.21%\n",
            "    Val Batch 070/102 | Loss: 0.4935 | Batch Acc: 86.21%\n",
            "    Val Batch 075/102 | Loss: 0.2663 | Batch Acc: 93.10%\n",
            "    Val Batch 080/102 | Loss: 0.5454 | Batch Acc: 82.76%\n",
            "    Val Batch 085/102 | Loss: 0.5962 | Batch Acc: 67.24%\n",
            "    Val Batch 090/102 | Loss: 0.1075 | Batch Acc: 94.83%\n",
            "    Val Batch 095/102 | Loss: 0.5720 | Batch Acc: 84.48%\n",
            "    Val Batch 100/102 | Loss: 0.5700 | Batch Acc: 86.21%\n",
            "    Val Batch 102/102 | Loss: 0.4430 | Batch Acc: 96.55%\n",
            "\n",
            "  Validation Summary | Epoch 2\n",
            "  Avg Loss: 0.5495 | Accuracy: 82.55%\n",
            "  Current Best Acc: 83.73%\n",
            "\n",
            "Epoch 3/5\n",
            "  Batch 010/898 | Loss: 0.8118 | CLoss: 0.6404 | FLoss: 0.3429 | LR: 1.96e-04\n",
            "  Batch 020/898 | Loss: 0.8615 | CLoss: 0.6406 | FLoss: 0.4418 | LR: 1.96e-04\n",
            "  Batch 030/898 | Loss: 1.1035 | CLoss: 0.9411 | FLoss: 0.3248 | LR: 1.96e-04\n",
            "  Batch 040/898 | Loss: 0.8254 | CLoss: 0.6101 | FLoss: 0.4306 | LR: 1.96e-04\n",
            "  Batch 050/898 | Loss: 0.7198 | CLoss: 0.5447 | FLoss: 0.3502 | LR: 1.96e-04\n",
            "  Batch 060/898 | Loss: 0.7239 | CLoss: 0.6355 | FLoss: 0.1768 | LR: 1.96e-04\n",
            "  Batch 070/898 | Loss: 0.4190 | CLoss: 0.2976 | FLoss: 0.2429 | LR: 1.96e-04\n",
            "  Batch 080/898 | Loss: 0.9747 | CLoss: 0.7521 | FLoss: 0.4451 | LR: 1.96e-04\n",
            "  Batch 090/898 | Loss: 1.1024 | CLoss: 0.8640 | FLoss: 0.4770 | LR: 1.96e-04\n",
            "  Batch 100/898 | Loss: 1.0678 | CLoss: 0.7875 | FLoss: 0.5606 | LR: 1.96e-04\n",
            "  Batch 110/898 | Loss: 0.7993 | CLoss: 0.5448 | FLoss: 0.5091 | LR: 1.96e-04\n",
            "  Batch 120/898 | Loss: 0.9469 | CLoss: 0.6939 | FLoss: 0.5060 | LR: 1.96e-04\n",
            "  Batch 130/898 | Loss: 0.6825 | CLoss: 0.5096 | FLoss: 0.3458 | LR: 1.96e-04\n",
            "  Batch 140/898 | Loss: 0.4733 | CLoss: 0.2971 | FLoss: 0.3523 | LR: 1.96e-04\n",
            "  Batch 150/898 | Loss: 1.2089 | CLoss: 0.9779 | FLoss: 0.4620 | LR: 1.96e-04\n",
            "  Batch 160/898 | Loss: 0.7143 | CLoss: 0.5468 | FLoss: 0.3351 | LR: 1.96e-04\n",
            "  Batch 170/898 | Loss: 0.9258 | CLoss: 0.7424 | FLoss: 0.3669 | LR: 1.96e-04\n",
            "  Batch 180/898 | Loss: 0.9056 | CLoss: 0.7320 | FLoss: 0.3473 | LR: 1.96e-04\n",
            "  Batch 190/898 | Loss: 0.8019 | CLoss: 0.5787 | FLoss: 0.4463 | LR: 1.96e-04\n",
            "  Batch 200/898 | Loss: 0.9386 | CLoss: 0.6205 | FLoss: 0.6363 | LR: 1.96e-04\n",
            "  Batch 210/898 | Loss: 0.4720 | CLoss: 0.3682 | FLoss: 0.2077 | LR: 1.96e-04\n",
            "  Batch 220/898 | Loss: 0.7851 | CLoss: 0.6190 | FLoss: 0.3321 | LR: 1.96e-04\n",
            "  Batch 230/898 | Loss: 0.7668 | CLoss: 0.6340 | FLoss: 0.2656 | LR: 1.96e-04\n",
            "  Batch 240/898 | Loss: 0.8216 | CLoss: 0.6799 | FLoss: 0.2835 | LR: 1.96e-04\n",
            "  Batch 250/898 | Loss: 0.6530 | CLoss: 0.5704 | FLoss: 0.1652 | LR: 1.96e-04\n",
            "  Batch 260/898 | Loss: 0.9847 | CLoss: 0.7938 | FLoss: 0.3818 | LR: 1.96e-04\n",
            "  Batch 270/898 | Loss: 0.7768 | CLoss: 0.6296 | FLoss: 0.2945 | LR: 1.96e-04\n",
            "  Batch 280/898 | Loss: 0.7288 | CLoss: 0.5345 | FLoss: 0.3886 | LR: 1.96e-04\n",
            "  Batch 290/898 | Loss: 0.8246 | CLoss: 0.7121 | FLoss: 0.2250 | LR: 1.96e-04\n",
            "  Batch 300/898 | Loss: 0.7847 | CLoss: 0.6149 | FLoss: 0.3397 | LR: 1.96e-04\n",
            "  Batch 310/898 | Loss: 1.0165 | CLoss: 0.7939 | FLoss: 0.4452 | LR: 1.96e-04\n",
            "  Batch 320/898 | Loss: 0.5639 | CLoss: 0.3751 | FLoss: 0.3776 | LR: 1.96e-04\n",
            "  Batch 330/898 | Loss: 0.6874 | CLoss: 0.4934 | FLoss: 0.3880 | LR: 1.96e-04\n",
            "  Batch 340/898 | Loss: 0.3844 | CLoss: 0.2868 | FLoss: 0.1951 | LR: 1.96e-04\n",
            "  Batch 350/898 | Loss: 0.8680 | CLoss: 0.6945 | FLoss: 0.3470 | LR: 1.96e-04\n",
            "  Batch 360/898 | Loss: 0.5607 | CLoss: 0.3929 | FLoss: 0.3356 | LR: 1.96e-04\n",
            "  Batch 370/898 | Loss: 1.1112 | CLoss: 0.8726 | FLoss: 0.4770 | LR: 1.96e-04\n",
            "  Batch 380/898 | Loss: 0.5621 | CLoss: 0.4151 | FLoss: 0.2939 | LR: 1.96e-04\n",
            "  Batch 390/898 | Loss: 0.8611 | CLoss: 0.6167 | FLoss: 0.4888 | LR: 1.96e-04\n",
            "  Batch 400/898 | Loss: 0.5379 | CLoss: 0.3768 | FLoss: 0.3221 | LR: 1.96e-04\n",
            "  Batch 410/898 | Loss: 0.9220 | CLoss: 0.7174 | FLoss: 0.4091 | LR: 1.96e-04\n",
            "  Batch 420/898 | Loss: 0.6702 | CLoss: 0.5165 | FLoss: 0.3073 | LR: 1.96e-04\n",
            "  Batch 430/898 | Loss: 0.6010 | CLoss: 0.4269 | FLoss: 0.3483 | LR: 1.96e-04\n",
            "  Batch 440/898 | Loss: 0.9817 | CLoss: 0.7120 | FLoss: 0.5394 | LR: 1.96e-04\n",
            "  Batch 450/898 | Loss: 0.7275 | CLoss: 0.5406 | FLoss: 0.3738 | LR: 1.96e-04\n",
            "  Batch 460/898 | Loss: 0.7783 | CLoss: 0.5919 | FLoss: 0.3728 | LR: 1.96e-04\n",
            "  Batch 470/898 | Loss: 0.8935 | CLoss: 0.6785 | FLoss: 0.4301 | LR: 1.96e-04\n",
            "  Batch 480/898 | Loss: 0.7238 | CLoss: 0.5580 | FLoss: 0.3316 | LR: 1.96e-04\n",
            "  Batch 490/898 | Loss: 0.5317 | CLoss: 0.3794 | FLoss: 0.3045 | LR: 1.96e-04\n",
            "  Batch 500/898 | Loss: 1.0058 | CLoss: 0.7561 | FLoss: 0.4995 | LR: 1.96e-04\n",
            "  Batch 510/898 | Loss: 0.5409 | CLoss: 0.2983 | FLoss: 0.4852 | LR: 1.96e-04\n",
            "  Batch 520/898 | Loss: 0.8225 | CLoss: 0.6658 | FLoss: 0.3135 | LR: 1.96e-04\n",
            "  Batch 530/898 | Loss: 0.5667 | CLoss: 0.4289 | FLoss: 0.2757 | LR: 1.96e-04\n",
            "  Batch 540/898 | Loss: 0.7638 | CLoss: 0.6119 | FLoss: 0.3039 | LR: 1.96e-04\n",
            "  Batch 550/898 | Loss: 0.4930 | CLoss: 0.3164 | FLoss: 0.3532 | LR: 1.96e-04\n",
            "  Batch 560/898 | Loss: 0.7480 | CLoss: 0.6062 | FLoss: 0.2837 | LR: 1.96e-04\n",
            "  Batch 570/898 | Loss: 1.2310 | CLoss: 0.9030 | FLoss: 0.6559 | LR: 1.96e-04\n",
            "  Batch 580/898 | Loss: 0.4891 | CLoss: 0.3856 | FLoss: 0.2069 | LR: 1.96e-04\n",
            "  Batch 590/898 | Loss: 0.7638 | CLoss: 0.6474 | FLoss: 0.2328 | LR: 1.96e-04\n",
            "  Batch 600/898 | Loss: 0.6512 | CLoss: 0.5082 | FLoss: 0.2860 | LR: 1.96e-04\n",
            "  Batch 610/898 | Loss: 0.6385 | CLoss: 0.4726 | FLoss: 0.3318 | LR: 1.96e-04\n",
            "  Batch 620/898 | Loss: 0.3326 | CLoss: 0.2507 | FLoss: 0.1638 | LR: 1.96e-04\n",
            "  Batch 630/898 | Loss: 0.3881 | CLoss: 0.3111 | FLoss: 0.1542 | LR: 1.96e-04\n",
            "  Batch 640/898 | Loss: 0.7344 | CLoss: 0.5883 | FLoss: 0.2921 | LR: 1.96e-04\n",
            "  Batch 650/898 | Loss: 1.0629 | CLoss: 0.7512 | FLoss: 0.6235 | LR: 1.96e-04\n",
            "  Batch 660/898 | Loss: 0.9605 | CLoss: 0.7190 | FLoss: 0.4831 | LR: 1.96e-04\n",
            "  Batch 670/898 | Loss: 1.0485 | CLoss: 0.8328 | FLoss: 0.4315 | LR: 1.96e-04\n",
            "  Batch 680/898 | Loss: 0.4991 | CLoss: 0.3902 | FLoss: 0.2177 | LR: 1.96e-04\n",
            "  Batch 690/898 | Loss: 0.6533 | CLoss: 0.5391 | FLoss: 0.2283 | LR: 1.96e-04\n",
            "  Batch 700/898 | Loss: 0.7896 | CLoss: 0.5835 | FLoss: 0.4123 | LR: 1.96e-04\n",
            "  Batch 710/898 | Loss: 1.2199 | CLoss: 0.8966 | FLoss: 0.6467 | LR: 1.96e-04\n",
            "  Batch 720/898 | Loss: 0.5054 | CLoss: 0.3447 | FLoss: 0.3215 | LR: 1.96e-04\n",
            "  Batch 730/898 | Loss: 0.7431 | CLoss: 0.6481 | FLoss: 0.1901 | LR: 1.96e-04\n",
            "  Batch 740/898 | Loss: 0.3040 | CLoss: 0.2026 | FLoss: 0.2028 | LR: 1.96e-04\n",
            "  Batch 750/898 | Loss: 0.8927 | CLoss: 0.7015 | FLoss: 0.3823 | LR: 1.96e-04\n",
            "  Batch 760/898 | Loss: 0.4953 | CLoss: 0.4289 | FLoss: 0.1327 | LR: 1.96e-04\n",
            "  Batch 770/898 | Loss: 0.8220 | CLoss: 0.6998 | FLoss: 0.2443 | LR: 1.96e-04\n",
            "  Batch 780/898 | Loss: 0.6484 | CLoss: 0.4246 | FLoss: 0.4474 | LR: 1.96e-04\n",
            "  Batch 790/898 | Loss: 0.5770 | CLoss: 0.3985 | FLoss: 0.3571 | LR: 1.96e-04\n",
            "  Batch 800/898 | Loss: 0.6094 | CLoss: 0.4382 | FLoss: 0.3423 | LR: 1.96e-04\n",
            "  Batch 810/898 | Loss: 0.9681 | CLoss: 0.7101 | FLoss: 0.5161 | LR: 1.96e-04\n",
            "  Batch 820/898 | Loss: 0.4706 | CLoss: 0.3041 | FLoss: 0.3330 | LR: 1.96e-04\n",
            "  Batch 830/898 | Loss: 1.0834 | CLoss: 0.9161 | FLoss: 0.3346 | LR: 1.96e-04\n",
            "  Batch 840/898 | Loss: 1.3841 | CLoss: 1.1499 | FLoss: 0.4684 | LR: 1.96e-04\n",
            "  Batch 850/898 | Loss: 0.6394 | CLoss: 0.5171 | FLoss: 0.2444 | LR: 1.96e-04\n",
            "  Batch 860/898 | Loss: 0.3312 | CLoss: 0.2077 | FLoss: 0.2472 | LR: 1.96e-04\n",
            "  Batch 870/898 | Loss: 0.5118 | CLoss: 0.4112 | FLoss: 0.2011 | LR: 1.96e-04\n",
            "  Batch 880/898 | Loss: 0.8992 | CLoss: 0.7016 | FLoss: 0.3952 | LR: 1.96e-04\n",
            "  Batch 890/898 | Loss: 0.6915 | CLoss: 0.5528 | FLoss: 0.2774 | LR: 1.96e-04\n",
            "  Batch 898/898 | Loss: 0.4611 | CLoss: 0.0777 | FLoss: 0.7669 | LR: 1.96e-04\n",
            "\n",
            "  Training Summary | Epoch 3\n",
            "  Avg Loss: 0.7642\n",
            "  Last Batch Loss: 0.4611\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/102 | Loss: 0.8935 | Batch Acc: 70.69%\n",
            "    Val Batch 010/102 | Loss: 0.4985 | Batch Acc: 82.76%\n",
            "    Val Batch 015/102 | Loss: 0.6909 | Batch Acc: 77.59%\n",
            "    Val Batch 020/102 | Loss: 0.6684 | Batch Acc: 60.34%\n",
            "    Val Batch 025/102 | Loss: 0.2798 | Batch Acc: 89.66%\n",
            "    Val Batch 030/102 | Loss: 0.0373 | Batch Acc: 98.28%\n",
            "    Val Batch 035/102 | Loss: 0.1272 | Batch Acc: 98.28%\n",
            "    Val Batch 040/102 | Loss: 0.5537 | Batch Acc: 89.66%\n",
            "    Val Batch 045/102 | Loss: 0.6680 | Batch Acc: 94.83%\n",
            "    Val Batch 050/102 | Loss: 0.4620 | Batch Acc: 89.66%\n",
            "    Val Batch 055/102 | Loss: 0.7651 | Batch Acc: 70.69%\n",
            "    Val Batch 060/102 | Loss: 0.8094 | Batch Acc: 74.14%\n",
            "    Val Batch 065/102 | Loss: 0.6738 | Batch Acc: 79.31%\n",
            "    Val Batch 070/102 | Loss: 0.5006 | Batch Acc: 82.76%\n",
            "    Val Batch 075/102 | Loss: 0.1162 | Batch Acc: 96.55%\n",
            "    Val Batch 080/102 | Loss: 0.1630 | Batch Acc: 94.83%\n",
            "    Val Batch 085/102 | Loss: 0.3757 | Batch Acc: 89.66%\n",
            "    Val Batch 090/102 | Loss: 0.1941 | Batch Acc: 94.83%\n",
            "    Val Batch 095/102 | Loss: 0.1175 | Batch Acc: 93.10%\n",
            "    Val Batch 100/102 | Loss: 0.1308 | Batch Acc: 94.83%\n",
            "    Val Batch 102/102 | Loss: 0.1766 | Batch Acc: 96.55%\n",
            "\n",
            "  Validation Summary | Epoch 3\n",
            "  Avg Loss: 0.4697 | Accuracy: 84.83%\n",
            "  Current Best Acc: 84.83%\n",
            "\n",
            "Epoch 4/5\n",
            "  Batch 010/898 | Loss: 0.6092 | CLoss: 0.5283 | FLoss: 0.1617 | LR: 1.04e-04\n",
            "  Batch 020/898 | Loss: 0.5102 | CLoss: 0.3532 | FLoss: 0.3140 | LR: 1.04e-04\n",
            "  Batch 030/898 | Loss: 0.4702 | CLoss: 0.3366 | FLoss: 0.2672 | LR: 1.04e-04\n",
            "  Batch 040/898 | Loss: 0.7511 | CLoss: 0.4533 | FLoss: 0.5956 | LR: 1.04e-04\n",
            "  Batch 050/898 | Loss: 0.9035 | CLoss: 0.7414 | FLoss: 0.3243 | LR: 1.04e-04\n",
            "  Batch 060/898 | Loss: 0.3971 | CLoss: 0.3097 | FLoss: 0.1748 | LR: 1.04e-04\n",
            "  Batch 070/898 | Loss: 0.8548 | CLoss: 0.6748 | FLoss: 0.3600 | LR: 1.04e-04\n",
            "  Batch 080/898 | Loss: 0.7582 | CLoss: 0.5012 | FLoss: 0.5140 | LR: 1.04e-04\n",
            "  Batch 090/898 | Loss: 0.5713 | CLoss: 0.4325 | FLoss: 0.2776 | LR: 1.04e-04\n",
            "  Batch 100/898 | Loss: 0.9569 | CLoss: 0.7490 | FLoss: 0.4158 | LR: 1.04e-04\n",
            "  Batch 110/898 | Loss: 0.4421 | CLoss: 0.3436 | FLoss: 0.1971 | LR: 1.04e-04\n",
            "  Batch 120/898 | Loss: 0.6577 | CLoss: 0.4755 | FLoss: 0.3643 | LR: 1.04e-04\n",
            "  Batch 130/898 | Loss: 0.9704 | CLoss: 0.7292 | FLoss: 0.4824 | LR: 1.04e-04\n",
            "  Batch 140/898 | Loss: 0.5937 | CLoss: 0.4573 | FLoss: 0.2728 | LR: 1.04e-04\n",
            "  Batch 150/898 | Loss: 0.4253 | CLoss: 0.2789 | FLoss: 0.2927 | LR: 1.04e-04\n",
            "  Batch 160/898 | Loss: 0.7619 | CLoss: 0.5633 | FLoss: 0.3972 | LR: 1.04e-04\n",
            "  Batch 170/898 | Loss: 0.9425 | CLoss: 0.7207 | FLoss: 0.4435 | LR: 1.04e-04\n",
            "  Batch 180/898 | Loss: 0.7730 | CLoss: 0.5941 | FLoss: 0.3578 | LR: 1.04e-04\n",
            "  Batch 190/898 | Loss: 0.6171 | CLoss: 0.4913 | FLoss: 0.2516 | LR: 1.04e-04\n",
            "  Batch 200/898 | Loss: 0.6304 | CLoss: 0.4665 | FLoss: 0.3277 | LR: 1.04e-04\n",
            "  Batch 210/898 | Loss: 0.6896 | CLoss: 0.5236 | FLoss: 0.3320 | LR: 1.04e-04\n",
            "  Batch 220/898 | Loss: 1.0961 | CLoss: 0.8536 | FLoss: 0.4850 | LR: 1.04e-04\n",
            "  Batch 230/898 | Loss: 0.7042 | CLoss: 0.5434 | FLoss: 0.3217 | LR: 1.04e-04\n",
            "  Batch 240/898 | Loss: 0.8002 | CLoss: 0.6043 | FLoss: 0.3917 | LR: 1.04e-04\n",
            "  Batch 250/898 | Loss: 0.9586 | CLoss: 0.7116 | FLoss: 0.4939 | LR: 1.04e-04\n",
            "  Batch 260/898 | Loss: 0.9450 | CLoss: 0.7674 | FLoss: 0.3552 | LR: 1.04e-04\n",
            "  Batch 270/898 | Loss: 0.7250 | CLoss: 0.4823 | FLoss: 0.4854 | LR: 1.04e-04\n",
            "  Batch 280/898 | Loss: 0.4379 | CLoss: 0.2568 | FLoss: 0.3622 | LR: 1.04e-04\n",
            "  Batch 290/898 | Loss: 0.5520 | CLoss: 0.4185 | FLoss: 0.2670 | LR: 1.04e-04\n",
            "  Batch 300/898 | Loss: 0.5806 | CLoss: 0.4783 | FLoss: 0.2046 | LR: 1.04e-04\n",
            "  Batch 310/898 | Loss: 0.5463 | CLoss: 0.4130 | FLoss: 0.2666 | LR: 1.04e-04\n",
            "  Batch 320/898 | Loss: 0.4242 | CLoss: 0.2409 | FLoss: 0.3666 | LR: 1.04e-04\n",
            "  Batch 330/898 | Loss: 0.4050 | CLoss: 0.2979 | FLoss: 0.2142 | LR: 1.04e-04\n",
            "  Batch 340/898 | Loss: 0.8525 | CLoss: 0.5706 | FLoss: 0.5638 | LR: 1.04e-04\n",
            "  Batch 350/898 | Loss: 1.0156 | CLoss: 0.8454 | FLoss: 0.3403 | LR: 1.04e-04\n",
            "  Batch 360/898 | Loss: 0.5451 | CLoss: 0.3803 | FLoss: 0.3297 | LR: 1.04e-04\n",
            "  Batch 370/898 | Loss: 0.6582 | CLoss: 0.5219 | FLoss: 0.2727 | LR: 1.04e-04\n",
            "  Batch 380/898 | Loss: 0.6281 | CLoss: 0.4962 | FLoss: 0.2639 | LR: 1.04e-04\n",
            "  Batch 390/898 | Loss: 0.5495 | CLoss: 0.3871 | FLoss: 0.3249 | LR: 1.04e-04\n",
            "  Batch 400/898 | Loss: 0.7151 | CLoss: 0.5608 | FLoss: 0.3087 | LR: 1.04e-04\n",
            "  Batch 410/898 | Loss: 0.7883 | CLoss: 0.6343 | FLoss: 0.3080 | LR: 1.04e-04\n",
            "  Batch 420/898 | Loss: 0.6144 | CLoss: 0.4535 | FLoss: 0.3218 | LR: 1.04e-04\n",
            "  Batch 430/898 | Loss: 0.8620 | CLoss: 0.7029 | FLoss: 0.3181 | LR: 1.04e-04\n",
            "  Batch 440/898 | Loss: 0.6837 | CLoss: 0.5088 | FLoss: 0.3498 | LR: 1.04e-04\n",
            "  Batch 450/898 | Loss: 0.6078 | CLoss: 0.4963 | FLoss: 0.2229 | LR: 1.04e-04\n",
            "  Batch 460/898 | Loss: 0.4963 | CLoss: 0.3536 | FLoss: 0.2854 | LR: 1.04e-04\n",
            "  Batch 470/898 | Loss: 0.7784 | CLoss: 0.5737 | FLoss: 0.4092 | LR: 1.04e-04\n",
            "  Batch 480/898 | Loss: 0.8323 | CLoss: 0.6225 | FLoss: 0.4196 | LR: 1.04e-04\n",
            "  Batch 490/898 | Loss: 0.7390 | CLoss: 0.5558 | FLoss: 0.3663 | LR: 1.04e-04\n",
            "  Batch 500/898 | Loss: 0.5876 | CLoss: 0.4021 | FLoss: 0.3711 | LR: 1.04e-04\n",
            "  Batch 510/898 | Loss: 0.4208 | CLoss: 0.3398 | FLoss: 0.1622 | LR: 1.04e-04\n",
            "  Batch 520/898 | Loss: 0.6574 | CLoss: 0.4906 | FLoss: 0.3335 | LR: 1.04e-04\n",
            "  Batch 530/898 | Loss: 0.5156 | CLoss: 0.3961 | FLoss: 0.2390 | LR: 1.04e-04\n",
            "  Batch 540/898 | Loss: 0.6349 | CLoss: 0.4774 | FLoss: 0.3151 | LR: 1.04e-04\n",
            "  Batch 550/898 | Loss: 0.6150 | CLoss: 0.4503 | FLoss: 0.3294 | LR: 1.04e-04\n",
            "  Batch 560/898 | Loss: 0.7375 | CLoss: 0.6481 | FLoss: 0.1787 | LR: 1.04e-04\n",
            "  Batch 570/898 | Loss: 0.8076 | CLoss: 0.6613 | FLoss: 0.2927 | LR: 1.04e-04\n",
            "  Batch 580/898 | Loss: 0.8121 | CLoss: 0.6137 | FLoss: 0.3968 | LR: 1.04e-04\n",
            "  Batch 590/898 | Loss: 0.5509 | CLoss: 0.3890 | FLoss: 0.3238 | LR: 1.04e-04\n",
            "  Batch 600/898 | Loss: 0.8800 | CLoss: 0.7081 | FLoss: 0.3439 | LR: 1.04e-04\n",
            "  Batch 610/898 | Loss: 0.7491 | CLoss: 0.5655 | FLoss: 0.3671 | LR: 1.04e-04\n",
            "  Batch 620/898 | Loss: 0.8675 | CLoss: 0.6125 | FLoss: 0.5100 | LR: 1.04e-04\n",
            "  Batch 630/898 | Loss: 0.6003 | CLoss: 0.4339 | FLoss: 0.3327 | LR: 1.04e-04\n",
            "  Batch 640/898 | Loss: 0.5999 | CLoss: 0.4857 | FLoss: 0.2283 | LR: 1.04e-04\n",
            "  Batch 650/898 | Loss: 0.8319 | CLoss: 0.6398 | FLoss: 0.3841 | LR: 1.04e-04\n",
            "  Batch 660/898 | Loss: 1.0096 | CLoss: 0.8716 | FLoss: 0.2759 | LR: 1.04e-04\n",
            "  Batch 670/898 | Loss: 0.5400 | CLoss: 0.3827 | FLoss: 0.3147 | LR: 1.04e-04\n",
            "  Batch 680/898 | Loss: 0.7408 | CLoss: 0.5527 | FLoss: 0.3761 | LR: 1.04e-04\n",
            "  Batch 690/898 | Loss: 0.4439 | CLoss: 0.3442 | FLoss: 0.1994 | LR: 1.04e-04\n",
            "  Batch 700/898 | Loss: 0.6377 | CLoss: 0.4593 | FLoss: 0.3570 | LR: 1.04e-04\n",
            "  Batch 710/898 | Loss: 0.8069 | CLoss: 0.6909 | FLoss: 0.2319 | LR: 1.04e-04\n",
            "  Batch 720/898 | Loss: 0.7084 | CLoss: 0.5715 | FLoss: 0.2738 | LR: 1.04e-04\n",
            "  Batch 730/898 | Loss: 0.8890 | CLoss: 0.7457 | FLoss: 0.2866 | LR: 1.04e-04\n",
            "  Batch 740/898 | Loss: 0.8981 | CLoss: 0.7453 | FLoss: 0.3055 | LR: 1.04e-04\n",
            "  Batch 750/898 | Loss: 0.6804 | CLoss: 0.4802 | FLoss: 0.4003 | LR: 1.04e-04\n",
            "  Batch 760/898 | Loss: 1.3674 | CLoss: 1.0918 | FLoss: 0.5510 | LR: 1.04e-04\n",
            "  Batch 770/898 | Loss: 0.8448 | CLoss: 0.6542 | FLoss: 0.3812 | LR: 1.04e-04\n",
            "  Batch 780/898 | Loss: 0.8643 | CLoss: 0.6717 | FLoss: 0.3852 | LR: 1.04e-04\n",
            "  Batch 790/898 | Loss: 0.8806 | CLoss: 0.6394 | FLoss: 0.4824 | LR: 1.04e-04\n",
            "  Batch 800/898 | Loss: 0.5162 | CLoss: 0.3558 | FLoss: 0.3209 | LR: 1.04e-04\n",
            "  Batch 810/898 | Loss: 0.5529 | CLoss: 0.4640 | FLoss: 0.1779 | LR: 1.04e-04\n",
            "  Batch 820/898 | Loss: 0.9871 | CLoss: 0.7415 | FLoss: 0.4913 | LR: 1.04e-04\n",
            "  Batch 830/898 | Loss: 0.5824 | CLoss: 0.4860 | FLoss: 0.1928 | LR: 1.04e-04\n",
            "  Batch 840/898 | Loss: 0.7867 | CLoss: 0.6030 | FLoss: 0.3673 | LR: 1.04e-04\n",
            "  Batch 850/898 | Loss: 0.5491 | CLoss: 0.4212 | FLoss: 0.2557 | LR: 1.04e-04\n",
            "  Batch 860/898 | Loss: 1.0754 | CLoss: 0.9532 | FLoss: 0.2443 | LR: 1.04e-04\n",
            "  Batch 870/898 | Loss: 0.3513 | CLoss: 0.2242 | FLoss: 0.2541 | LR: 1.04e-04\n",
            "  Batch 880/898 | Loss: 0.9503 | CLoss: 0.7340 | FLoss: 0.4326 | LR: 1.04e-04\n",
            "  Batch 890/898 | Loss: 0.6786 | CLoss: 0.5961 | FLoss: 0.1651 | LR: 1.04e-04\n",
            "  Batch 898/898 | Loss: 0.4207 | CLoss: 0.0001 | FLoss: 0.8411 | LR: 1.04e-04\n",
            "\n",
            "  Training Summary | Epoch 4\n",
            "  Avg Loss: 0.6938\n",
            "  Last Batch Loss: 0.4207\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/102 | Loss: 0.6034 | Batch Acc: 70.69%\n",
            "    Val Batch 010/102 | Loss: 0.4235 | Batch Acc: 93.10%\n",
            "    Val Batch 015/102 | Loss: 0.6637 | Batch Acc: 86.21%\n",
            "    Val Batch 020/102 | Loss: 1.0743 | Batch Acc: 41.38%\n",
            "    Val Batch 025/102 | Loss: 0.2894 | Batch Acc: 91.38%\n",
            "    Val Batch 030/102 | Loss: 0.1645 | Batch Acc: 96.55%\n",
            "    Val Batch 035/102 | Loss: 0.3277 | Batch Acc: 93.10%\n",
            "    Val Batch 040/102 | Loss: 0.3856 | Batch Acc: 96.55%\n",
            "    Val Batch 045/102 | Loss: 0.8195 | Batch Acc: 98.28%\n",
            "    Val Batch 050/102 | Loss: 0.3953 | Batch Acc: 87.93%\n",
            "    Val Batch 055/102 | Loss: 0.8375 | Batch Acc: 81.03%\n",
            "    Val Batch 060/102 | Loss: 1.1084 | Batch Acc: 74.14%\n",
            "    Val Batch 065/102 | Loss: 0.6870 | Batch Acc: 81.03%\n",
            "    Val Batch 070/102 | Loss: 0.8951 | Batch Acc: 81.03%\n",
            "    Val Batch 075/102 | Loss: 0.1042 | Batch Acc: 94.83%\n",
            "    Val Batch 080/102 | Loss: 0.4307 | Batch Acc: 86.21%\n",
            "    Val Batch 085/102 | Loss: 0.4180 | Batch Acc: 86.21%\n",
            "    Val Batch 090/102 | Loss: 0.1923 | Batch Acc: 94.83%\n",
            "    Val Batch 095/102 | Loss: 0.0850 | Batch Acc: 96.55%\n",
            "    Val Batch 100/102 | Loss: 0.0185 | Batch Acc: 100.00%\n",
            "    Val Batch 102/102 | Loss: 0.3485 | Batch Acc: 93.10%\n",
            "\n",
            "  Validation Summary | Epoch 4\n",
            "  Avg Loss: 0.4610 | Accuracy: 86.04%\n",
            "  Current Best Acc: 86.04%\n",
            "\n",
            "Epoch 5/5\n",
            "  Batch 010/898 | Loss: 0.7092 | CLoss: 0.5351 | FLoss: 0.3482 | LR: 2.86e-05\n",
            "  Batch 020/898 | Loss: 0.8042 | CLoss: 0.6107 | FLoss: 0.3870 | LR: 2.86e-05\n",
            "  Batch 030/898 | Loss: 0.7604 | CLoss: 0.5980 | FLoss: 0.3248 | LR: 2.86e-05\n",
            "  Batch 040/898 | Loss: 0.9643 | CLoss: 0.6988 | FLoss: 0.5309 | LR: 2.86e-05\n",
            "  Batch 050/898 | Loss: 0.6171 | CLoss: 0.4234 | FLoss: 0.3874 | LR: 2.86e-05\n",
            "  Batch 060/898 | Loss: 0.5466 | CLoss: 0.4032 | FLoss: 0.2868 | LR: 2.86e-05\n",
            "  Batch 070/898 | Loss: 0.7410 | CLoss: 0.6242 | FLoss: 0.2337 | LR: 2.86e-05\n",
            "  Batch 080/898 | Loss: 0.4923 | CLoss: 0.3852 | FLoss: 0.2143 | LR: 2.86e-05\n",
            "  Batch 090/898 | Loss: 0.4413 | CLoss: 0.2984 | FLoss: 0.2859 | LR: 2.86e-05\n",
            "  Batch 100/898 | Loss: 0.8708 | CLoss: 0.7277 | FLoss: 0.2862 | LR: 2.86e-05\n",
            "  Batch 110/898 | Loss: 0.6014 | CLoss: 0.4436 | FLoss: 0.3156 | LR: 2.86e-05\n",
            "  Batch 120/898 | Loss: 0.5586 | CLoss: 0.4302 | FLoss: 0.2568 | LR: 2.86e-05\n",
            "  Batch 130/898 | Loss: 0.5733 | CLoss: 0.4400 | FLoss: 0.2666 | LR: 2.86e-05\n",
            "  Batch 140/898 | Loss: 0.5181 | CLoss: 0.3828 | FLoss: 0.2705 | LR: 2.86e-05\n",
            "  Batch 150/898 | Loss: 1.2421 | CLoss: 1.0463 | FLoss: 0.3916 | LR: 2.86e-05\n",
            "  Batch 160/898 | Loss: 1.0218 | CLoss: 0.8574 | FLoss: 0.3289 | LR: 2.86e-05\n",
            "  Batch 170/898 | Loss: 1.0181 | CLoss: 0.9004 | FLoss: 0.2354 | LR: 2.86e-05\n",
            "  Batch 180/898 | Loss: 0.5974 | CLoss: 0.4717 | FLoss: 0.2514 | LR: 2.86e-05\n",
            "  Batch 190/898 | Loss: 0.4319 | CLoss: 0.3426 | FLoss: 0.1787 | LR: 2.86e-05\n",
            "  Batch 200/898 | Loss: 0.6363 | CLoss: 0.5148 | FLoss: 0.2429 | LR: 2.86e-05\n",
            "  Batch 210/898 | Loss: 0.5321 | CLoss: 0.4375 | FLoss: 0.1891 | LR: 2.86e-05\n",
            "  Batch 220/898 | Loss: 0.3361 | CLoss: 0.2555 | FLoss: 0.1613 | LR: 2.86e-05\n",
            "  Batch 230/898 | Loss: 0.4470 | CLoss: 0.2982 | FLoss: 0.2977 | LR: 2.86e-05\n",
            "  Batch 240/898 | Loss: 0.8843 | CLoss: 0.7390 | FLoss: 0.2906 | LR: 2.86e-05\n",
            "  Batch 250/898 | Loss: 0.6912 | CLoss: 0.5585 | FLoss: 0.2654 | LR: 2.86e-05\n",
            "  Batch 260/898 | Loss: 0.3536 | CLoss: 0.2573 | FLoss: 0.1925 | LR: 2.86e-05\n",
            "  Batch 270/898 | Loss: 0.5512 | CLoss: 0.4027 | FLoss: 0.2969 | LR: 2.86e-05\n",
            "  Batch 280/898 | Loss: 0.7167 | CLoss: 0.5981 | FLoss: 0.2373 | LR: 2.86e-05\n",
            "  Batch 290/898 | Loss: 0.8351 | CLoss: 0.6487 | FLoss: 0.3730 | LR: 2.86e-05\n",
            "  Batch 300/898 | Loss: 0.8755 | CLoss: 0.7272 | FLoss: 0.2966 | LR: 2.86e-05\n",
            "  Batch 310/898 | Loss: 0.5427 | CLoss: 0.3921 | FLoss: 0.3011 | LR: 2.86e-05\n",
            "  Batch 320/898 | Loss: 0.3774 | CLoss: 0.2832 | FLoss: 0.1883 | LR: 2.86e-05\n",
            "  Batch 330/898 | Loss: 0.4676 | CLoss: 0.3207 | FLoss: 0.2937 | LR: 2.86e-05\n",
            "  Batch 340/898 | Loss: 0.8616 | CLoss: 0.6590 | FLoss: 0.4052 | LR: 2.86e-05\n",
            "  Batch 350/898 | Loss: 0.4183 | CLoss: 0.2266 | FLoss: 0.3834 | LR: 2.86e-05\n",
            "  Batch 360/898 | Loss: 0.8694 | CLoss: 0.6835 | FLoss: 0.3719 | LR: 2.86e-05\n",
            "  Batch 370/898 | Loss: 0.7525 | CLoss: 0.6424 | FLoss: 0.2202 | LR: 2.86e-05\n",
            "  Batch 380/898 | Loss: 0.2121 | CLoss: 0.1510 | FLoss: 0.1222 | LR: 2.86e-05\n",
            "  Batch 390/898 | Loss: 0.3457 | CLoss: 0.2109 | FLoss: 0.2696 | LR: 2.86e-05\n",
            "  Batch 400/898 | Loss: 0.8359 | CLoss: 0.6620 | FLoss: 0.3478 | LR: 2.86e-05\n",
            "  Batch 410/898 | Loss: 0.6270 | CLoss: 0.5454 | FLoss: 0.1632 | LR: 2.86e-05\n",
            "  Batch 420/898 | Loss: 0.7221 | CLoss: 0.6177 | FLoss: 0.2087 | LR: 2.86e-05\n",
            "  Batch 430/898 | Loss: 0.5380 | CLoss: 0.4391 | FLoss: 0.1977 | LR: 2.86e-05\n",
            "  Batch 440/898 | Loss: 0.6258 | CLoss: 0.4625 | FLoss: 0.3265 | LR: 2.86e-05\n",
            "  Batch 450/898 | Loss: 0.7329 | CLoss: 0.6089 | FLoss: 0.2481 | LR: 2.86e-05\n",
            "  Batch 460/898 | Loss: 0.4713 | CLoss: 0.3489 | FLoss: 0.2448 | LR: 2.86e-05\n",
            "  Batch 470/898 | Loss: 0.6619 | CLoss: 0.5505 | FLoss: 0.2227 | LR: 2.86e-05\n",
            "  Batch 480/898 | Loss: 0.4612 | CLoss: 0.3332 | FLoss: 0.2561 | LR: 2.86e-05\n",
            "  Batch 490/898 | Loss: 0.7523 | CLoss: 0.5876 | FLoss: 0.3293 | LR: 2.86e-05\n",
            "  Batch 500/898 | Loss: 0.5922 | CLoss: 0.4821 | FLoss: 0.2203 | LR: 2.86e-05\n",
            "  Batch 510/898 | Loss: 0.5227 | CLoss: 0.3056 | FLoss: 0.4342 | LR: 2.86e-05\n",
            "  Batch 520/898 | Loss: 0.8205 | CLoss: 0.6747 | FLoss: 0.2916 | LR: 2.86e-05\n",
            "  Batch 530/898 | Loss: 0.3338 | CLoss: 0.2303 | FLoss: 0.2070 | LR: 2.86e-05\n",
            "  Batch 540/898 | Loss: 0.5894 | CLoss: 0.5009 | FLoss: 0.1771 | LR: 2.86e-05\n",
            "  Batch 550/898 | Loss: 0.6426 | CLoss: 0.5214 | FLoss: 0.2424 | LR: 2.86e-05\n",
            "  Batch 560/898 | Loss: 0.7541 | CLoss: 0.5860 | FLoss: 0.3363 | LR: 2.86e-05\n",
            "  Batch 570/898 | Loss: 0.5237 | CLoss: 0.3662 | FLoss: 0.3150 | LR: 2.86e-05\n",
            "  Batch 580/898 | Loss: 0.3853 | CLoss: 0.2393 | FLoss: 0.2920 | LR: 2.86e-05\n",
            "  Batch 590/898 | Loss: 0.4956 | CLoss: 0.3573 | FLoss: 0.2766 | LR: 2.86e-05\n",
            "  Batch 600/898 | Loss: 0.5715 | CLoss: 0.4143 | FLoss: 0.3143 | LR: 2.86e-05\n",
            "  Batch 610/898 | Loss: 0.7259 | CLoss: 0.5113 | FLoss: 0.4293 | LR: 2.86e-05\n",
            "  Batch 620/898 | Loss: 0.2873 | CLoss: 0.1864 | FLoss: 0.2019 | LR: 2.86e-05\n",
            "  Batch 630/898 | Loss: 0.4139 | CLoss: 0.2957 | FLoss: 0.2364 | LR: 2.86e-05\n",
            "  Batch 640/898 | Loss: 0.4690 | CLoss: 0.3725 | FLoss: 0.1930 | LR: 2.86e-05\n",
            "  Batch 650/898 | Loss: 0.7748 | CLoss: 0.5777 | FLoss: 0.3943 | LR: 2.86e-05\n",
            "  Batch 660/898 | Loss: 0.7800 | CLoss: 0.6074 | FLoss: 0.3453 | LR: 2.86e-05\n",
            "  Batch 670/898 | Loss: 0.2645 | CLoss: 0.1796 | FLoss: 0.1698 | LR: 2.86e-05\n",
            "  Batch 680/898 | Loss: 0.3920 | CLoss: 0.3135 | FLoss: 0.1570 | LR: 2.86e-05\n",
            "  Batch 690/898 | Loss: 0.6927 | CLoss: 0.5699 | FLoss: 0.2455 | LR: 2.86e-05\n",
            "  Batch 700/898 | Loss: 0.8564 | CLoss: 0.6463 | FLoss: 0.4201 | LR: 2.86e-05\n",
            "  Batch 710/898 | Loss: 0.5077 | CLoss: 0.3819 | FLoss: 0.2516 | LR: 2.86e-05\n",
            "  Batch 720/898 | Loss: 0.3910 | CLoss: 0.3296 | FLoss: 0.1228 | LR: 2.86e-05\n",
            "  Batch 730/898 | Loss: 0.6134 | CLoss: 0.4647 | FLoss: 0.2974 | LR: 2.86e-05\n",
            "  Batch 740/898 | Loss: 0.9558 | CLoss: 0.7113 | FLoss: 0.4889 | LR: 2.86e-05\n",
            "  Batch 750/898 | Loss: 0.7229 | CLoss: 0.5740 | FLoss: 0.2978 | LR: 2.86e-05\n",
            "  Batch 760/898 | Loss: 1.0119 | CLoss: 0.8538 | FLoss: 0.3162 | LR: 2.86e-05\n",
            "  Batch 770/898 | Loss: 1.1135 | CLoss: 0.8917 | FLoss: 0.4437 | LR: 2.86e-05\n",
            "  Batch 780/898 | Loss: 0.2032 | CLoss: 0.1235 | FLoss: 0.1595 | LR: 2.86e-05\n",
            "  Batch 790/898 | Loss: 0.7104 | CLoss: 0.6407 | FLoss: 0.1394 | LR: 2.86e-05\n",
            "  Batch 800/898 | Loss: 0.6596 | CLoss: 0.4663 | FLoss: 0.3867 | LR: 2.86e-05\n",
            "  Batch 810/898 | Loss: 0.6818 | CLoss: 0.5233 | FLoss: 0.3170 | LR: 2.86e-05\n",
            "  Batch 820/898 | Loss: 0.3243 | CLoss: 0.2347 | FLoss: 0.1792 | LR: 2.86e-05\n",
            "  Batch 830/898 | Loss: 0.5448 | CLoss: 0.3430 | FLoss: 0.4036 | LR: 2.86e-05\n",
            "  Batch 840/898 | Loss: 0.5651 | CLoss: 0.4035 | FLoss: 0.3231 | LR: 2.86e-05\n",
            "  Batch 850/898 | Loss: 1.0036 | CLoss: 0.8218 | FLoss: 0.3637 | LR: 2.86e-05\n",
            "  Batch 860/898 | Loss: 0.6479 | CLoss: 0.4922 | FLoss: 0.3114 | LR: 2.86e-05\n",
            "  Batch 870/898 | Loss: 0.8943 | CLoss: 0.6629 | FLoss: 0.4628 | LR: 2.86e-05\n",
            "  Batch 880/898 | Loss: 0.9397 | CLoss: 0.7783 | FLoss: 0.3227 | LR: 2.86e-05\n",
            "  Batch 890/898 | Loss: 0.4025 | CLoss: 0.3326 | FLoss: 0.1397 | LR: 2.86e-05\n",
            "  Batch 898/898 | Loss: 0.2874 | CLoss: 0.0000 | FLoss: 0.5748 | LR: 2.86e-05\n",
            "\n",
            "  Training Summary | Epoch 5\n",
            "  Avg Loss: 0.6345\n",
            "  Last Batch Loss: 0.2874\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/102 | Loss: 0.5109 | Batch Acc: 74.14%\n",
            "    Val Batch 010/102 | Loss: 0.2944 | Batch Acc: 91.38%\n",
            "    Val Batch 015/102 | Loss: 0.2881 | Batch Acc: 87.93%\n",
            "    Val Batch 020/102 | Loss: 0.6582 | Batch Acc: 56.90%\n",
            "    Val Batch 025/102 | Loss: 0.5719 | Batch Acc: 82.76%\n",
            "    Val Batch 030/102 | Loss: 0.0191 | Batch Acc: 100.00%\n",
            "    Val Batch 035/102 | Loss: 0.1105 | Batch Acc: 96.55%\n",
            "    Val Batch 040/102 | Loss: 0.3998 | Batch Acc: 98.28%\n",
            "    Val Batch 045/102 | Loss: 0.8532 | Batch Acc: 91.38%\n",
            "    Val Batch 050/102 | Loss: 0.1874 | Batch Acc: 94.83%\n",
            "    Val Batch 055/102 | Loss: 0.5095 | Batch Acc: 86.21%\n",
            "    Val Batch 060/102 | Loss: 1.0030 | Batch Acc: 68.97%\n",
            "    Val Batch 065/102 | Loss: 0.4674 | Batch Acc: 84.48%\n",
            "    Val Batch 070/102 | Loss: 0.3318 | Batch Acc: 87.93%\n",
            "    Val Batch 075/102 | Loss: 0.1106 | Batch Acc: 98.28%\n",
            "    Val Batch 080/102 | Loss: 0.3655 | Batch Acc: 91.38%\n",
            "    Val Batch 085/102 | Loss: 0.3813 | Batch Acc: 89.66%\n",
            "    Val Batch 090/102 | Loss: 0.3517 | Batch Acc: 86.21%\n",
            "    Val Batch 095/102 | Loss: 0.0696 | Batch Acc: 98.28%\n",
            "    Val Batch 100/102 | Loss: 0.3698 | Batch Acc: 87.93%\n",
            "    Val Batch 102/102 | Loss: 0.0152 | Batch Acc: 100.00%\n",
            "\n",
            "  Validation Summary | Epoch 5\n",
            "  Avg Loss: 0.4090 | Accuracy: 87.34%\n",
            "  Current Best Acc: 87.34%\n",
            "\n",
            "========================================\n",
            "=== Fold 7 Completed ===\n",
            "Best Validation Accuracy: 87.34%\n",
            "\n",
            "========================================\n",
            "=== Fold 8/10 ====================\n",
            "========================================\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "  Batch 010/902 | Loss: 0.8527 | CLoss: 0.6130 | FLoss: 0.4794 | LR: 3.00e-04\n",
            "  Batch 020/902 | Loss: 0.9054 | CLoss: 0.7994 | FLoss: 0.2120 | LR: 3.00e-04\n",
            "  Batch 030/902 | Loss: 0.9340 | CLoss: 0.7634 | FLoss: 0.3413 | LR: 3.00e-04\n",
            "  Batch 040/902 | Loss: 0.4170 | CLoss: 0.2830 | FLoss: 0.2680 | LR: 3.00e-04\n",
            "  Batch 050/902 | Loss: 0.5683 | CLoss: 0.4570 | FLoss: 0.2226 | LR: 3.00e-04\n",
            "  Batch 060/902 | Loss: 0.4813 | CLoss: 0.3338 | FLoss: 0.2950 | LR: 3.00e-04\n",
            "  Batch 070/902 | Loss: 0.5828 | CLoss: 0.4055 | FLoss: 0.3546 | LR: 3.00e-04\n",
            "  Batch 080/902 | Loss: 0.8890 | CLoss: 0.7195 | FLoss: 0.3390 | LR: 3.00e-04\n",
            "  Batch 090/902 | Loss: 1.2455 | CLoss: 0.9671 | FLoss: 0.5566 | LR: 3.00e-04\n",
            "  Batch 100/902 | Loss: 1.1129 | CLoss: 0.9285 | FLoss: 0.3688 | LR: 3.00e-04\n",
            "  Batch 110/902 | Loss: 0.9634 | CLoss: 0.7260 | FLoss: 0.4750 | LR: 3.00e-04\n",
            "  Batch 120/902 | Loss: 0.5305 | CLoss: 0.3780 | FLoss: 0.3051 | LR: 3.00e-04\n",
            "  Batch 130/902 | Loss: 1.0755 | CLoss: 0.8338 | FLoss: 0.4833 | LR: 3.00e-04\n",
            "  Batch 140/902 | Loss: 0.4517 | CLoss: 0.3011 | FLoss: 0.3011 | LR: 3.00e-04\n",
            "  Batch 150/902 | Loss: 1.0048 | CLoss: 0.7052 | FLoss: 0.5992 | LR: 3.00e-04\n",
            "  Batch 160/902 | Loss: 1.1362 | CLoss: 0.8111 | FLoss: 0.6503 | LR: 3.00e-04\n",
            "  Batch 170/902 | Loss: 0.7180 | CLoss: 0.4377 | FLoss: 0.5605 | LR: 3.00e-04\n",
            "  Batch 180/902 | Loss: 0.7535 | CLoss: 0.5270 | FLoss: 0.4529 | LR: 3.00e-04\n",
            "  Batch 190/902 | Loss: 1.1545 | CLoss: 0.9382 | FLoss: 0.4326 | LR: 3.00e-04\n",
            "  Batch 200/902 | Loss: 0.5285 | CLoss: 0.4345 | FLoss: 0.1880 | LR: 3.00e-04\n",
            "  Batch 210/902 | Loss: 0.8053 | CLoss: 0.6086 | FLoss: 0.3934 | LR: 3.00e-04\n",
            "  Batch 220/902 | Loss: 0.6287 | CLoss: 0.5197 | FLoss: 0.2181 | LR: 3.00e-04\n",
            "  Batch 230/902 | Loss: 0.4894 | CLoss: 0.3622 | FLoss: 0.2543 | LR: 3.00e-04\n",
            "  Batch 240/902 | Loss: 0.7018 | CLoss: 0.5243 | FLoss: 0.3551 | LR: 3.00e-04\n",
            "  Batch 250/902 | Loss: 0.5643 | CLoss: 0.4685 | FLoss: 0.1915 | LR: 3.00e-04\n",
            "  Batch 260/902 | Loss: 0.6025 | CLoss: 0.2713 | FLoss: 0.6624 | LR: 3.00e-04\n",
            "  Batch 270/902 | Loss: 0.5156 | CLoss: 0.3742 | FLoss: 0.2829 | LR: 3.00e-04\n",
            "  Batch 280/902 | Loss: 1.0860 | CLoss: 0.8544 | FLoss: 0.4632 | LR: 3.00e-04\n",
            "  Batch 290/902 | Loss: 0.7786 | CLoss: 0.6083 | FLoss: 0.3406 | LR: 3.00e-04\n",
            "  Batch 300/902 | Loss: 0.5237 | CLoss: 0.3605 | FLoss: 0.3265 | LR: 3.00e-04\n",
            "  Batch 310/902 | Loss: 0.9485 | CLoss: 0.7568 | FLoss: 0.3834 | LR: 3.00e-04\n",
            "  Batch 320/902 | Loss: 0.4644 | CLoss: 0.2802 | FLoss: 0.3684 | LR: 3.00e-04\n",
            "  Batch 330/902 | Loss: 0.4913 | CLoss: 0.3914 | FLoss: 0.1997 | LR: 3.00e-04\n",
            "  Batch 340/902 | Loss: 0.9140 | CLoss: 0.6300 | FLoss: 0.5681 | LR: 3.00e-04\n",
            "  Batch 350/902 | Loss: 0.7260 | CLoss: 0.5670 | FLoss: 0.3181 | LR: 3.00e-04\n",
            "  Batch 360/902 | Loss: 1.0406 | CLoss: 0.7409 | FLoss: 0.5995 | LR: 3.00e-04\n",
            "  Batch 370/902 | Loss: 0.6002 | CLoss: 0.4085 | FLoss: 0.3834 | LR: 3.00e-04\n",
            "  Batch 380/902 | Loss: 1.1063 | CLoss: 0.8655 | FLoss: 0.4817 | LR: 3.00e-04\n",
            "  Batch 390/902 | Loss: 0.8374 | CLoss: 0.6434 | FLoss: 0.3881 | LR: 3.00e-04\n",
            "  Batch 400/902 | Loss: 0.7396 | CLoss: 0.5465 | FLoss: 0.3861 | LR: 3.00e-04\n",
            "  Batch 410/902 | Loss: 0.7356 | CLoss: 0.5949 | FLoss: 0.2813 | LR: 3.00e-04\n",
            "  Batch 420/902 | Loss: 0.9739 | CLoss: 0.7255 | FLoss: 0.4967 | LR: 3.00e-04\n",
            "  Batch 430/902 | Loss: 0.5468 | CLoss: 0.4414 | FLoss: 0.2109 | LR: 3.00e-04\n",
            "  Batch 440/902 | Loss: 0.8111 | CLoss: 0.6373 | FLoss: 0.3476 | LR: 3.00e-04\n",
            "  Batch 450/902 | Loss: 0.4513 | CLoss: 0.3926 | FLoss: 0.1174 | LR: 3.00e-04\n",
            "  Batch 460/902 | Loss: 0.6150 | CLoss: 0.4755 | FLoss: 0.2790 | LR: 3.00e-04\n",
            "  Batch 470/902 | Loss: 0.6245 | CLoss: 0.5227 | FLoss: 0.2036 | LR: 3.00e-04\n",
            "  Batch 480/902 | Loss: 0.5180 | CLoss: 0.3320 | FLoss: 0.3720 | LR: 3.00e-04\n",
            "  Batch 490/902 | Loss: 0.9359 | CLoss: 0.6640 | FLoss: 0.5437 | LR: 3.00e-04\n",
            "  Batch 500/902 | Loss: 0.6820 | CLoss: 0.4593 | FLoss: 0.4455 | LR: 3.00e-04\n",
            "  Batch 510/902 | Loss: 0.4956 | CLoss: 0.3556 | FLoss: 0.2800 | LR: 3.00e-04\n",
            "  Batch 520/902 | Loss: 0.6853 | CLoss: 0.5395 | FLoss: 0.2916 | LR: 3.00e-04\n",
            "  Batch 530/902 | Loss: 0.5352 | CLoss: 0.3771 | FLoss: 0.3162 | LR: 3.00e-04\n",
            "  Batch 540/902 | Loss: 0.6548 | CLoss: 0.5243 | FLoss: 0.2608 | LR: 3.00e-04\n",
            "  Batch 550/902 | Loss: 0.8255 | CLoss: 0.6335 | FLoss: 0.3840 | LR: 3.00e-04\n",
            "  Batch 560/902 | Loss: 0.6739 | CLoss: 0.5063 | FLoss: 0.3352 | LR: 3.00e-04\n",
            "  Batch 570/902 | Loss: 0.9825 | CLoss: 0.7112 | FLoss: 0.5427 | LR: 3.00e-04\n",
            "  Batch 580/902 | Loss: 0.9762 | CLoss: 0.8175 | FLoss: 0.3174 | LR: 3.00e-04\n",
            "  Batch 590/902 | Loss: 0.5122 | CLoss: 0.3516 | FLoss: 0.3212 | LR: 3.00e-04\n",
            "  Batch 600/902 | Loss: 0.5472 | CLoss: 0.3469 | FLoss: 0.4006 | LR: 3.00e-04\n",
            "  Batch 610/902 | Loss: 0.9570 | CLoss: 0.7909 | FLoss: 0.3320 | LR: 3.00e-04\n",
            "  Batch 620/902 | Loss: 0.8262 | CLoss: 0.6068 | FLoss: 0.4387 | LR: 3.00e-04\n",
            "  Batch 630/902 | Loss: 0.8393 | CLoss: 0.6786 | FLoss: 0.3213 | LR: 3.00e-04\n",
            "  Batch 640/902 | Loss: 0.8296 | CLoss: 0.6670 | FLoss: 0.3252 | LR: 3.00e-04\n",
            "  Batch 650/902 | Loss: 0.6873 | CLoss: 0.4782 | FLoss: 0.4182 | LR: 3.00e-04\n",
            "  Batch 660/902 | Loss: 0.7194 | CLoss: 0.5702 | FLoss: 0.2985 | LR: 3.00e-04\n",
            "  Batch 670/902 | Loss: 0.7036 | CLoss: 0.5242 | FLoss: 0.3587 | LR: 3.00e-04\n",
            "  Batch 680/902 | Loss: 0.7247 | CLoss: 0.5266 | FLoss: 0.3962 | LR: 3.00e-04\n",
            "  Batch 690/902 | Loss: 0.9215 | CLoss: 0.6472 | FLoss: 0.5486 | LR: 3.00e-04\n",
            "  Batch 700/902 | Loss: 1.0842 | CLoss: 0.9160 | FLoss: 0.3363 | LR: 3.00e-04\n",
            "  Batch 710/902 | Loss: 0.9839 | CLoss: 0.8131 | FLoss: 0.3417 | LR: 3.00e-04\n",
            "  Batch 720/902 | Loss: 0.4537 | CLoss: 0.2533 | FLoss: 0.4010 | LR: 3.00e-04\n",
            "  Batch 730/902 | Loss: 0.6498 | CLoss: 0.5102 | FLoss: 0.2792 | LR: 3.00e-04\n",
            "  Batch 740/902 | Loss: 0.5333 | CLoss: 0.3653 | FLoss: 0.3359 | LR: 3.00e-04\n",
            "  Batch 750/902 | Loss: 1.1371 | CLoss: 0.9435 | FLoss: 0.3871 | LR: 3.00e-04\n",
            "  Batch 760/902 | Loss: 0.6811 | CLoss: 0.4872 | FLoss: 0.3877 | LR: 3.00e-04\n",
            "  Batch 770/902 | Loss: 0.6210 | CLoss: 0.4810 | FLoss: 0.2799 | LR: 3.00e-04\n",
            "  Batch 780/902 | Loss: 0.3530 | CLoss: 0.2990 | FLoss: 0.1080 | LR: 3.00e-04\n",
            "  Batch 790/902 | Loss: 0.8512 | CLoss: 0.6820 | FLoss: 0.3384 | LR: 3.00e-04\n",
            "  Batch 800/902 | Loss: 1.0442 | CLoss: 0.8101 | FLoss: 0.4683 | LR: 3.00e-04\n",
            "  Batch 810/902 | Loss: 1.0182 | CLoss: 0.8363 | FLoss: 0.3638 | LR: 3.00e-04\n",
            "  Batch 820/902 | Loss: 0.7938 | CLoss: 0.5428 | FLoss: 0.5019 | LR: 3.00e-04\n",
            "  Batch 830/902 | Loss: 0.6509 | CLoss: 0.5167 | FLoss: 0.2683 | LR: 3.00e-04\n",
            "  Batch 840/902 | Loss: 0.7031 | CLoss: 0.5149 | FLoss: 0.3764 | LR: 3.00e-04\n",
            "  Batch 850/902 | Loss: 0.8100 | CLoss: 0.6632 | FLoss: 0.2935 | LR: 3.00e-04\n",
            "  Batch 860/902 | Loss: 0.7840 | CLoss: 0.6219 | FLoss: 0.3242 | LR: 3.00e-04\n",
            "  Batch 870/902 | Loss: 0.7573 | CLoss: 0.5721 | FLoss: 0.3704 | LR: 3.00e-04\n",
            "  Batch 880/902 | Loss: 0.5122 | CLoss: 0.3902 | FLoss: 0.2441 | LR: 3.00e-04\n",
            "  Batch 890/902 | Loss: 0.7470 | CLoss: 0.5938 | FLoss: 0.3065 | LR: 3.00e-04\n",
            "  Batch 900/902 | Loss: 0.7707 | CLoss: 0.5945 | FLoss: 0.3524 | LR: 3.00e-04\n",
            "  Batch 902/902 | Loss: 1.9702 | CLoss: 0.0000 | FLoss: 3.9404 | LR: 3.00e-04\n",
            "\n",
            "  Training Summary | Epoch 1\n",
            "  Avg Loss: 0.8038\n",
            "  Last Batch Loss: 1.9702\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/98 | Loss: 0.6330 | Batch Acc: 74.14%\n",
            "    Val Batch 010/98 | Loss: 1.0665 | Batch Acc: 68.97%\n",
            "    Val Batch 015/98 | Loss: 1.3417 | Batch Acc: 67.24%\n",
            "    Val Batch 020/98 | Loss: 0.7151 | Batch Acc: 86.21%\n",
            "    Val Batch 025/98 | Loss: 0.8334 | Batch Acc: 75.86%\n",
            "    Val Batch 030/98 | Loss: 0.6494 | Batch Acc: 77.59%\n",
            "    Val Batch 035/98 | Loss: 0.9484 | Batch Acc: 70.69%\n",
            "    Val Batch 040/98 | Loss: 0.8094 | Batch Acc: 46.55%\n",
            "    Val Batch 045/98 | Loss: 1.5304 | Batch Acc: 67.24%\n",
            "    Val Batch 050/98 | Loss: 1.4958 | Batch Acc: 62.07%\n",
            "    Val Batch 055/98 | Loss: 0.6136 | Batch Acc: 82.76%\n",
            "    Val Batch 060/98 | Loss: 0.4327 | Batch Acc: 89.66%\n",
            "    Val Batch 065/98 | Loss: 0.5655 | Batch Acc: 84.48%\n",
            "    Val Batch 070/98 | Loss: 0.2482 | Batch Acc: 91.38%\n",
            "    Val Batch 075/98 | Loss: 0.0540 | Batch Acc: 98.28%\n",
            "    Val Batch 080/98 | Loss: 0.2010 | Batch Acc: 93.10%\n",
            "    Val Batch 085/98 | Loss: 0.2673 | Batch Acc: 93.10%\n",
            "    Val Batch 090/98 | Loss: 0.3030 | Batch Acc: 87.93%\n",
            "    Val Batch 095/98 | Loss: 0.5177 | Batch Acc: 86.21%\n",
            "    Val Batch 098/98 | Loss: 0.0310 | Batch Acc: 100.00%\n",
            "\n",
            "  Validation Summary | Epoch 1\n",
            "  Avg Loss: 0.6928 | Accuracy: 78.51%\n",
            "  Current Best Acc: 78.51%\n",
            "\n",
            "Epoch 2/5\n",
            "  Batch 010/902 | Loss: 0.7386 | CLoss: 0.5671 | FLoss: 0.3432 | LR: 2.71e-04\n",
            "  Batch 020/902 | Loss: 0.6947 | CLoss: 0.4811 | FLoss: 0.4271 | LR: 2.71e-04\n",
            "  Batch 030/902 | Loss: 0.6495 | CLoss: 0.4614 | FLoss: 0.3762 | LR: 2.71e-04\n",
            "  Batch 040/902 | Loss: 0.7795 | CLoss: 0.6383 | FLoss: 0.2823 | LR: 2.71e-04\n",
            "  Batch 050/902 | Loss: 0.5104 | CLoss: 0.3543 | FLoss: 0.3121 | LR: 2.71e-04\n",
            "  Batch 060/902 | Loss: 0.9421 | CLoss: 0.6981 | FLoss: 0.4880 | LR: 2.71e-04\n",
            "  Batch 070/902 | Loss: 0.7422 | CLoss: 0.5486 | FLoss: 0.3872 | LR: 2.71e-04\n",
            "  Batch 080/902 | Loss: 0.5605 | CLoss: 0.4428 | FLoss: 0.2354 | LR: 2.71e-04\n",
            "  Batch 090/902 | Loss: 0.6903 | CLoss: 0.5382 | FLoss: 0.3042 | LR: 2.71e-04\n",
            "  Batch 100/902 | Loss: 1.0902 | CLoss: 0.8297 | FLoss: 0.5211 | LR: 2.71e-04\n",
            "  Batch 110/902 | Loss: 0.9269 | CLoss: 0.6855 | FLoss: 0.4827 | LR: 2.71e-04\n",
            "  Batch 120/902 | Loss: 0.9109 | CLoss: 0.6843 | FLoss: 0.4532 | LR: 2.71e-04\n",
            "  Batch 130/902 | Loss: 0.8518 | CLoss: 0.7136 | FLoss: 0.2764 | LR: 2.71e-04\n",
            "  Batch 140/902 | Loss: 0.5976 | CLoss: 0.4117 | FLoss: 0.3718 | LR: 2.71e-04\n",
            "  Batch 150/902 | Loss: 0.7436 | CLoss: 0.5742 | FLoss: 0.3387 | LR: 2.71e-04\n",
            "  Batch 160/902 | Loss: 0.4055 | CLoss: 0.3530 | FLoss: 0.1050 | LR: 2.71e-04\n",
            "  Batch 170/902 | Loss: 0.9246 | CLoss: 0.7398 | FLoss: 0.3695 | LR: 2.71e-04\n",
            "  Batch 180/902 | Loss: 0.7279 | CLoss: 0.5480 | FLoss: 0.3598 | LR: 2.71e-04\n",
            "  Batch 190/902 | Loss: 0.8139 | CLoss: 0.4908 | FLoss: 0.6462 | LR: 2.71e-04\n",
            "  Batch 200/902 | Loss: 1.0382 | CLoss: 0.8135 | FLoss: 0.4495 | LR: 2.71e-04\n",
            "  Batch 210/902 | Loss: 0.6423 | CLoss: 0.4706 | FLoss: 0.3433 | LR: 2.71e-04\n",
            "  Batch 220/902 | Loss: 0.8320 | CLoss: 0.6871 | FLoss: 0.2897 | LR: 2.71e-04\n",
            "  Batch 230/902 | Loss: 0.6134 | CLoss: 0.4669 | FLoss: 0.2931 | LR: 2.71e-04\n",
            "  Batch 240/902 | Loss: 0.8181 | CLoss: 0.5520 | FLoss: 0.5321 | LR: 2.71e-04\n",
            "  Batch 250/902 | Loss: 0.8621 | CLoss: 0.6928 | FLoss: 0.3386 | LR: 2.71e-04\n",
            "  Batch 260/902 | Loss: 0.4080 | CLoss: 0.2781 | FLoss: 0.2597 | LR: 2.71e-04\n",
            "  Batch 270/902 | Loss: 0.5300 | CLoss: 0.3606 | FLoss: 0.3389 | LR: 2.71e-04\n",
            "  Batch 280/902 | Loss: 0.6660 | CLoss: 0.4131 | FLoss: 0.5058 | LR: 2.71e-04\n",
            "  Batch 290/902 | Loss: 0.5837 | CLoss: 0.4725 | FLoss: 0.2224 | LR: 2.71e-04\n",
            "  Batch 300/902 | Loss: 0.6937 | CLoss: 0.5101 | FLoss: 0.3671 | LR: 2.71e-04\n",
            "  Batch 310/902 | Loss: 0.9960 | CLoss: 0.8881 | FLoss: 0.2158 | LR: 2.71e-04\n",
            "  Batch 320/902 | Loss: 0.9928 | CLoss: 0.8375 | FLoss: 0.3107 | LR: 2.71e-04\n",
            "  Batch 330/902 | Loss: 0.9369 | CLoss: 0.7636 | FLoss: 0.3465 | LR: 2.71e-04\n",
            "  Batch 340/902 | Loss: 0.6591 | CLoss: 0.4919 | FLoss: 0.3345 | LR: 2.71e-04\n",
            "  Batch 350/902 | Loss: 0.8483 | CLoss: 0.7076 | FLoss: 0.2814 | LR: 2.71e-04\n",
            "  Batch 360/902 | Loss: 0.9182 | CLoss: 0.7151 | FLoss: 0.4061 | LR: 2.71e-04\n",
            "  Batch 370/902 | Loss: 0.9699 | CLoss: 0.7694 | FLoss: 0.4011 | LR: 2.71e-04\n",
            "  Batch 380/902 | Loss: 0.6762 | CLoss: 0.4965 | FLoss: 0.3593 | LR: 2.71e-04\n",
            "  Batch 390/902 | Loss: 0.4725 | CLoss: 0.2757 | FLoss: 0.3936 | LR: 2.71e-04\n",
            "  Batch 400/902 | Loss: 0.7945 | CLoss: 0.6600 | FLoss: 0.2690 | LR: 2.71e-04\n",
            "  Batch 410/902 | Loss: 1.0900 | CLoss: 0.9282 | FLoss: 0.3236 | LR: 2.71e-04\n",
            "  Batch 420/902 | Loss: 1.0560 | CLoss: 0.7980 | FLoss: 0.5159 | LR: 2.71e-04\n",
            "  Batch 430/902 | Loss: 1.0384 | CLoss: 0.8443 | FLoss: 0.3883 | LR: 2.71e-04\n",
            "  Batch 440/902 | Loss: 1.0942 | CLoss: 0.9112 | FLoss: 0.3660 | LR: 2.71e-04\n",
            "  Batch 450/902 | Loss: 0.7011 | CLoss: 0.5022 | FLoss: 0.3978 | LR: 2.71e-04\n",
            "  Batch 460/902 | Loss: 0.7204 | CLoss: 0.5156 | FLoss: 0.4097 | LR: 2.71e-04\n",
            "  Batch 470/902 | Loss: 0.4487 | CLoss: 0.2932 | FLoss: 0.3110 | LR: 2.71e-04\n",
            "  Batch 480/902 | Loss: 0.3331 | CLoss: 0.2870 | FLoss: 0.0922 | LR: 2.71e-04\n",
            "  Batch 490/902 | Loss: 1.0706 | CLoss: 0.8305 | FLoss: 0.4803 | LR: 2.71e-04\n",
            "  Batch 500/902 | Loss: 0.6885 | CLoss: 0.5701 | FLoss: 0.2366 | LR: 2.71e-04\n",
            "  Batch 510/902 | Loss: 0.5664 | CLoss: 0.4529 | FLoss: 0.2271 | LR: 2.71e-04\n",
            "  Batch 520/902 | Loss: 0.4884 | CLoss: 0.2921 | FLoss: 0.3926 | LR: 2.71e-04\n",
            "  Batch 530/902 | Loss: 0.7032 | CLoss: 0.4308 | FLoss: 0.5450 | LR: 2.71e-04\n",
            "  Batch 540/902 | Loss: 0.9290 | CLoss: 0.6610 | FLoss: 0.5361 | LR: 2.71e-04\n",
            "  Batch 550/902 | Loss: 0.9103 | CLoss: 0.7904 | FLoss: 0.2399 | LR: 2.71e-04\n",
            "  Batch 560/902 | Loss: 1.0260 | CLoss: 0.7313 | FLoss: 0.5894 | LR: 2.71e-04\n",
            "  Batch 570/902 | Loss: 1.0081 | CLoss: 0.7616 | FLoss: 0.4930 | LR: 2.71e-04\n",
            "  Batch 580/902 | Loss: 0.5036 | CLoss: 0.3698 | FLoss: 0.2675 | LR: 2.71e-04\n",
            "  Batch 590/902 | Loss: 0.7526 | CLoss: 0.5963 | FLoss: 0.3127 | LR: 2.71e-04\n",
            "  Batch 600/902 | Loss: 0.4682 | CLoss: 0.3632 | FLoss: 0.2100 | LR: 2.71e-04\n",
            "  Batch 610/902 | Loss: 0.7251 | CLoss: 0.5979 | FLoss: 0.2543 | LR: 2.71e-04\n",
            "  Batch 620/902 | Loss: 0.9773 | CLoss: 0.7725 | FLoss: 0.4096 | LR: 2.71e-04\n",
            "  Batch 630/902 | Loss: 0.5717 | CLoss: 0.4084 | FLoss: 0.3266 | LR: 2.71e-04\n",
            "  Batch 640/902 | Loss: 1.0129 | CLoss: 0.7527 | FLoss: 0.5205 | LR: 2.71e-04\n",
            "  Batch 650/902 | Loss: 1.0146 | CLoss: 0.8358 | FLoss: 0.3577 | LR: 2.71e-04\n",
            "  Batch 660/902 | Loss: 0.9832 | CLoss: 0.8116 | FLoss: 0.3432 | LR: 2.71e-04\n",
            "  Batch 670/902 | Loss: 0.5550 | CLoss: 0.4164 | FLoss: 0.2772 | LR: 2.71e-04\n",
            "  Batch 680/902 | Loss: 0.8794 | CLoss: 0.7001 | FLoss: 0.3585 | LR: 2.71e-04\n",
            "  Batch 690/902 | Loss: 0.7057 | CLoss: 0.4949 | FLoss: 0.4217 | LR: 2.71e-04\n",
            "  Batch 700/902 | Loss: 1.0970 | CLoss: 0.9138 | FLoss: 0.3663 | LR: 2.71e-04\n",
            "  Batch 710/902 | Loss: 1.0458 | CLoss: 0.8209 | FLoss: 0.4499 | LR: 2.71e-04\n",
            "  Batch 720/902 | Loss: 1.1366 | CLoss: 0.9101 | FLoss: 0.4529 | LR: 2.71e-04\n",
            "  Batch 730/902 | Loss: 0.6131 | CLoss: 0.4772 | FLoss: 0.2718 | LR: 2.71e-04\n",
            "  Batch 740/902 | Loss: 0.6254 | CLoss: 0.4885 | FLoss: 0.2738 | LR: 2.71e-04\n",
            "  Batch 750/902 | Loss: 1.1792 | CLoss: 1.0792 | FLoss: 0.2000 | LR: 2.71e-04\n",
            "  Batch 760/902 | Loss: 1.2226 | CLoss: 0.9305 | FLoss: 0.5841 | LR: 2.71e-04\n",
            "  Batch 770/902 | Loss: 1.0452 | CLoss: 0.7367 | FLoss: 0.6171 | LR: 2.71e-04\n",
            "  Batch 780/902 | Loss: 1.0037 | CLoss: 0.7845 | FLoss: 0.4383 | LR: 2.71e-04\n",
            "  Batch 790/902 | Loss: 0.9115 | CLoss: 0.7896 | FLoss: 0.2439 | LR: 2.71e-04\n",
            "  Batch 800/902 | Loss: 1.1134 | CLoss: 0.8841 | FLoss: 0.4586 | LR: 2.71e-04\n",
            "  Batch 810/902 | Loss: 0.9825 | CLoss: 0.6449 | FLoss: 0.6753 | LR: 2.71e-04\n",
            "  Batch 820/902 | Loss: 0.7264 | CLoss: 0.6134 | FLoss: 0.2260 | LR: 2.71e-04\n",
            "  Batch 830/902 | Loss: 0.4984 | CLoss: 0.3921 | FLoss: 0.2125 | LR: 2.71e-04\n",
            "  Batch 840/902 | Loss: 0.6924 | CLoss: 0.4786 | FLoss: 0.4276 | LR: 2.71e-04\n",
            "  Batch 850/902 | Loss: 0.5823 | CLoss: 0.4458 | FLoss: 0.2729 | LR: 2.71e-04\n",
            "  Batch 860/902 | Loss: 0.9162 | CLoss: 0.6845 | FLoss: 0.4634 | LR: 2.71e-04\n",
            "  Batch 870/902 | Loss: 1.1478 | CLoss: 0.9136 | FLoss: 0.4684 | LR: 2.71e-04\n",
            "  Batch 880/902 | Loss: 0.9614 | CLoss: 0.8373 | FLoss: 0.2481 | LR: 2.71e-04\n",
            "  Batch 890/902 | Loss: 0.9810 | CLoss: 0.7778 | FLoss: 0.4063 | LR: 2.71e-04\n",
            "  Batch 900/902 | Loss: 0.5888 | CLoss: 0.4451 | FLoss: 0.2874 | LR: 2.71e-04\n",
            "  Batch 902/902 | Loss: 1.6668 | CLoss: 0.0000 | FLoss: 3.3335 | LR: 2.71e-04\n",
            "\n",
            "  Training Summary | Epoch 2\n",
            "  Avg Loss: 0.7743\n",
            "  Last Batch Loss: 1.6668\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/98 | Loss: 0.6719 | Batch Acc: 70.69%\n",
            "    Val Batch 010/98 | Loss: 0.4906 | Batch Acc: 81.03%\n",
            "    Val Batch 015/98 | Loss: 0.4417 | Batch Acc: 81.03%\n",
            "    Val Batch 020/98 | Loss: 1.0938 | Batch Acc: 27.59%\n",
            "    Val Batch 025/98 | Loss: 1.0210 | Batch Acc: 65.52%\n",
            "    Val Batch 030/98 | Loss: 1.0747 | Batch Acc: 70.69%\n",
            "    Val Batch 035/98 | Loss: 0.5234 | Batch Acc: 82.76%\n",
            "    Val Batch 040/98 | Loss: 0.5114 | Batch Acc: 96.55%\n",
            "    Val Batch 045/98 | Loss: 1.9332 | Batch Acc: 51.72%\n",
            "    Val Batch 050/98 | Loss: 1.3449 | Batch Acc: 63.79%\n",
            "    Val Batch 055/98 | Loss: 0.8253 | Batch Acc: 68.97%\n",
            "    Val Batch 060/98 | Loss: 0.3806 | Batch Acc: 89.66%\n",
            "    Val Batch 065/98 | Loss: 0.2996 | Batch Acc: 89.66%\n",
            "    Val Batch 070/98 | Loss: 0.3987 | Batch Acc: 84.48%\n",
            "    Val Batch 075/98 | Loss: 0.4543 | Batch Acc: 86.21%\n",
            "    Val Batch 080/98 | Loss: 0.2039 | Batch Acc: 93.10%\n",
            "    Val Batch 085/98 | Loss: 0.0997 | Batch Acc: 96.55%\n",
            "    Val Batch 090/98 | Loss: 0.1146 | Batch Acc: 96.55%\n",
            "    Val Batch 095/98 | Loss: 0.1474 | Batch Acc: 93.10%\n",
            "    Val Batch 098/98 | Loss: 0.4346 | Batch Acc: 87.88%\n",
            "\n",
            "  Validation Summary | Epoch 2\n",
            "  Avg Loss: 0.6415 | Accuracy: 78.28%\n",
            "  Current Best Acc: 78.51%\n",
            "\n",
            "Epoch 3/5\n",
            "  Batch 010/902 | Loss: 0.4769 | CLoss: 0.2929 | FLoss: 0.3680 | LR: 1.96e-04\n",
            "  Batch 020/902 | Loss: 0.5892 | CLoss: 0.4605 | FLoss: 0.2574 | LR: 1.96e-04\n",
            "  Batch 030/902 | Loss: 0.8649 | CLoss: 0.7026 | FLoss: 0.3246 | LR: 1.96e-04\n",
            "  Batch 040/902 | Loss: 0.6002 | CLoss: 0.4875 | FLoss: 0.2253 | LR: 1.96e-04\n",
            "  Batch 050/902 | Loss: 0.5775 | CLoss: 0.4192 | FLoss: 0.3166 | LR: 1.96e-04\n",
            "  Batch 060/902 | Loss: 0.5571 | CLoss: 0.4429 | FLoss: 0.2284 | LR: 1.96e-04\n",
            "  Batch 070/902 | Loss: 0.6348 | CLoss: 0.4929 | FLoss: 0.2837 | LR: 1.96e-04\n",
            "  Batch 080/902 | Loss: 0.7181 | CLoss: 0.5875 | FLoss: 0.2612 | LR: 1.96e-04\n",
            "  Batch 090/902 | Loss: 0.9120 | CLoss: 0.7797 | FLoss: 0.2645 | LR: 1.96e-04\n",
            "  Batch 100/902 | Loss: 0.5124 | CLoss: 0.4033 | FLoss: 0.2183 | LR: 1.96e-04\n",
            "  Batch 110/902 | Loss: 0.8220 | CLoss: 0.6536 | FLoss: 0.3367 | LR: 1.96e-04\n",
            "  Batch 120/902 | Loss: 0.8530 | CLoss: 0.6047 | FLoss: 0.4966 | LR: 1.96e-04\n",
            "  Batch 130/902 | Loss: 0.5882 | CLoss: 0.4156 | FLoss: 0.3452 | LR: 1.96e-04\n",
            "  Batch 140/902 | Loss: 0.5274 | CLoss: 0.3707 | FLoss: 0.3134 | LR: 1.96e-04\n",
            "  Batch 150/902 | Loss: 0.9549 | CLoss: 0.7016 | FLoss: 0.5067 | LR: 1.96e-04\n",
            "  Batch 160/902 | Loss: 0.5177 | CLoss: 0.3545 | FLoss: 0.3263 | LR: 1.96e-04\n",
            "  Batch 170/902 | Loss: 0.8019 | CLoss: 0.6004 | FLoss: 0.4030 | LR: 1.96e-04\n",
            "  Batch 180/902 | Loss: 0.8281 | CLoss: 0.7157 | FLoss: 0.2249 | LR: 1.96e-04\n",
            "  Batch 190/902 | Loss: 0.7527 | CLoss: 0.6235 | FLoss: 0.2585 | LR: 1.96e-04\n",
            "  Batch 200/902 | Loss: 0.5443 | CLoss: 0.4178 | FLoss: 0.2530 | LR: 1.96e-04\n",
            "  Batch 210/902 | Loss: 0.6156 | CLoss: 0.4762 | FLoss: 0.2789 | LR: 1.96e-04\n",
            "  Batch 220/902 | Loss: 0.7119 | CLoss: 0.5307 | FLoss: 0.3625 | LR: 1.96e-04\n",
            "  Batch 230/902 | Loss: 0.5140 | CLoss: 0.3951 | FLoss: 0.2378 | LR: 1.96e-04\n",
            "  Batch 240/902 | Loss: 0.5246 | CLoss: 0.3654 | FLoss: 0.3182 | LR: 1.96e-04\n",
            "  Batch 250/902 | Loss: 0.8560 | CLoss: 0.7116 | FLoss: 0.2888 | LR: 1.96e-04\n",
            "  Batch 260/902 | Loss: 0.5116 | CLoss: 0.3779 | FLoss: 0.2675 | LR: 1.96e-04\n",
            "  Batch 270/902 | Loss: 0.7071 | CLoss: 0.5883 | FLoss: 0.2375 | LR: 1.96e-04\n",
            "  Batch 280/902 | Loss: 0.4856 | CLoss: 0.3747 | FLoss: 0.2219 | LR: 1.96e-04\n",
            "  Batch 290/902 | Loss: 0.7845 | CLoss: 0.6357 | FLoss: 0.2975 | LR: 1.96e-04\n",
            "  Batch 300/902 | Loss: 0.3655 | CLoss: 0.2144 | FLoss: 0.3023 | LR: 1.96e-04\n",
            "  Batch 310/902 | Loss: 0.7169 | CLoss: 0.6015 | FLoss: 0.2307 | LR: 1.96e-04\n",
            "  Batch 320/902 | Loss: 0.2733 | CLoss: 0.1613 | FLoss: 0.2240 | LR: 1.96e-04\n",
            "  Batch 330/902 | Loss: 0.7725 | CLoss: 0.5185 | FLoss: 0.5079 | LR: 1.96e-04\n",
            "  Batch 340/902 | Loss: 0.3432 | CLoss: 0.2465 | FLoss: 0.1934 | LR: 1.96e-04\n",
            "  Batch 350/902 | Loss: 0.4264 | CLoss: 0.2959 | FLoss: 0.2610 | LR: 1.96e-04\n",
            "  Batch 360/902 | Loss: 0.8381 | CLoss: 0.6720 | FLoss: 0.3322 | LR: 1.96e-04\n",
            "  Batch 370/902 | Loss: 0.9587 | CLoss: 0.7198 | FLoss: 0.4780 | LR: 1.96e-04\n",
            "  Batch 380/902 | Loss: 0.9456 | CLoss: 0.7569 | FLoss: 0.3772 | LR: 1.96e-04\n",
            "  Batch 390/902 | Loss: 0.6746 | CLoss: 0.4846 | FLoss: 0.3802 | LR: 1.96e-04\n",
            "  Batch 400/902 | Loss: 0.6085 | CLoss: 0.5073 | FLoss: 0.2024 | LR: 1.96e-04\n",
            "  Batch 410/902 | Loss: 0.8788 | CLoss: 0.7396 | FLoss: 0.2785 | LR: 1.96e-04\n",
            "  Batch 420/902 | Loss: 0.5213 | CLoss: 0.3876 | FLoss: 0.2674 | LR: 1.96e-04\n",
            "  Batch 430/902 | Loss: 0.8135 | CLoss: 0.6877 | FLoss: 0.2515 | LR: 1.96e-04\n",
            "  Batch 440/902 | Loss: 0.6144 | CLoss: 0.5037 | FLoss: 0.2213 | LR: 1.96e-04\n",
            "  Batch 450/902 | Loss: 0.8993 | CLoss: 0.6986 | FLoss: 0.4013 | LR: 1.96e-04\n",
            "  Batch 460/902 | Loss: 0.4147 | CLoss: 0.3189 | FLoss: 0.1916 | LR: 1.96e-04\n",
            "  Batch 470/902 | Loss: 0.9847 | CLoss: 0.7535 | FLoss: 0.4622 | LR: 1.96e-04\n",
            "  Batch 480/902 | Loss: 1.0087 | CLoss: 0.8847 | FLoss: 0.2481 | LR: 1.96e-04\n",
            "  Batch 490/902 | Loss: 0.5202 | CLoss: 0.4460 | FLoss: 0.1482 | LR: 1.96e-04\n",
            "  Batch 500/902 | Loss: 0.6530 | CLoss: 0.5660 | FLoss: 0.1739 | LR: 1.96e-04\n",
            "  Batch 510/902 | Loss: 0.4602 | CLoss: 0.3491 | FLoss: 0.2223 | LR: 1.96e-04\n",
            "  Batch 520/902 | Loss: 0.9625 | CLoss: 0.6357 | FLoss: 0.6535 | LR: 1.96e-04\n",
            "  Batch 530/902 | Loss: 1.0229 | CLoss: 0.7558 | FLoss: 0.5342 | LR: 1.96e-04\n",
            "  Batch 540/902 | Loss: 0.7687 | CLoss: 0.5441 | FLoss: 0.4493 | LR: 1.96e-04\n",
            "  Batch 550/902 | Loss: 0.6188 | CLoss: 0.4756 | FLoss: 0.2863 | LR: 1.96e-04\n",
            "  Batch 560/902 | Loss: 0.9454 | CLoss: 0.7662 | FLoss: 0.3585 | LR: 1.96e-04\n",
            "  Batch 570/902 | Loss: 0.8465 | CLoss: 0.6412 | FLoss: 0.4106 | LR: 1.96e-04\n",
            "  Batch 580/902 | Loss: 0.6677 | CLoss: 0.5274 | FLoss: 0.2807 | LR: 1.96e-04\n",
            "  Batch 590/902 | Loss: 0.8613 | CLoss: 0.6478 | FLoss: 0.4270 | LR: 1.96e-04\n",
            "  Batch 600/902 | Loss: 0.5723 | CLoss: 0.4687 | FLoss: 0.2073 | LR: 1.96e-04\n",
            "  Batch 610/902 | Loss: 0.7999 | CLoss: 0.6484 | FLoss: 0.3031 | LR: 1.96e-04\n",
            "  Batch 620/902 | Loss: 0.2316 | CLoss: 0.1478 | FLoss: 0.1676 | LR: 1.96e-04\n",
            "  Batch 630/902 | Loss: 0.6240 | CLoss: 0.4697 | FLoss: 0.3085 | LR: 1.96e-04\n",
            "  Batch 640/902 | Loss: 0.6460 | CLoss: 0.5186 | FLoss: 0.2548 | LR: 1.96e-04\n",
            "  Batch 650/902 | Loss: 0.8199 | CLoss: 0.6671 | FLoss: 0.3057 | LR: 1.96e-04\n",
            "  Batch 660/902 | Loss: 0.8632 | CLoss: 0.7367 | FLoss: 0.2530 | LR: 1.96e-04\n",
            "  Batch 670/902 | Loss: 0.5081 | CLoss: 0.3046 | FLoss: 0.4069 | LR: 1.96e-04\n",
            "  Batch 680/902 | Loss: 0.7745 | CLoss: 0.5961 | FLoss: 0.3567 | LR: 1.96e-04\n",
            "  Batch 690/902 | Loss: 0.8319 | CLoss: 0.7013 | FLoss: 0.2610 | LR: 1.96e-04\n",
            "  Batch 700/902 | Loss: 0.5975 | CLoss: 0.4647 | FLoss: 0.2656 | LR: 1.96e-04\n",
            "  Batch 710/902 | Loss: 0.8307 | CLoss: 0.7196 | FLoss: 0.2221 | LR: 1.96e-04\n",
            "  Batch 720/902 | Loss: 0.3585 | CLoss: 0.2954 | FLoss: 0.1262 | LR: 1.96e-04\n",
            "  Batch 730/902 | Loss: 0.9091 | CLoss: 0.6759 | FLoss: 0.4664 | LR: 1.96e-04\n",
            "  Batch 740/902 | Loss: 0.8348 | CLoss: 0.6892 | FLoss: 0.2911 | LR: 1.96e-04\n",
            "  Batch 750/902 | Loss: 0.2789 | CLoss: 0.2070 | FLoss: 0.1436 | LR: 1.96e-04\n",
            "  Batch 760/902 | Loss: 0.6883 | CLoss: 0.5333 | FLoss: 0.3101 | LR: 1.96e-04\n",
            "  Batch 770/902 | Loss: 0.5243 | CLoss: 0.4168 | FLoss: 0.2149 | LR: 1.96e-04\n",
            "  Batch 780/902 | Loss: 0.6298 | CLoss: 0.4464 | FLoss: 0.3669 | LR: 1.96e-04\n",
            "  Batch 790/902 | Loss: 0.4239 | CLoss: 0.3370 | FLoss: 0.1737 | LR: 1.96e-04\n",
            "  Batch 800/902 | Loss: 0.7371 | CLoss: 0.5893 | FLoss: 0.2956 | LR: 1.96e-04\n",
            "  Batch 810/902 | Loss: 0.3590 | CLoss: 0.2788 | FLoss: 0.1605 | LR: 1.96e-04\n",
            "  Batch 820/902 | Loss: 0.7267 | CLoss: 0.5342 | FLoss: 0.3850 | LR: 1.96e-04\n",
            "  Batch 830/902 | Loss: 0.9038 | CLoss: 0.7477 | FLoss: 0.3123 | LR: 1.96e-04\n",
            "  Batch 840/902 | Loss: 0.5681 | CLoss: 0.4180 | FLoss: 0.3002 | LR: 1.96e-04\n",
            "  Batch 850/902 | Loss: 0.7200 | CLoss: 0.5270 | FLoss: 0.3860 | LR: 1.96e-04\n",
            "  Batch 860/902 | Loss: 0.5263 | CLoss: 0.4559 | FLoss: 0.1408 | LR: 1.96e-04\n",
            "  Batch 870/902 | Loss: 0.7441 | CLoss: 0.6219 | FLoss: 0.2442 | LR: 1.96e-04\n",
            "  Batch 880/902 | Loss: 0.5611 | CLoss: 0.4606 | FLoss: 0.2010 | LR: 1.96e-04\n",
            "  Batch 890/902 | Loss: 0.8832 | CLoss: 0.6517 | FLoss: 0.4629 | LR: 1.96e-04\n",
            "  Batch 900/902 | Loss: 0.6102 | CLoss: 0.4573 | FLoss: 0.3056 | LR: 1.96e-04\n",
            "  Batch 902/902 | Loss: 1.4058 | CLoss: 0.0000 | FLoss: 2.8116 | LR: 1.96e-04\n",
            "\n",
            "  Training Summary | Epoch 3\n",
            "  Avg Loss: 0.7085\n",
            "  Last Batch Loss: 1.4058\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/98 | Loss: 0.9920 | Batch Acc: 68.97%\n",
            "    Val Batch 010/98 | Loss: 0.9597 | Batch Acc: 70.69%\n",
            "    Val Batch 015/98 | Loss: 0.8167 | Batch Acc: 82.76%\n",
            "    Val Batch 020/98 | Loss: 1.2568 | Batch Acc: 29.31%\n",
            "    Val Batch 025/98 | Loss: 1.8613 | Batch Acc: 43.10%\n",
            "    Val Batch 030/98 | Loss: 1.3598 | Batch Acc: 62.07%\n",
            "    Val Batch 035/98 | Loss: 1.3981 | Batch Acc: 67.24%\n",
            "    Val Batch 040/98 | Loss: 0.7858 | Batch Acc: 60.34%\n",
            "    Val Batch 045/98 | Loss: 1.9786 | Batch Acc: 53.45%\n",
            "    Val Batch 050/98 | Loss: 1.7504 | Batch Acc: 63.79%\n",
            "    Val Batch 055/98 | Loss: 1.2718 | Batch Acc: 60.34%\n",
            "    Val Batch 060/98 | Loss: 0.2194 | Batch Acc: 93.10%\n",
            "    Val Batch 065/98 | Loss: 0.3809 | Batch Acc: 87.93%\n",
            "    Val Batch 070/98 | Loss: 0.2794 | Batch Acc: 89.66%\n",
            "    Val Batch 075/98 | Loss: 0.1449 | Batch Acc: 94.83%\n",
            "    Val Batch 080/98 | Loss: 0.1840 | Batch Acc: 93.10%\n",
            "    Val Batch 085/98 | Loss: 0.1711 | Batch Acc: 93.10%\n",
            "    Val Batch 090/98 | Loss: 0.2326 | Batch Acc: 91.38%\n",
            "    Val Batch 095/98 | Loss: 0.3575 | Batch Acc: 86.21%\n",
            "    Val Batch 098/98 | Loss: 0.3054 | Batch Acc: 90.91%\n",
            "\n",
            "  Validation Summary | Epoch 3\n",
            "  Avg Loss: 0.8744 | Accuracy: 71.34%\n",
            "  Current Best Acc: 78.51%\n",
            "\n",
            "Epoch 4/5\n",
            "  Batch 010/902 | Loss: 0.8194 | CLoss: 0.6552 | FLoss: 0.3284 | LR: 1.04e-04\n",
            "  Batch 020/902 | Loss: 0.6603 | CLoss: 0.4913 | FLoss: 0.3381 | LR: 1.04e-04\n",
            "  Batch 030/902 | Loss: 0.6927 | CLoss: 0.5230 | FLoss: 0.3394 | LR: 1.04e-04\n",
            "  Batch 040/902 | Loss: 0.2996 | CLoss: 0.2295 | FLoss: 0.1403 | LR: 1.04e-04\n",
            "  Batch 050/902 | Loss: 0.3873 | CLoss: 0.3034 | FLoss: 0.1677 | LR: 1.04e-04\n",
            "  Batch 060/902 | Loss: 0.5260 | CLoss: 0.4408 | FLoss: 0.1704 | LR: 1.04e-04\n",
            "  Batch 070/902 | Loss: 0.5691 | CLoss: 0.4063 | FLoss: 0.3256 | LR: 1.04e-04\n",
            "  Batch 080/902 | Loss: 0.6828 | CLoss: 0.5204 | FLoss: 0.3247 | LR: 1.04e-04\n",
            "  Batch 090/902 | Loss: 0.6636 | CLoss: 0.5593 | FLoss: 0.2086 | LR: 1.04e-04\n",
            "  Batch 100/902 | Loss: 0.6975 | CLoss: 0.5730 | FLoss: 0.2491 | LR: 1.04e-04\n",
            "  Batch 110/902 | Loss: 0.3999 | CLoss: 0.2825 | FLoss: 0.2349 | LR: 1.04e-04\n",
            "  Batch 120/902 | Loss: 0.6597 | CLoss: 0.5922 | FLoss: 0.1349 | LR: 1.04e-04\n",
            "  Batch 130/902 | Loss: 0.7598 | CLoss: 0.6207 | FLoss: 0.2784 | LR: 1.04e-04\n",
            "  Batch 140/902 | Loss: 0.5823 | CLoss: 0.4621 | FLoss: 0.2405 | LR: 1.04e-04\n",
            "  Batch 150/902 | Loss: 0.4317 | CLoss: 0.3546 | FLoss: 0.1542 | LR: 1.04e-04\n",
            "  Batch 160/902 | Loss: 0.6075 | CLoss: 0.4898 | FLoss: 0.2354 | LR: 1.04e-04\n",
            "  Batch 170/902 | Loss: 0.9445 | CLoss: 0.7650 | FLoss: 0.3591 | LR: 1.04e-04\n",
            "  Batch 180/902 | Loss: 0.4440 | CLoss: 0.3569 | FLoss: 0.1742 | LR: 1.04e-04\n",
            "  Batch 190/902 | Loss: 0.6549 | CLoss: 0.4802 | FLoss: 0.3493 | LR: 1.04e-04\n",
            "  Batch 200/902 | Loss: 0.4844 | CLoss: 0.3645 | FLoss: 0.2398 | LR: 1.04e-04\n",
            "  Batch 210/902 | Loss: 0.3281 | CLoss: 0.2135 | FLoss: 0.2293 | LR: 1.04e-04\n",
            "  Batch 220/902 | Loss: 0.7009 | CLoss: 0.5744 | FLoss: 0.2530 | LR: 1.04e-04\n",
            "  Batch 230/902 | Loss: 0.2727 | CLoss: 0.2106 | FLoss: 0.1242 | LR: 1.04e-04\n",
            "  Batch 240/902 | Loss: 0.3765 | CLoss: 0.2930 | FLoss: 0.1669 | LR: 1.04e-04\n",
            "  Batch 250/902 | Loss: 1.0615 | CLoss: 0.9217 | FLoss: 0.2796 | LR: 1.04e-04\n",
            "  Batch 260/902 | Loss: 0.6511 | CLoss: 0.5404 | FLoss: 0.2213 | LR: 1.04e-04\n",
            "  Batch 270/902 | Loss: 0.6499 | CLoss: 0.5371 | FLoss: 0.2258 | LR: 1.04e-04\n",
            "  Batch 280/902 | Loss: 0.6323 | CLoss: 0.5348 | FLoss: 0.1950 | LR: 1.04e-04\n",
            "  Batch 290/902 | Loss: 0.7254 | CLoss: 0.5887 | FLoss: 0.2734 | LR: 1.04e-04\n",
            "  Batch 300/902 | Loss: 0.7867 | CLoss: 0.5834 | FLoss: 0.4068 | LR: 1.04e-04\n",
            "  Batch 310/902 | Loss: 0.5496 | CLoss: 0.4261 | FLoss: 0.2470 | LR: 1.04e-04\n",
            "  Batch 320/902 | Loss: 0.6064 | CLoss: 0.4303 | FLoss: 0.3522 | LR: 1.04e-04\n",
            "  Batch 330/902 | Loss: 0.6796 | CLoss: 0.5228 | FLoss: 0.3136 | LR: 1.04e-04\n",
            "  Batch 340/902 | Loss: 0.5503 | CLoss: 0.3914 | FLoss: 0.3177 | LR: 1.04e-04\n",
            "  Batch 350/902 | Loss: 0.6307 | CLoss: 0.5146 | FLoss: 0.2323 | LR: 1.04e-04\n",
            "  Batch 360/902 | Loss: 0.3724 | CLoss: 0.2819 | FLoss: 0.1810 | LR: 1.04e-04\n",
            "  Batch 370/902 | Loss: 0.2675 | CLoss: 0.2266 | FLoss: 0.0817 | LR: 1.04e-04\n",
            "  Batch 380/902 | Loss: 0.7157 | CLoss: 0.5903 | FLoss: 0.2507 | LR: 1.04e-04\n",
            "  Batch 390/902 | Loss: 0.7561 | CLoss: 0.6193 | FLoss: 0.2735 | LR: 1.04e-04\n",
            "  Batch 400/902 | Loss: 0.4189 | CLoss: 0.3213 | FLoss: 0.1953 | LR: 1.04e-04\n",
            "  Batch 410/902 | Loss: 0.4994 | CLoss: 0.3551 | FLoss: 0.2887 | LR: 1.04e-04\n",
            "  Batch 420/902 | Loss: 0.7546 | CLoss: 0.5320 | FLoss: 0.4452 | LR: 1.04e-04\n",
            "  Batch 430/902 | Loss: 0.7971 | CLoss: 0.6056 | FLoss: 0.3829 | LR: 1.04e-04\n",
            "  Batch 440/902 | Loss: 0.9151 | CLoss: 0.7519 | FLoss: 0.3265 | LR: 1.04e-04\n",
            "  Batch 450/902 | Loss: 1.0044 | CLoss: 0.8285 | FLoss: 0.3519 | LR: 1.04e-04\n",
            "  Batch 460/902 | Loss: 0.6084 | CLoss: 0.5205 | FLoss: 0.1758 | LR: 1.04e-04\n",
            "  Batch 470/902 | Loss: 0.5208 | CLoss: 0.3192 | FLoss: 0.4033 | LR: 1.04e-04\n",
            "  Batch 480/902 | Loss: 0.6386 | CLoss: 0.4874 | FLoss: 0.3023 | LR: 1.04e-04\n",
            "  Batch 490/902 | Loss: 0.6293 | CLoss: 0.4990 | FLoss: 0.2605 | LR: 1.04e-04\n",
            "  Batch 500/902 | Loss: 0.4470 | CLoss: 0.3403 | FLoss: 0.2134 | LR: 1.04e-04\n",
            "  Batch 510/902 | Loss: 0.9348 | CLoss: 0.6280 | FLoss: 0.6136 | LR: 1.04e-04\n",
            "  Batch 520/902 | Loss: 0.7434 | CLoss: 0.5484 | FLoss: 0.3900 | LR: 1.04e-04\n",
            "  Batch 530/902 | Loss: 0.8494 | CLoss: 0.6715 | FLoss: 0.3557 | LR: 1.04e-04\n",
            "  Batch 540/902 | Loss: 0.7705 | CLoss: 0.6806 | FLoss: 0.1798 | LR: 1.04e-04\n",
            "  Batch 550/902 | Loss: 0.9010 | CLoss: 0.6942 | FLoss: 0.4136 | LR: 1.04e-04\n",
            "  Batch 560/902 | Loss: 0.6246 | CLoss: 0.5077 | FLoss: 0.2337 | LR: 1.04e-04\n",
            "  Batch 570/902 | Loss: 0.5314 | CLoss: 0.4425 | FLoss: 0.1779 | LR: 1.04e-04\n",
            "  Batch 580/902 | Loss: 0.6350 | CLoss: 0.5145 | FLoss: 0.2410 | LR: 1.04e-04\n",
            "  Batch 590/902 | Loss: 0.7786 | CLoss: 0.5632 | FLoss: 0.4308 | LR: 1.04e-04\n",
            "  Batch 600/902 | Loss: 0.7466 | CLoss: 0.6123 | FLoss: 0.2686 | LR: 1.04e-04\n",
            "  Batch 610/902 | Loss: 0.4393 | CLoss: 0.3850 | FLoss: 0.1085 | LR: 1.04e-04\n",
            "  Batch 620/902 | Loss: 0.4227 | CLoss: 0.3512 | FLoss: 0.1432 | LR: 1.04e-04\n",
            "  Batch 630/902 | Loss: 0.4873 | CLoss: 0.3830 | FLoss: 0.2087 | LR: 1.04e-04\n",
            "  Batch 640/902 | Loss: 0.8573 | CLoss: 0.6205 | FLoss: 0.4736 | LR: 1.04e-04\n",
            "  Batch 650/902 | Loss: 0.6223 | CLoss: 0.4424 | FLoss: 0.3599 | LR: 1.04e-04\n",
            "  Batch 660/902 | Loss: 0.6833 | CLoss: 0.5365 | FLoss: 0.2937 | LR: 1.04e-04\n",
            "  Batch 670/902 | Loss: 0.7202 | CLoss: 0.6272 | FLoss: 0.1860 | LR: 1.04e-04\n",
            "  Batch 680/902 | Loss: 0.6348 | CLoss: 0.4621 | FLoss: 0.3454 | LR: 1.04e-04\n",
            "  Batch 690/902 | Loss: 0.6751 | CLoss: 0.5566 | FLoss: 0.2369 | LR: 1.04e-04\n",
            "  Batch 700/902 | Loss: 0.5464 | CLoss: 0.4443 | FLoss: 0.2042 | LR: 1.04e-04\n",
            "  Batch 710/902 | Loss: 0.7357 | CLoss: 0.5764 | FLoss: 0.3187 | LR: 1.04e-04\n",
            "  Batch 720/902 | Loss: 0.5166 | CLoss: 0.3729 | FLoss: 0.2874 | LR: 1.04e-04\n",
            "  Batch 730/902 | Loss: 0.5271 | CLoss: 0.3856 | FLoss: 0.2830 | LR: 1.04e-04\n",
            "  Batch 740/902 | Loss: 0.5000 | CLoss: 0.3688 | FLoss: 0.2625 | LR: 1.04e-04\n",
            "  Batch 750/902 | Loss: 0.3102 | CLoss: 0.2025 | FLoss: 0.2155 | LR: 1.04e-04\n",
            "  Batch 760/902 | Loss: 0.3396 | CLoss: 0.2981 | FLoss: 0.0830 | LR: 1.04e-04\n",
            "  Batch 770/902 | Loss: 0.4741 | CLoss: 0.3847 | FLoss: 0.1787 | LR: 1.04e-04\n",
            "  Batch 780/902 | Loss: 0.5989 | CLoss: 0.5053 | FLoss: 0.1871 | LR: 1.04e-04\n",
            "  Batch 790/902 | Loss: 0.7923 | CLoss: 0.6501 | FLoss: 0.2844 | LR: 1.04e-04\n",
            "  Batch 800/902 | Loss: 1.0452 | CLoss: 0.8811 | FLoss: 0.3282 | LR: 1.04e-04\n",
            "  Batch 810/902 | Loss: 0.4465 | CLoss: 0.3520 | FLoss: 0.1890 | LR: 1.04e-04\n",
            "  Batch 820/902 | Loss: 0.6911 | CLoss: 0.5345 | FLoss: 0.3131 | LR: 1.04e-04\n",
            "  Batch 830/902 | Loss: 0.7582 | CLoss: 0.6072 | FLoss: 0.3019 | LR: 1.04e-04\n",
            "  Batch 840/902 | Loss: 0.4489 | CLoss: 0.3124 | FLoss: 0.2731 | LR: 1.04e-04\n",
            "  Batch 850/902 | Loss: 0.5989 | CLoss: 0.4745 | FLoss: 0.2486 | LR: 1.04e-04\n",
            "  Batch 860/902 | Loss: 0.4897 | CLoss: 0.3992 | FLoss: 0.1808 | LR: 1.04e-04\n",
            "  Batch 870/902 | Loss: 0.8248 | CLoss: 0.6183 | FLoss: 0.4130 | LR: 1.04e-04\n",
            "  Batch 880/902 | Loss: 0.6804 | CLoss: 0.4757 | FLoss: 0.4094 | LR: 1.04e-04\n",
            "  Batch 890/902 | Loss: 0.3815 | CLoss: 0.2639 | FLoss: 0.2354 | LR: 1.04e-04\n",
            "  Batch 900/902 | Loss: 0.7007 | CLoss: 0.6114 | FLoss: 0.1787 | LR: 1.04e-04\n",
            "  Batch 902/902 | Loss: 1.2274 | CLoss: 0.0000 | FLoss: 2.4548 | LR: 1.04e-04\n",
            "\n",
            "  Training Summary | Epoch 4\n",
            "  Avg Loss: 0.6429\n",
            "  Last Batch Loss: 1.2274\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/98 | Loss: 0.8205 | Batch Acc: 70.69%\n",
            "    Val Batch 010/98 | Loss: 2.0949 | Batch Acc: 58.62%\n",
            "    Val Batch 015/98 | Loss: 1.8687 | Batch Acc: 60.34%\n",
            "    Val Batch 020/98 | Loss: 0.9796 | Batch Acc: 27.59%\n",
            "    Val Batch 025/98 | Loss: 0.8895 | Batch Acc: 68.97%\n",
            "    Val Batch 030/98 | Loss: 0.6099 | Batch Acc: 79.31%\n",
            "    Val Batch 035/98 | Loss: 0.8411 | Batch Acc: 74.14%\n",
            "    Val Batch 040/98 | Loss: 0.6440 | Batch Acc: 94.83%\n",
            "    Val Batch 045/98 | Loss: 1.5842 | Batch Acc: 67.24%\n",
            "    Val Batch 050/98 | Loss: 1.5479 | Batch Acc: 65.52%\n",
            "    Val Batch 055/98 | Loss: 0.5122 | Batch Acc: 86.21%\n",
            "    Val Batch 060/98 | Loss: 0.3366 | Batch Acc: 87.93%\n",
            "    Val Batch 065/98 | Loss: 0.3661 | Batch Acc: 84.48%\n",
            "    Val Batch 070/98 | Loss: 0.3100 | Batch Acc: 87.93%\n",
            "    Val Batch 075/98 | Loss: 0.1579 | Batch Acc: 93.10%\n",
            "    Val Batch 080/98 | Loss: 0.2510 | Batch Acc: 94.83%\n",
            "    Val Batch 085/98 | Loss: 0.2669 | Batch Acc: 91.38%\n",
            "    Val Batch 090/98 | Loss: 0.6257 | Batch Acc: 79.31%\n",
            "    Val Batch 095/98 | Loss: 0.2561 | Batch Acc: 96.55%\n",
            "    Val Batch 098/98 | Loss: 0.4057 | Batch Acc: 90.91%\n",
            "\n",
            "  Validation Summary | Epoch 4\n",
            "  Avg Loss: 0.7494 | Accuracy: 76.00%\n",
            "  Current Best Acc: 78.51%\n",
            "\n",
            "Epoch 5/5\n",
            "  Batch 010/902 | Loss: 0.9526 | CLoss: 0.7465 | FLoss: 0.4121 | LR: 2.86e-05\n",
            "  Batch 020/902 | Loss: 0.9046 | CLoss: 0.6314 | FLoss: 0.5464 | LR: 2.86e-05\n",
            "  Batch 030/902 | Loss: 0.6327 | CLoss: 0.5312 | FLoss: 0.2031 | LR: 2.86e-05\n",
            "  Batch 040/902 | Loss: 0.3518 | CLoss: 0.2685 | FLoss: 0.1665 | LR: 2.86e-05\n",
            "  Batch 050/902 | Loss: 1.0371 | CLoss: 0.8023 | FLoss: 0.4695 | LR: 2.86e-05\n",
            "  Batch 060/902 | Loss: 0.6644 | CLoss: 0.4707 | FLoss: 0.3875 | LR: 2.86e-05\n",
            "  Batch 070/902 | Loss: 0.3291 | CLoss: 0.2774 | FLoss: 0.1034 | LR: 2.86e-05\n",
            "  Batch 080/902 | Loss: 0.6380 | CLoss: 0.5623 | FLoss: 0.1515 | LR: 2.86e-05\n",
            "  Batch 090/902 | Loss: 0.4526 | CLoss: 0.3387 | FLoss: 0.2278 | LR: 2.86e-05\n",
            "  Batch 100/902 | Loss: 0.8988 | CLoss: 0.7304 | FLoss: 0.3367 | LR: 2.86e-05\n",
            "  Batch 110/902 | Loss: 0.5941 | CLoss: 0.4598 | FLoss: 0.2686 | LR: 2.86e-05\n",
            "  Batch 120/902 | Loss: 0.4627 | CLoss: 0.3730 | FLoss: 0.1794 | LR: 2.86e-05\n",
            "  Batch 130/902 | Loss: 0.4579 | CLoss: 0.4066 | FLoss: 0.1026 | LR: 2.86e-05\n",
            "  Batch 140/902 | Loss: 0.7943 | CLoss: 0.6424 | FLoss: 0.3039 | LR: 2.86e-05\n",
            "  Batch 150/902 | Loss: 0.6783 | CLoss: 0.5088 | FLoss: 0.3389 | LR: 2.86e-05\n",
            "  Batch 160/902 | Loss: 0.7258 | CLoss: 0.5912 | FLoss: 0.2691 | LR: 2.86e-05\n",
            "  Batch 170/902 | Loss: 0.2311 | CLoss: 0.1166 | FLoss: 0.2291 | LR: 2.86e-05\n",
            "  Batch 180/902 | Loss: 0.4678 | CLoss: 0.3408 | FLoss: 0.2540 | LR: 2.86e-05\n",
            "  Batch 190/902 | Loss: 0.6683 | CLoss: 0.5068 | FLoss: 0.3230 | LR: 2.86e-05\n",
            "  Batch 200/902 | Loss: 0.7250 | CLoss: 0.6085 | FLoss: 0.2328 | LR: 2.86e-05\n",
            "  Batch 210/902 | Loss: 0.3336 | CLoss: 0.2554 | FLoss: 0.1565 | LR: 2.86e-05\n",
            "  Batch 220/902 | Loss: 0.7125 | CLoss: 0.5891 | FLoss: 0.2467 | LR: 2.86e-05\n",
            "  Batch 230/902 | Loss: 0.6965 | CLoss: 0.5739 | FLoss: 0.2451 | LR: 2.86e-05\n",
            "  Batch 240/902 | Loss: 0.7671 | CLoss: 0.5951 | FLoss: 0.3441 | LR: 2.86e-05\n",
            "  Batch 250/902 | Loss: 0.4110 | CLoss: 0.2976 | FLoss: 0.2267 | LR: 2.86e-05\n",
            "  Batch 260/902 | Loss: 0.7531 | CLoss: 0.6358 | FLoss: 0.2345 | LR: 2.86e-05\n",
            "  Batch 270/902 | Loss: 0.3852 | CLoss: 0.2751 | FLoss: 0.2201 | LR: 2.86e-05\n",
            "  Batch 280/902 | Loss: 0.6373 | CLoss: 0.5020 | FLoss: 0.2706 | LR: 2.86e-05\n",
            "  Batch 290/902 | Loss: 0.3986 | CLoss: 0.3371 | FLoss: 0.1229 | LR: 2.86e-05\n",
            "  Batch 300/902 | Loss: 0.7847 | CLoss: 0.6462 | FLoss: 0.2771 | LR: 2.86e-05\n",
            "  Batch 310/902 | Loss: 0.4613 | CLoss: 0.3052 | FLoss: 0.3121 | LR: 2.86e-05\n",
            "  Batch 320/902 | Loss: 0.7260 | CLoss: 0.5639 | FLoss: 0.3241 | LR: 2.86e-05\n",
            "  Batch 330/902 | Loss: 0.5510 | CLoss: 0.4391 | FLoss: 0.2238 | LR: 2.86e-05\n",
            "  Batch 340/902 | Loss: 0.6294 | CLoss: 0.4748 | FLoss: 0.3091 | LR: 2.86e-05\n",
            "  Batch 350/902 | Loss: 0.4341 | CLoss: 0.3564 | FLoss: 0.1554 | LR: 2.86e-05\n",
            "  Batch 360/902 | Loss: 0.3682 | CLoss: 0.2100 | FLoss: 0.3164 | LR: 2.86e-05\n",
            "  Batch 370/902 | Loss: 0.4961 | CLoss: 0.3615 | FLoss: 0.2692 | LR: 2.86e-05\n",
            "  Batch 380/902 | Loss: 0.4310 | CLoss: 0.2967 | FLoss: 0.2687 | LR: 2.86e-05\n",
            "  Batch 390/902 | Loss: 0.5479 | CLoss: 0.4027 | FLoss: 0.2903 | LR: 2.86e-05\n",
            "  Batch 400/902 | Loss: 0.7639 | CLoss: 0.6432 | FLoss: 0.2414 | LR: 2.86e-05\n",
            "  Batch 410/902 | Loss: 0.7173 | CLoss: 0.5681 | FLoss: 0.2984 | LR: 2.86e-05\n",
            "  Batch 420/902 | Loss: 0.1387 | CLoss: 0.0855 | FLoss: 0.1064 | LR: 2.86e-05\n",
            "  Batch 430/902 | Loss: 0.6600 | CLoss: 0.5234 | FLoss: 0.2732 | LR: 2.86e-05\n",
            "  Batch 440/902 | Loss: 0.8068 | CLoss: 0.6683 | FLoss: 0.2769 | LR: 2.86e-05\n",
            "  Batch 450/902 | Loss: 0.8299 | CLoss: 0.6019 | FLoss: 0.4561 | LR: 2.86e-05\n",
            "  Batch 460/902 | Loss: 0.4591 | CLoss: 0.3779 | FLoss: 0.1624 | LR: 2.86e-05\n",
            "  Batch 470/902 | Loss: 0.6448 | CLoss: 0.5291 | FLoss: 0.2314 | LR: 2.86e-05\n",
            "  Batch 480/902 | Loss: 1.1163 | CLoss: 0.9104 | FLoss: 0.4117 | LR: 2.86e-05\n",
            "  Batch 490/902 | Loss: 0.3562 | CLoss: 0.2425 | FLoss: 0.2274 | LR: 2.86e-05\n",
            "  Batch 500/902 | Loss: 0.5454 | CLoss: 0.3997 | FLoss: 0.2915 | LR: 2.86e-05\n",
            "  Batch 510/902 | Loss: 0.4667 | CLoss: 0.3687 | FLoss: 0.1960 | LR: 2.86e-05\n",
            "  Batch 520/902 | Loss: 0.8571 | CLoss: 0.6427 | FLoss: 0.4290 | LR: 2.86e-05\n",
            "  Batch 530/902 | Loss: 0.6343 | CLoss: 0.5197 | FLoss: 0.2293 | LR: 2.86e-05\n",
            "  Batch 540/902 | Loss: 0.5497 | CLoss: 0.4843 | FLoss: 0.1308 | LR: 2.86e-05\n",
            "  Batch 550/902 | Loss: 0.7867 | CLoss: 0.6460 | FLoss: 0.2813 | LR: 2.86e-05\n",
            "  Batch 560/902 | Loss: 0.5942 | CLoss: 0.4255 | FLoss: 0.3375 | LR: 2.86e-05\n",
            "  Batch 570/902 | Loss: 0.5540 | CLoss: 0.3827 | FLoss: 0.3426 | LR: 2.86e-05\n",
            "  Batch 580/902 | Loss: 0.4703 | CLoss: 0.3765 | FLoss: 0.1876 | LR: 2.86e-05\n",
            "  Batch 590/902 | Loss: 0.8518 | CLoss: 0.7111 | FLoss: 0.2814 | LR: 2.86e-05\n",
            "  Batch 600/902 | Loss: 0.4098 | CLoss: 0.2843 | FLoss: 0.2509 | LR: 2.86e-05\n",
            "  Batch 610/902 | Loss: 0.4528 | CLoss: 0.3395 | FLoss: 0.2267 | LR: 2.86e-05\n",
            "  Batch 620/902 | Loss: 0.3762 | CLoss: 0.3075 | FLoss: 0.1373 | LR: 2.86e-05\n",
            "  Batch 630/902 | Loss: 0.4708 | CLoss: 0.3117 | FLoss: 0.3183 | LR: 2.86e-05\n",
            "  Batch 640/902 | Loss: 0.2233 | CLoss: 0.1511 | FLoss: 0.1444 | LR: 2.86e-05\n",
            "  Batch 650/902 | Loss: 0.3599 | CLoss: 0.2361 | FLoss: 0.2476 | LR: 2.86e-05\n",
            "  Batch 660/902 | Loss: 0.6568 | CLoss: 0.5069 | FLoss: 0.2998 | LR: 2.86e-05\n",
            "  Batch 670/902 | Loss: 0.3176 | CLoss: 0.2190 | FLoss: 0.1973 | LR: 2.86e-05\n",
            "  Batch 680/902 | Loss: 0.4920 | CLoss: 0.3652 | FLoss: 0.2536 | LR: 2.86e-05\n",
            "  Batch 690/902 | Loss: 0.5293 | CLoss: 0.4376 | FLoss: 0.1834 | LR: 2.86e-05\n",
            "  Batch 700/902 | Loss: 0.6189 | CLoss: 0.5072 | FLoss: 0.2235 | LR: 2.86e-05\n",
            "  Batch 710/902 | Loss: 0.2995 | CLoss: 0.2149 | FLoss: 0.1692 | LR: 2.86e-05\n",
            "  Batch 720/902 | Loss: 0.8079 | CLoss: 0.6949 | FLoss: 0.2261 | LR: 2.86e-05\n",
            "  Batch 730/902 | Loss: 0.5530 | CLoss: 0.4268 | FLoss: 0.2524 | LR: 2.86e-05\n",
            "  Batch 740/902 | Loss: 0.3986 | CLoss: 0.3112 | FLoss: 0.1749 | LR: 2.86e-05\n",
            "  Batch 750/902 | Loss: 0.4510 | CLoss: 0.3909 | FLoss: 0.1202 | LR: 2.86e-05\n",
            "  Batch 760/902 | Loss: 0.5738 | CLoss: 0.4656 | FLoss: 0.2163 | LR: 2.86e-05\n",
            "  Batch 770/902 | Loss: 0.6261 | CLoss: 0.5221 | FLoss: 0.2079 | LR: 2.86e-05\n",
            "  Batch 780/902 | Loss: 0.2252 | CLoss: 0.1161 | FLoss: 0.2183 | LR: 2.86e-05\n",
            "  Batch 790/902 | Loss: 0.5799 | CLoss: 0.4450 | FLoss: 0.2700 | LR: 2.86e-05\n",
            "  Batch 800/902 | Loss: 0.4511 | CLoss: 0.3330 | FLoss: 0.2362 | LR: 2.86e-05\n",
            "  Batch 810/902 | Loss: 0.6058 | CLoss: 0.4834 | FLoss: 0.2447 | LR: 2.86e-05\n",
            "  Batch 820/902 | Loss: 0.3686 | CLoss: 0.2965 | FLoss: 0.1443 | LR: 2.86e-05\n",
            "  Batch 830/902 | Loss: 0.5416 | CLoss: 0.4164 | FLoss: 0.2503 | LR: 2.86e-05\n",
            "  Batch 840/902 | Loss: 0.5533 | CLoss: 0.4483 | FLoss: 0.2098 | LR: 2.86e-05\n",
            "  Batch 850/902 | Loss: 0.4728 | CLoss: 0.4011 | FLoss: 0.1434 | LR: 2.86e-05\n",
            "  Batch 860/902 | Loss: 0.5674 | CLoss: 0.4608 | FLoss: 0.2132 | LR: 2.86e-05\n",
            "  Batch 870/902 | Loss: 0.9553 | CLoss: 0.7263 | FLoss: 0.4580 | LR: 2.86e-05\n",
            "  Batch 880/902 | Loss: 1.0435 | CLoss: 0.9034 | FLoss: 0.2801 | LR: 2.86e-05\n",
            "  Batch 890/902 | Loss: 0.3585 | CLoss: 0.2704 | FLoss: 0.1762 | LR: 2.86e-05\n",
            "  Batch 900/902 | Loss: 0.4892 | CLoss: 0.3344 | FLoss: 0.3096 | LR: 2.86e-05\n",
            "  Batch 902/902 | Loss: 0.9188 | CLoss: 0.0000 | FLoss: 1.8376 | LR: 2.86e-05\n",
            "\n",
            "  Training Summary | Epoch 5\n",
            "  Avg Loss: 0.5930\n",
            "  Last Batch Loss: 0.9188\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/98 | Loss: 0.9659 | Batch Acc: 72.41%\n",
            "    Val Batch 010/98 | Loss: 0.6088 | Batch Acc: 82.76%\n",
            "    Val Batch 015/98 | Loss: 1.0766 | Batch Acc: 72.41%\n",
            "    Val Batch 020/98 | Loss: 4.4743 | Batch Acc: 18.97%\n",
            "    Val Batch 025/98 | Loss: 1.0624 | Batch Acc: 62.07%\n",
            "    Val Batch 030/98 | Loss: 0.8932 | Batch Acc: 74.14%\n",
            "    Val Batch 035/98 | Loss: 0.1252 | Batch Acc: 96.55%\n",
            "    Val Batch 040/98 | Loss: 1.9639 | Batch Acc: 58.62%\n",
            "    Val Batch 045/98 | Loss: 1.5404 | Batch Acc: 75.86%\n",
            "    Val Batch 050/98 | Loss: 1.1984 | Batch Acc: 75.86%\n",
            "    Val Batch 055/98 | Loss: 1.7238 | Batch Acc: 56.90%\n",
            "    Val Batch 060/98 | Loss: 1.2716 | Batch Acc: 72.41%\n",
            "    Val Batch 065/98 | Loss: 1.4524 | Batch Acc: 65.52%\n",
            "    Val Batch 070/98 | Loss: 0.6225 | Batch Acc: 87.93%\n",
            "    Val Batch 075/98 | Loss: 0.7001 | Batch Acc: 84.48%\n",
            "    Val Batch 080/98 | Loss: 0.8376 | Batch Acc: 81.03%\n",
            "    Val Batch 085/98 | Loss: 0.2854 | Batch Acc: 89.66%\n",
            "    Val Batch 090/98 | Loss: 0.2334 | Batch Acc: 91.38%\n",
            "    Val Batch 095/98 | Loss: 0.2250 | Batch Acc: 93.10%\n",
            "    Val Batch 098/98 | Loss: 0.2939 | Batch Acc: 87.88%\n",
            "\n",
            "  Validation Summary | Epoch 5\n",
            "  Avg Loss: 1.2098 | Accuracy: 72.73%\n",
            "  Current Best Acc: 78.51%\n",
            "\n",
            "========================================\n",
            "=== Fold 8 Completed ===\n",
            "Best Validation Accuracy: 78.51%\n",
            "\n",
            "========================================\n",
            "=== Fold 9/10 ====================\n",
            "========================================\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "  Batch 010/901 | Loss: 0.5860 | CLoss: 0.4398 | FLoss: 0.2925 | LR: 3.00e-04\n",
            "  Batch 020/901 | Loss: 1.0143 | CLoss: 0.8216 | FLoss: 0.3855 | LR: 3.00e-04\n",
            "  Batch 030/901 | Loss: 0.5987 | CLoss: 0.4991 | FLoss: 0.1991 | LR: 3.00e-04\n",
            "  Batch 040/901 | Loss: 0.7476 | CLoss: 0.5244 | FLoss: 0.4463 | LR: 3.00e-04\n",
            "  Batch 050/901 | Loss: 1.2954 | CLoss: 1.1544 | FLoss: 0.2820 | LR: 3.00e-04\n",
            "  Batch 060/901 | Loss: 0.6400 | CLoss: 0.3961 | FLoss: 0.4877 | LR: 3.00e-04\n",
            "  Batch 070/901 | Loss: 0.5971 | CLoss: 0.5101 | FLoss: 0.1741 | LR: 3.00e-04\n",
            "  Batch 080/901 | Loss: 0.6696 | CLoss: 0.4869 | FLoss: 0.3655 | LR: 3.00e-04\n",
            "  Batch 090/901 | Loss: 0.7656 | CLoss: 0.5232 | FLoss: 0.4849 | LR: 3.00e-04\n",
            "  Batch 100/901 | Loss: 0.6899 | CLoss: 0.3649 | FLoss: 0.6501 | LR: 3.00e-04\n",
            "  Batch 110/901 | Loss: 0.5995 | CLoss: 0.4698 | FLoss: 0.2593 | LR: 3.00e-04\n",
            "  Batch 120/901 | Loss: 0.7428 | CLoss: 0.5143 | FLoss: 0.4570 | LR: 3.00e-04\n",
            "  Batch 130/901 | Loss: 1.2478 | CLoss: 0.9999 | FLoss: 0.4958 | LR: 3.00e-04\n",
            "  Batch 140/901 | Loss: 0.8630 | CLoss: 0.6947 | FLoss: 0.3367 | LR: 3.00e-04\n",
            "  Batch 150/901 | Loss: 0.5117 | CLoss: 0.3763 | FLoss: 0.2707 | LR: 3.00e-04\n",
            "  Batch 160/901 | Loss: 0.7163 | CLoss: 0.5085 | FLoss: 0.4155 | LR: 3.00e-04\n",
            "  Batch 170/901 | Loss: 0.3999 | CLoss: 0.2509 | FLoss: 0.2979 | LR: 3.00e-04\n",
            "  Batch 180/901 | Loss: 1.0985 | CLoss: 0.8530 | FLoss: 0.4909 | LR: 3.00e-04\n",
            "  Batch 190/901 | Loss: 0.5260 | CLoss: 0.4136 | FLoss: 0.2248 | LR: 3.00e-04\n",
            "  Batch 200/901 | Loss: 0.6210 | CLoss: 0.3920 | FLoss: 0.4579 | LR: 3.00e-04\n",
            "  Batch 210/901 | Loss: 1.3132 | CLoss: 1.0695 | FLoss: 0.4875 | LR: 3.00e-04\n",
            "  Batch 220/901 | Loss: 0.7501 | CLoss: 0.5198 | FLoss: 0.4605 | LR: 3.00e-04\n",
            "  Batch 230/901 | Loss: 0.6176 | CLoss: 0.4919 | FLoss: 0.2514 | LR: 3.00e-04\n",
            "  Batch 240/901 | Loss: 1.0864 | CLoss: 0.8340 | FLoss: 0.5050 | LR: 3.00e-04\n",
            "  Batch 250/901 | Loss: 0.9403 | CLoss: 0.6873 | FLoss: 0.5060 | LR: 3.00e-04\n",
            "  Batch 260/901 | Loss: 0.6715 | CLoss: 0.5119 | FLoss: 0.3191 | LR: 3.00e-04\n",
            "  Batch 270/901 | Loss: 0.8561 | CLoss: 0.7699 | FLoss: 0.1725 | LR: 3.00e-04\n",
            "  Batch 280/901 | Loss: 0.7884 | CLoss: 0.6407 | FLoss: 0.2953 | LR: 3.00e-04\n",
            "  Batch 290/901 | Loss: 0.9928 | CLoss: 0.6742 | FLoss: 0.6373 | LR: 3.00e-04\n",
            "  Batch 300/901 | Loss: 0.7985 | CLoss: 0.5983 | FLoss: 0.4005 | LR: 3.00e-04\n",
            "  Batch 310/901 | Loss: 0.9581 | CLoss: 0.8181 | FLoss: 0.2800 | LR: 3.00e-04\n",
            "  Batch 320/901 | Loss: 0.8657 | CLoss: 0.6725 | FLoss: 0.3864 | LR: 3.00e-04\n",
            "  Batch 330/901 | Loss: 0.7818 | CLoss: 0.5379 | FLoss: 0.4880 | LR: 3.00e-04\n",
            "  Batch 340/901 | Loss: 1.1812 | CLoss: 0.9665 | FLoss: 0.4294 | LR: 3.00e-04\n",
            "  Batch 350/901 | Loss: 0.9659 | CLoss: 0.7363 | FLoss: 0.4591 | LR: 3.00e-04\n",
            "  Batch 360/901 | Loss: 0.7788 | CLoss: 0.6598 | FLoss: 0.2379 | LR: 3.00e-04\n",
            "  Batch 370/901 | Loss: 0.7256 | CLoss: 0.4812 | FLoss: 0.4888 | LR: 3.00e-04\n",
            "  Batch 380/901 | Loss: 0.5971 | CLoss: 0.4744 | FLoss: 0.2455 | LR: 3.00e-04\n",
            "  Batch 390/901 | Loss: 0.9622 | CLoss: 0.7933 | FLoss: 0.3378 | LR: 3.00e-04\n",
            "  Batch 400/901 | Loss: 0.9131 | CLoss: 0.7009 | FLoss: 0.4243 | LR: 3.00e-04\n",
            "  Batch 410/901 | Loss: 0.8056 | CLoss: 0.6148 | FLoss: 0.3816 | LR: 3.00e-04\n",
            "  Batch 420/901 | Loss: 0.6782 | CLoss: 0.5024 | FLoss: 0.3516 | LR: 3.00e-04\n",
            "  Batch 430/901 | Loss: 0.6902 | CLoss: 0.5254 | FLoss: 0.3297 | LR: 3.00e-04\n",
            "  Batch 440/901 | Loss: 0.4978 | CLoss: 0.4131 | FLoss: 0.1693 | LR: 3.00e-04\n",
            "  Batch 450/901 | Loss: 0.5743 | CLoss: 0.3871 | FLoss: 0.3743 | LR: 3.00e-04\n",
            "  Batch 460/901 | Loss: 0.9000 | CLoss: 0.7158 | FLoss: 0.3684 | LR: 3.00e-04\n",
            "  Batch 470/901 | Loss: 0.8433 | CLoss: 0.6619 | FLoss: 0.3629 | LR: 3.00e-04\n",
            "  Batch 480/901 | Loss: 1.1134 | CLoss: 0.9800 | FLoss: 0.2669 | LR: 3.00e-04\n",
            "  Batch 490/901 | Loss: 0.8881 | CLoss: 0.7398 | FLoss: 0.2966 | LR: 3.00e-04\n",
            "  Batch 500/901 | Loss: 0.7481 | CLoss: 0.5580 | FLoss: 0.3801 | LR: 3.00e-04\n",
            "  Batch 510/901 | Loss: 1.4410 | CLoss: 1.1468 | FLoss: 0.5883 | LR: 3.00e-04\n",
            "  Batch 520/901 | Loss: 0.6994 | CLoss: 0.5372 | FLoss: 0.3244 | LR: 3.00e-04\n",
            "  Batch 530/901 | Loss: 0.8001 | CLoss: 0.6416 | FLoss: 0.3171 | LR: 3.00e-04\n",
            "  Batch 540/901 | Loss: 0.5329 | CLoss: 0.3352 | FLoss: 0.3955 | LR: 3.00e-04\n",
            "  Batch 550/901 | Loss: 0.5777 | CLoss: 0.4288 | FLoss: 0.2978 | LR: 3.00e-04\n",
            "  Batch 560/901 | Loss: 0.5100 | CLoss: 0.3862 | FLoss: 0.2475 | LR: 3.00e-04\n",
            "  Batch 570/901 | Loss: 0.7730 | CLoss: 0.5603 | FLoss: 0.4255 | LR: 3.00e-04\n",
            "  Batch 580/901 | Loss: 0.8461 | CLoss: 0.6645 | FLoss: 0.3633 | LR: 3.00e-04\n",
            "  Batch 590/901 | Loss: 0.5841 | CLoss: 0.4536 | FLoss: 0.2609 | LR: 3.00e-04\n",
            "  Batch 600/901 | Loss: 0.9319 | CLoss: 0.7439 | FLoss: 0.3759 | LR: 3.00e-04\n",
            "  Batch 610/901 | Loss: 0.7768 | CLoss: 0.6534 | FLoss: 0.2467 | LR: 3.00e-04\n",
            "  Batch 620/901 | Loss: 0.8020 | CLoss: 0.6460 | FLoss: 0.3119 | LR: 3.00e-04\n",
            "  Batch 630/901 | Loss: 0.9854 | CLoss: 0.7543 | FLoss: 0.4621 | LR: 3.00e-04\n",
            "  Batch 640/901 | Loss: 1.0494 | CLoss: 0.7867 | FLoss: 0.5255 | LR: 3.00e-04\n",
            "  Batch 650/901 | Loss: 0.7785 | CLoss: 0.6000 | FLoss: 0.3572 | LR: 3.00e-04\n",
            "  Batch 660/901 | Loss: 1.1192 | CLoss: 0.8279 | FLoss: 0.5826 | LR: 3.00e-04\n",
            "  Batch 670/901 | Loss: 1.1145 | CLoss: 0.8662 | FLoss: 0.4967 | LR: 3.00e-04\n",
            "  Batch 680/901 | Loss: 0.7009 | CLoss: 0.5432 | FLoss: 0.3154 | LR: 3.00e-04\n",
            "  Batch 690/901 | Loss: 1.5398 | CLoss: 1.2331 | FLoss: 0.6135 | LR: 3.00e-04\n",
            "  Batch 700/901 | Loss: 1.2392 | CLoss: 1.0747 | FLoss: 0.3291 | LR: 3.00e-04\n",
            "  Batch 710/901 | Loss: 0.8778 | CLoss: 0.5933 | FLoss: 0.5689 | LR: 3.00e-04\n",
            "  Batch 720/901 | Loss: 0.3497 | CLoss: 0.2564 | FLoss: 0.1867 | LR: 3.00e-04\n",
            "  Batch 730/901 | Loss: 0.6497 | CLoss: 0.4961 | FLoss: 0.3073 | LR: 3.00e-04\n",
            "  Batch 740/901 | Loss: 0.9347 | CLoss: 0.7392 | FLoss: 0.3910 | LR: 3.00e-04\n",
            "  Batch 750/901 | Loss: 1.4189 | CLoss: 1.1081 | FLoss: 0.6216 | LR: 3.00e-04\n",
            "  Batch 760/901 | Loss: 0.9457 | CLoss: 0.8133 | FLoss: 0.2648 | LR: 3.00e-04\n",
            "  Batch 770/901 | Loss: 0.7710 | CLoss: 0.6133 | FLoss: 0.3154 | LR: 3.00e-04\n",
            "  Batch 780/901 | Loss: 0.6738 | CLoss: 0.5037 | FLoss: 0.3402 | LR: 3.00e-04\n",
            "  Batch 790/901 | Loss: 1.1817 | CLoss: 0.9751 | FLoss: 0.4131 | LR: 3.00e-04\n",
            "  Batch 800/901 | Loss: 0.8446 | CLoss: 0.6281 | FLoss: 0.4329 | LR: 3.00e-04\n",
            "  Batch 810/901 | Loss: 0.8960 | CLoss: 0.6360 | FLoss: 0.5199 | LR: 3.00e-04\n",
            "  Batch 820/901 | Loss: 0.8225 | CLoss: 0.6691 | FLoss: 0.3068 | LR: 3.00e-04\n",
            "  Batch 830/901 | Loss: 1.0728 | CLoss: 0.9128 | FLoss: 0.3199 | LR: 3.00e-04\n",
            "  Batch 840/901 | Loss: 0.5489 | CLoss: 0.4361 | FLoss: 0.2256 | LR: 3.00e-04\n",
            "  Batch 850/901 | Loss: 0.6397 | CLoss: 0.5063 | FLoss: 0.2667 | LR: 3.00e-04\n",
            "  Batch 860/901 | Loss: 0.9239 | CLoss: 0.6678 | FLoss: 0.5122 | LR: 3.00e-04\n",
            "  Batch 870/901 | Loss: 0.9198 | CLoss: 0.6895 | FLoss: 0.4605 | LR: 3.00e-04\n",
            "  Batch 880/901 | Loss: 0.8092 | CLoss: 0.6905 | FLoss: 0.2376 | LR: 3.00e-04\n",
            "  Batch 890/901 | Loss: 0.6782 | CLoss: 0.4878 | FLoss: 0.3809 | LR: 3.00e-04\n",
            "  Batch 900/901 | Loss: 0.6974 | CLoss: 0.5004 | FLoss: 0.3940 | LR: 3.00e-04\n",
            "  Batch 901/901 | Loss: 0.5983 | CLoss: 0.2373 | FLoss: 0.7220 | LR: 3.00e-04\n",
            "\n",
            "  Training Summary | Epoch 1\n",
            "  Avg Loss: 0.7875\n",
            "  Last Batch Loss: 0.5983\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 1.0662 | Batch Acc: 62.07%\n",
            "    Val Batch 010/99 | Loss: 0.1960 | Batch Acc: 93.10%\n",
            "    Val Batch 015/99 | Loss: 0.3309 | Batch Acc: 93.10%\n",
            "    Val Batch 020/99 | Loss: 1.0568 | Batch Acc: 29.31%\n",
            "    Val Batch 025/99 | Loss: 0.4094 | Batch Acc: 84.48%\n",
            "    Val Batch 030/99 | Loss: 0.3269 | Batch Acc: 91.38%\n",
            "    Val Batch 035/99 | Loss: 0.1689 | Batch Acc: 93.10%\n",
            "    Val Batch 040/99 | Loss: 0.5318 | Batch Acc: 91.38%\n",
            "    Val Batch 045/99 | Loss: 0.1883 | Batch Acc: 93.10%\n",
            "    Val Batch 050/99 | Loss: 0.1899 | Batch Acc: 91.38%\n",
            "    Val Batch 055/99 | Loss: 0.2009 | Batch Acc: 93.10%\n",
            "    Val Batch 060/99 | Loss: 0.2952 | Batch Acc: 87.93%\n",
            "    Val Batch 065/99 | Loss: 0.6287 | Batch Acc: 77.59%\n",
            "    Val Batch 070/99 | Loss: 0.2297 | Batch Acc: 94.83%\n",
            "    Val Batch 075/99 | Loss: 0.2747 | Batch Acc: 87.93%\n",
            "    Val Batch 080/99 | Loss: 0.1286 | Batch Acc: 94.83%\n",
            "    Val Batch 085/99 | Loss: 0.0708 | Batch Acc: 98.28%\n",
            "    Val Batch 090/99 | Loss: 0.2792 | Batch Acc: 91.38%\n",
            "    Val Batch 095/99 | Loss: 0.0308 | Batch Acc: 98.28%\n",
            "    Val Batch 099/99 | Loss: 0.0096 | Batch Acc: 100.00%\n",
            "\n",
            "  Validation Summary | Epoch 1\n",
            "  Avg Loss: 0.3887 | Accuracy: 85.07%\n",
            "  Current Best Acc: 85.07%\n",
            "\n",
            "Epoch 2/5\n",
            "  Batch 010/901 | Loss: 0.5918 | CLoss: 0.4500 | FLoss: 0.2836 | LR: 2.71e-04\n",
            "  Batch 020/901 | Loss: 0.3964 | CLoss: 0.2266 | FLoss: 0.3396 | LR: 2.71e-04\n",
            "  Batch 030/901 | Loss: 0.5997 | CLoss: 0.4189 | FLoss: 0.3615 | LR: 2.71e-04\n",
            "  Batch 040/901 | Loss: 0.4809 | CLoss: 0.3671 | FLoss: 0.2275 | LR: 2.71e-04\n",
            "  Batch 050/901 | Loss: 1.0393 | CLoss: 0.7429 | FLoss: 0.5928 | LR: 2.71e-04\n",
            "  Batch 060/901 | Loss: 0.6070 | CLoss: 0.4537 | FLoss: 0.3065 | LR: 2.71e-04\n",
            "  Batch 070/901 | Loss: 0.8831 | CLoss: 0.6775 | FLoss: 0.4113 | LR: 2.71e-04\n",
            "  Batch 080/901 | Loss: 0.7513 | CLoss: 0.5983 | FLoss: 0.3060 | LR: 2.71e-04\n",
            "  Batch 090/901 | Loss: 0.8706 | CLoss: 0.7228 | FLoss: 0.2955 | LR: 2.71e-04\n",
            "  Batch 100/901 | Loss: 0.6194 | CLoss: 0.5594 | FLoss: 0.1200 | LR: 2.71e-04\n",
            "  Batch 110/901 | Loss: 0.5684 | CLoss: 0.4626 | FLoss: 0.2116 | LR: 2.71e-04\n",
            "  Batch 120/901 | Loss: 0.5565 | CLoss: 0.4413 | FLoss: 0.2305 | LR: 2.71e-04\n",
            "  Batch 130/901 | Loss: 1.0687 | CLoss: 0.8579 | FLoss: 0.4216 | LR: 2.71e-04\n",
            "  Batch 140/901 | Loss: 0.4879 | CLoss: 0.3430 | FLoss: 0.2898 | LR: 2.71e-04\n",
            "  Batch 150/901 | Loss: 0.6523 | CLoss: 0.4708 | FLoss: 0.3630 | LR: 2.71e-04\n",
            "  Batch 160/901 | Loss: 0.3411 | CLoss: 0.2011 | FLoss: 0.2801 | LR: 2.71e-04\n",
            "  Batch 170/901 | Loss: 0.8394 | CLoss: 0.6994 | FLoss: 0.2800 | LR: 2.71e-04\n",
            "  Batch 180/901 | Loss: 0.4758 | CLoss: 0.3762 | FLoss: 0.1994 | LR: 2.71e-04\n",
            "  Batch 190/901 | Loss: 0.8446 | CLoss: 0.6808 | FLoss: 0.3277 | LR: 2.71e-04\n",
            "  Batch 200/901 | Loss: 0.8444 | CLoss: 0.6319 | FLoss: 0.4249 | LR: 2.71e-04\n",
            "  Batch 210/901 | Loss: 0.7117 | CLoss: 0.5838 | FLoss: 0.2559 | LR: 2.71e-04\n",
            "  Batch 220/901 | Loss: 0.4315 | CLoss: 0.3229 | FLoss: 0.2172 | LR: 2.71e-04\n",
            "  Batch 230/901 | Loss: 0.8502 | CLoss: 0.6985 | FLoss: 0.3034 | LR: 2.71e-04\n",
            "  Batch 240/901 | Loss: 1.3324 | CLoss: 1.0830 | FLoss: 0.4988 | LR: 2.71e-04\n",
            "  Batch 250/901 | Loss: 0.7858 | CLoss: 0.6144 | FLoss: 0.3428 | LR: 2.71e-04\n",
            "  Batch 260/901 | Loss: 1.0841 | CLoss: 0.9178 | FLoss: 0.3326 | LR: 2.71e-04\n",
            "  Batch 270/901 | Loss: 1.1653 | CLoss: 0.8696 | FLoss: 0.5915 | LR: 2.71e-04\n",
            "  Batch 280/901 | Loss: 1.1611 | CLoss: 0.9394 | FLoss: 0.4434 | LR: 2.71e-04\n",
            "  Batch 290/901 | Loss: 0.5839 | CLoss: 0.4308 | FLoss: 0.3062 | LR: 2.71e-04\n",
            "  Batch 300/901 | Loss: 0.7892 | CLoss: 0.6483 | FLoss: 0.2819 | LR: 2.71e-04\n",
            "  Batch 310/901 | Loss: 1.0735 | CLoss: 0.8452 | FLoss: 0.4565 | LR: 2.71e-04\n",
            "  Batch 320/901 | Loss: 0.8535 | CLoss: 0.7117 | FLoss: 0.2835 | LR: 2.71e-04\n",
            "  Batch 330/901 | Loss: 1.0560 | CLoss: 0.8740 | FLoss: 0.3639 | LR: 2.71e-04\n",
            "  Batch 340/901 | Loss: 0.6965 | CLoss: 0.5060 | FLoss: 0.3811 | LR: 2.71e-04\n",
            "  Batch 350/901 | Loss: 1.1533 | CLoss: 0.8733 | FLoss: 0.5599 | LR: 2.71e-04\n",
            "  Batch 360/901 | Loss: 0.4449 | CLoss: 0.3157 | FLoss: 0.2583 | LR: 2.71e-04\n",
            "  Batch 370/901 | Loss: 0.9010 | CLoss: 0.7030 | FLoss: 0.3961 | LR: 2.71e-04\n",
            "  Batch 380/901 | Loss: 0.6504 | CLoss: 0.5210 | FLoss: 0.2587 | LR: 2.71e-04\n",
            "  Batch 390/901 | Loss: 0.6417 | CLoss: 0.4907 | FLoss: 0.3021 | LR: 2.71e-04\n",
            "  Batch 400/901 | Loss: 1.1490 | CLoss: 0.9050 | FLoss: 0.4880 | LR: 2.71e-04\n",
            "  Batch 410/901 | Loss: 0.7827 | CLoss: 0.6037 | FLoss: 0.3580 | LR: 2.71e-04\n",
            "  Batch 420/901 | Loss: 1.0220 | CLoss: 0.8555 | FLoss: 0.3330 | LR: 2.71e-04\n",
            "  Batch 430/901 | Loss: 0.6598 | CLoss: 0.4864 | FLoss: 0.3468 | LR: 2.71e-04\n",
            "  Batch 440/901 | Loss: 0.5237 | CLoss: 0.3946 | FLoss: 0.2581 | LR: 2.71e-04\n",
            "  Batch 450/901 | Loss: 0.6614 | CLoss: 0.4395 | FLoss: 0.4439 | LR: 2.71e-04\n",
            "  Batch 460/901 | Loss: 1.1910 | CLoss: 0.9884 | FLoss: 0.4052 | LR: 2.71e-04\n",
            "  Batch 470/901 | Loss: 0.3369 | CLoss: 0.2389 | FLoss: 0.1959 | LR: 2.71e-04\n",
            "  Batch 480/901 | Loss: 0.6135 | CLoss: 0.4743 | FLoss: 0.2783 | LR: 2.71e-04\n",
            "  Batch 490/901 | Loss: 0.9214 | CLoss: 0.7308 | FLoss: 0.3812 | LR: 2.71e-04\n",
            "  Batch 500/901 | Loss: 0.7963 | CLoss: 0.6071 | FLoss: 0.3784 | LR: 2.71e-04\n",
            "  Batch 510/901 | Loss: 0.5503 | CLoss: 0.4658 | FLoss: 0.1688 | LR: 2.71e-04\n",
            "  Batch 520/901 | Loss: 0.4460 | CLoss: 0.3196 | FLoss: 0.2527 | LR: 2.71e-04\n",
            "  Batch 530/901 | Loss: 0.5603 | CLoss: 0.4284 | FLoss: 0.2637 | LR: 2.71e-04\n",
            "  Batch 540/901 | Loss: 0.7767 | CLoss: 0.5620 | FLoss: 0.4293 | LR: 2.71e-04\n",
            "  Batch 550/901 | Loss: 1.0410 | CLoss: 0.8135 | FLoss: 0.4550 | LR: 2.71e-04\n",
            "  Batch 560/901 | Loss: 0.9107 | CLoss: 0.6829 | FLoss: 0.4555 | LR: 2.71e-04\n",
            "  Batch 570/901 | Loss: 0.9299 | CLoss: 0.6567 | FLoss: 0.5464 | LR: 2.71e-04\n",
            "  Batch 580/901 | Loss: 0.5739 | CLoss: 0.4297 | FLoss: 0.2884 | LR: 2.71e-04\n",
            "  Batch 590/901 | Loss: 0.4567 | CLoss: 0.3975 | FLoss: 0.1184 | LR: 2.71e-04\n",
            "  Batch 600/901 | Loss: 0.7657 | CLoss: 0.6459 | FLoss: 0.2398 | LR: 2.71e-04\n",
            "  Batch 610/901 | Loss: 0.5643 | CLoss: 0.4292 | FLoss: 0.2702 | LR: 2.71e-04\n",
            "  Batch 620/901 | Loss: 0.5220 | CLoss: 0.4049 | FLoss: 0.2342 | LR: 2.71e-04\n",
            "  Batch 630/901 | Loss: 0.5960 | CLoss: 0.5050 | FLoss: 0.1820 | LR: 2.71e-04\n",
            "  Batch 640/901 | Loss: 0.8395 | CLoss: 0.6638 | FLoss: 0.3515 | LR: 2.71e-04\n",
            "  Batch 650/901 | Loss: 0.9162 | CLoss: 0.7453 | FLoss: 0.3417 | LR: 2.71e-04\n",
            "  Batch 660/901 | Loss: 1.2488 | CLoss: 1.0113 | FLoss: 0.4750 | LR: 2.71e-04\n",
            "  Batch 670/901 | Loss: 0.9390 | CLoss: 0.7502 | FLoss: 0.3777 | LR: 2.71e-04\n",
            "  Batch 680/901 | Loss: 0.7537 | CLoss: 0.6213 | FLoss: 0.2648 | LR: 2.71e-04\n",
            "  Batch 690/901 | Loss: 0.8591 | CLoss: 0.6533 | FLoss: 0.4114 | LR: 2.71e-04\n",
            "  Batch 700/901 | Loss: 0.6218 | CLoss: 0.4606 | FLoss: 0.3223 | LR: 2.71e-04\n",
            "  Batch 710/901 | Loss: 0.8236 | CLoss: 0.6058 | FLoss: 0.4355 | LR: 2.71e-04\n",
            "  Batch 720/901 | Loss: 0.9506 | CLoss: 0.7717 | FLoss: 0.3577 | LR: 2.71e-04\n",
            "  Batch 730/901 | Loss: 0.7485 | CLoss: 0.5370 | FLoss: 0.4231 | LR: 2.71e-04\n",
            "  Batch 740/901 | Loss: 0.6814 | CLoss: 0.4468 | FLoss: 0.4693 | LR: 2.71e-04\n",
            "  Batch 750/901 | Loss: 0.7073 | CLoss: 0.5285 | FLoss: 0.3575 | LR: 2.71e-04\n",
            "  Batch 760/901 | Loss: 0.7037 | CLoss: 0.5745 | FLoss: 0.2584 | LR: 2.71e-04\n",
            "  Batch 770/901 | Loss: 0.7384 | CLoss: 0.5786 | FLoss: 0.3194 | LR: 2.71e-04\n",
            "  Batch 780/901 | Loss: 0.8377 | CLoss: 0.6395 | FLoss: 0.3963 | LR: 2.71e-04\n",
            "  Batch 790/901 | Loss: 0.5739 | CLoss: 0.4074 | FLoss: 0.3331 | LR: 2.71e-04\n",
            "  Batch 800/901 | Loss: 0.4871 | CLoss: 0.4108 | FLoss: 0.1528 | LR: 2.71e-04\n",
            "  Batch 810/901 | Loss: 0.5571 | CLoss: 0.4632 | FLoss: 0.1877 | LR: 2.71e-04\n",
            "  Batch 820/901 | Loss: 0.9135 | CLoss: 0.7188 | FLoss: 0.3894 | LR: 2.71e-04\n",
            "  Batch 830/901 | Loss: 0.6182 | CLoss: 0.4255 | FLoss: 0.3855 | LR: 2.71e-04\n",
            "  Batch 840/901 | Loss: 0.5842 | CLoss: 0.5040 | FLoss: 0.1605 | LR: 2.71e-04\n",
            "  Batch 850/901 | Loss: 0.3612 | CLoss: 0.2833 | FLoss: 0.1558 | LR: 2.71e-04\n",
            "  Batch 860/901 | Loss: 1.1339 | CLoss: 0.8424 | FLoss: 0.5831 | LR: 2.71e-04\n",
            "  Batch 870/901 | Loss: 0.7975 | CLoss: 0.6350 | FLoss: 0.3250 | LR: 2.71e-04\n",
            "  Batch 880/901 | Loss: 0.6082 | CLoss: 0.3905 | FLoss: 0.4355 | LR: 2.71e-04\n",
            "  Batch 890/901 | Loss: 0.8474 | CLoss: 0.6663 | FLoss: 0.3623 | LR: 2.71e-04\n",
            "  Batch 900/901 | Loss: 0.8149 | CLoss: 0.5827 | FLoss: 0.4644 | LR: 2.71e-04\n",
            "  Batch 901/901 | Loss: 0.6533 | CLoss: 0.2776 | FLoss: 0.7514 | LR: 2.71e-04\n",
            "\n",
            "  Training Summary | Epoch 2\n",
            "  Avg Loss: 0.7612\n",
            "  Last Batch Loss: 0.6533\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.4125 | Batch Acc: 81.03%\n",
            "    Val Batch 010/99 | Loss: 0.4700 | Batch Acc: 82.76%\n",
            "    Val Batch 015/99 | Loss: 0.3146 | Batch Acc: 93.10%\n",
            "    Val Batch 020/99 | Loss: 1.0633 | Batch Acc: 74.14%\n",
            "    Val Batch 025/99 | Loss: 0.7933 | Batch Acc: 74.14%\n",
            "    Val Batch 030/99 | Loss: 0.4503 | Batch Acc: 86.21%\n",
            "    Val Batch 035/99 | Loss: 0.1511 | Batch Acc: 89.66%\n",
            "    Val Batch 040/99 | Loss: 0.6614 | Batch Acc: 56.90%\n",
            "    Val Batch 045/99 | Loss: 0.1906 | Batch Acc: 93.10%\n",
            "    Val Batch 050/99 | Loss: 0.3497 | Batch Acc: 86.21%\n",
            "    Val Batch 055/99 | Loss: 0.2604 | Batch Acc: 93.10%\n",
            "    Val Batch 060/99 | Loss: 0.4133 | Batch Acc: 87.93%\n",
            "    Val Batch 065/99 | Loss: 0.5643 | Batch Acc: 81.03%\n",
            "    Val Batch 070/99 | Loss: 0.2035 | Batch Acc: 94.83%\n",
            "    Val Batch 075/99 | Loss: 0.2074 | Batch Acc: 91.38%\n",
            "    Val Batch 080/99 | Loss: 0.3128 | Batch Acc: 87.93%\n",
            "    Val Batch 085/99 | Loss: 0.0930 | Batch Acc: 96.55%\n",
            "    Val Batch 090/99 | Loss: 0.3177 | Batch Acc: 89.66%\n",
            "    Val Batch 095/99 | Loss: 0.1443 | Batch Acc: 94.83%\n",
            "    Val Batch 099/99 | Loss: 0.3494 | Batch Acc: 81.82%\n",
            "\n",
            "  Validation Summary | Epoch 2\n",
            "  Avg Loss: 0.4178 | Accuracy: 85.37%\n",
            "  Current Best Acc: 85.37%\n",
            "\n",
            "Epoch 3/5\n",
            "  Batch 010/901 | Loss: 0.6411 | CLoss: 0.4899 | FLoss: 0.3024 | LR: 1.96e-04\n",
            "  Batch 020/901 | Loss: 0.6384 | CLoss: 0.5259 | FLoss: 0.2250 | LR: 1.96e-04\n",
            "  Batch 030/901 | Loss: 0.6319 | CLoss: 0.5129 | FLoss: 0.2381 | LR: 1.96e-04\n",
            "  Batch 040/901 | Loss: 0.7527 | CLoss: 0.5897 | FLoss: 0.3259 | LR: 1.96e-04\n",
            "  Batch 050/901 | Loss: 0.6894 | CLoss: 0.5076 | FLoss: 0.3635 | LR: 1.96e-04\n",
            "  Batch 060/901 | Loss: 0.9871 | CLoss: 0.7480 | FLoss: 0.4782 | LR: 1.96e-04\n",
            "  Batch 070/901 | Loss: 1.0050 | CLoss: 0.8008 | FLoss: 0.4085 | LR: 1.96e-04\n",
            "  Batch 080/901 | Loss: 0.5824 | CLoss: 0.4433 | FLoss: 0.2781 | LR: 1.96e-04\n",
            "  Batch 090/901 | Loss: 0.8200 | CLoss: 0.6714 | FLoss: 0.2972 | LR: 1.96e-04\n",
            "  Batch 100/901 | Loss: 0.5111 | CLoss: 0.4139 | FLoss: 0.1945 | LR: 1.96e-04\n",
            "  Batch 110/901 | Loss: 0.6587 | CLoss: 0.5395 | FLoss: 0.2383 | LR: 1.96e-04\n",
            "  Batch 120/901 | Loss: 0.4938 | CLoss: 0.4209 | FLoss: 0.1459 | LR: 1.96e-04\n",
            "  Batch 130/901 | Loss: 0.4624 | CLoss: 0.3566 | FLoss: 0.2116 | LR: 1.96e-04\n",
            "  Batch 140/901 | Loss: 0.6210 | CLoss: 0.4821 | FLoss: 0.2779 | LR: 1.96e-04\n",
            "  Batch 150/901 | Loss: 0.5870 | CLoss: 0.4674 | FLoss: 0.2391 | LR: 1.96e-04\n",
            "  Batch 160/901 | Loss: 0.5216 | CLoss: 0.3103 | FLoss: 0.4228 | LR: 1.96e-04\n",
            "  Batch 170/901 | Loss: 0.8783 | CLoss: 0.6854 | FLoss: 0.3857 | LR: 1.96e-04\n",
            "  Batch 180/901 | Loss: 0.8224 | CLoss: 0.6416 | FLoss: 0.3616 | LR: 1.96e-04\n",
            "  Batch 190/901 | Loss: 0.5565 | CLoss: 0.4521 | FLoss: 0.2088 | LR: 1.96e-04\n",
            "  Batch 200/901 | Loss: 0.6260 | CLoss: 0.4877 | FLoss: 0.2767 | LR: 1.96e-04\n",
            "  Batch 210/901 | Loss: 0.6368 | CLoss: 0.5200 | FLoss: 0.2335 | LR: 1.96e-04\n",
            "  Batch 220/901 | Loss: 0.5304 | CLoss: 0.4090 | FLoss: 0.2428 | LR: 1.96e-04\n",
            "  Batch 230/901 | Loss: 0.7123 | CLoss: 0.5714 | FLoss: 0.2818 | LR: 1.96e-04\n",
            "  Batch 240/901 | Loss: 0.6985 | CLoss: 0.5412 | FLoss: 0.3146 | LR: 1.96e-04\n",
            "  Batch 250/901 | Loss: 0.6605 | CLoss: 0.4815 | FLoss: 0.3580 | LR: 1.96e-04\n",
            "  Batch 260/901 | Loss: 0.4818 | CLoss: 0.3552 | FLoss: 0.2532 | LR: 1.96e-04\n",
            "  Batch 270/901 | Loss: 0.5409 | CLoss: 0.3882 | FLoss: 0.3054 | LR: 1.96e-04\n",
            "  Batch 280/901 | Loss: 0.3929 | CLoss: 0.3016 | FLoss: 0.1828 | LR: 1.96e-04\n",
            "  Batch 290/901 | Loss: 0.6112 | CLoss: 0.4120 | FLoss: 0.3984 | LR: 1.96e-04\n",
            "  Batch 300/901 | Loss: 0.8365 | CLoss: 0.6079 | FLoss: 0.4573 | LR: 1.96e-04\n",
            "  Batch 310/901 | Loss: 0.6131 | CLoss: 0.4278 | FLoss: 0.3707 | LR: 1.96e-04\n",
            "  Batch 320/901 | Loss: 0.6095 | CLoss: 0.4116 | FLoss: 0.3958 | LR: 1.96e-04\n",
            "  Batch 330/901 | Loss: 0.9994 | CLoss: 0.7971 | FLoss: 0.4046 | LR: 1.96e-04\n",
            "  Batch 340/901 | Loss: 0.4158 | CLoss: 0.3729 | FLoss: 0.0858 | LR: 1.96e-04\n",
            "  Batch 350/901 | Loss: 0.7899 | CLoss: 0.6629 | FLoss: 0.2540 | LR: 1.96e-04\n",
            "  Batch 360/901 | Loss: 0.3990 | CLoss: 0.2117 | FLoss: 0.3746 | LR: 1.96e-04\n",
            "  Batch 370/901 | Loss: 0.9026 | CLoss: 0.6805 | FLoss: 0.4441 | LR: 1.96e-04\n",
            "  Batch 380/901 | Loss: 0.5269 | CLoss: 0.4235 | FLoss: 0.2069 | LR: 1.96e-04\n",
            "  Batch 390/901 | Loss: 0.6820 | CLoss: 0.5877 | FLoss: 0.1886 | LR: 1.96e-04\n",
            "  Batch 400/901 | Loss: 0.4087 | CLoss: 0.2516 | FLoss: 0.3141 | LR: 1.96e-04\n",
            "  Batch 410/901 | Loss: 0.7436 | CLoss: 0.5752 | FLoss: 0.3368 | LR: 1.96e-04\n",
            "  Batch 420/901 | Loss: 0.5276 | CLoss: 0.4674 | FLoss: 0.1204 | LR: 1.96e-04\n",
            "  Batch 430/901 | Loss: 0.6829 | CLoss: 0.4981 | FLoss: 0.3695 | LR: 1.96e-04\n",
            "  Batch 440/901 | Loss: 0.5343 | CLoss: 0.3371 | FLoss: 0.3944 | LR: 1.96e-04\n",
            "  Batch 450/901 | Loss: 0.4574 | CLoss: 0.3531 | FLoss: 0.2087 | LR: 1.96e-04\n",
            "  Batch 460/901 | Loss: 0.3729 | CLoss: 0.2409 | FLoss: 0.2640 | LR: 1.96e-04\n",
            "  Batch 470/901 | Loss: 0.9509 | CLoss: 0.7399 | FLoss: 0.4220 | LR: 1.96e-04\n",
            "  Batch 480/901 | Loss: 0.4170 | CLoss: 0.3099 | FLoss: 0.2141 | LR: 1.96e-04\n",
            "  Batch 490/901 | Loss: 0.9545 | CLoss: 0.7338 | FLoss: 0.4414 | LR: 1.96e-04\n",
            "  Batch 500/901 | Loss: 0.5011 | CLoss: 0.4163 | FLoss: 0.1696 | LR: 1.96e-04\n",
            "  Batch 510/901 | Loss: 0.5371 | CLoss: 0.3779 | FLoss: 0.3185 | LR: 1.96e-04\n",
            "  Batch 520/901 | Loss: 0.4118 | CLoss: 0.3013 | FLoss: 0.2210 | LR: 1.96e-04\n",
            "  Batch 530/901 | Loss: 0.6234 | CLoss: 0.4178 | FLoss: 0.4111 | LR: 1.96e-04\n",
            "  Batch 540/901 | Loss: 0.7138 | CLoss: 0.5529 | FLoss: 0.3219 | LR: 1.96e-04\n",
            "  Batch 550/901 | Loss: 1.2517 | CLoss: 1.0393 | FLoss: 0.4249 | LR: 1.96e-04\n",
            "  Batch 560/901 | Loss: 0.7103 | CLoss: 0.5896 | FLoss: 0.2414 | LR: 1.96e-04\n",
            "  Batch 570/901 | Loss: 0.7661 | CLoss: 0.6191 | FLoss: 0.2939 | LR: 1.96e-04\n",
            "  Batch 580/901 | Loss: 0.8103 | CLoss: 0.6991 | FLoss: 0.2225 | LR: 1.96e-04\n",
            "  Batch 590/901 | Loss: 0.5401 | CLoss: 0.4026 | FLoss: 0.2750 | LR: 1.96e-04\n",
            "  Batch 600/901 | Loss: 0.5543 | CLoss: 0.4457 | FLoss: 0.2171 | LR: 1.96e-04\n",
            "  Batch 610/901 | Loss: 0.5247 | CLoss: 0.4075 | FLoss: 0.2344 | LR: 1.96e-04\n",
            "  Batch 620/901 | Loss: 0.7820 | CLoss: 0.6867 | FLoss: 0.1908 | LR: 1.96e-04\n",
            "  Batch 630/901 | Loss: 0.7517 | CLoss: 0.6288 | FLoss: 0.2459 | LR: 1.96e-04\n",
            "  Batch 640/901 | Loss: 0.5730 | CLoss: 0.4736 | FLoss: 0.1988 | LR: 1.96e-04\n",
            "  Batch 650/901 | Loss: 0.7914 | CLoss: 0.5707 | FLoss: 0.4415 | LR: 1.96e-04\n",
            "  Batch 660/901 | Loss: 0.7847 | CLoss: 0.5481 | FLoss: 0.4734 | LR: 1.96e-04\n",
            "  Batch 670/901 | Loss: 0.7333 | CLoss: 0.5752 | FLoss: 0.3163 | LR: 1.96e-04\n",
            "  Batch 680/901 | Loss: 0.6216 | CLoss: 0.4259 | FLoss: 0.3913 | LR: 1.96e-04\n",
            "  Batch 690/901 | Loss: 0.6701 | CLoss: 0.5053 | FLoss: 0.3296 | LR: 1.96e-04\n",
            "  Batch 700/901 | Loss: 0.4184 | CLoss: 0.3335 | FLoss: 0.1698 | LR: 1.96e-04\n",
            "  Batch 710/901 | Loss: 0.5702 | CLoss: 0.3951 | FLoss: 0.3502 | LR: 1.96e-04\n",
            "  Batch 720/901 | Loss: 0.7551 | CLoss: 0.6511 | FLoss: 0.2081 | LR: 1.96e-04\n",
            "  Batch 730/901 | Loss: 1.0240 | CLoss: 0.8099 | FLoss: 0.4281 | LR: 1.96e-04\n",
            "  Batch 740/901 | Loss: 1.1487 | CLoss: 0.9101 | FLoss: 0.4771 | LR: 1.96e-04\n",
            "  Batch 750/901 | Loss: 0.7307 | CLoss: 0.5954 | FLoss: 0.2704 | LR: 1.96e-04\n",
            "  Batch 760/901 | Loss: 0.7250 | CLoss: 0.5783 | FLoss: 0.2932 | LR: 1.96e-04\n",
            "  Batch 770/901 | Loss: 0.8308 | CLoss: 0.7106 | FLoss: 0.2405 | LR: 1.96e-04\n",
            "  Batch 780/901 | Loss: 0.4501 | CLoss: 0.2816 | FLoss: 0.3369 | LR: 1.96e-04\n",
            "  Batch 790/901 | Loss: 1.2692 | CLoss: 1.0111 | FLoss: 0.5162 | LR: 1.96e-04\n",
            "  Batch 800/901 | Loss: 0.9702 | CLoss: 0.7224 | FLoss: 0.4955 | LR: 1.96e-04\n",
            "  Batch 810/901 | Loss: 0.5454 | CLoss: 0.4070 | FLoss: 0.2769 | LR: 1.96e-04\n",
            "  Batch 820/901 | Loss: 0.6621 | CLoss: 0.4927 | FLoss: 0.3388 | LR: 1.96e-04\n",
            "  Batch 830/901 | Loss: 0.6500 | CLoss: 0.5101 | FLoss: 0.2798 | LR: 1.96e-04\n",
            "  Batch 840/901 | Loss: 0.6996 | CLoss: 0.6320 | FLoss: 0.1352 | LR: 1.96e-04\n",
            "  Batch 850/901 | Loss: 0.5951 | CLoss: 0.4052 | FLoss: 0.3798 | LR: 1.96e-04\n",
            "  Batch 860/901 | Loss: 0.7444 | CLoss: 0.6458 | FLoss: 0.1972 | LR: 1.96e-04\n",
            "  Batch 870/901 | Loss: 0.6007 | CLoss: 0.4243 | FLoss: 0.3527 | LR: 1.96e-04\n",
            "  Batch 880/901 | Loss: 0.5975 | CLoss: 0.5141 | FLoss: 0.1667 | LR: 1.96e-04\n",
            "  Batch 890/901 | Loss: 1.0955 | CLoss: 0.9373 | FLoss: 0.3165 | LR: 1.96e-04\n",
            "  Batch 900/901 | Loss: 0.9350 | CLoss: 0.8141 | FLoss: 0.2417 | LR: 1.96e-04\n",
            "  Batch 901/901 | Loss: 0.3552 | CLoss: 0.2517 | FLoss: 0.2070 | LR: 1.96e-04\n",
            "\n",
            "  Training Summary | Epoch 3\n",
            "  Avg Loss: 0.7005\n",
            "  Last Batch Loss: 0.3552\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.7827 | Batch Acc: 75.86%\n",
            "    Val Batch 010/99 | Loss: 0.1979 | Batch Acc: 93.10%\n",
            "    Val Batch 015/99 | Loss: 0.2766 | Batch Acc: 93.10%\n",
            "    Val Batch 020/99 | Loss: 0.9517 | Batch Acc: 27.59%\n",
            "    Val Batch 025/99 | Loss: 0.3771 | Batch Acc: 82.76%\n",
            "    Val Batch 030/99 | Loss: 0.2865 | Batch Acc: 84.48%\n",
            "    Val Batch 035/99 | Loss: 0.2474 | Batch Acc: 91.38%\n",
            "    Val Batch 040/99 | Loss: 0.4346 | Batch Acc: 89.66%\n",
            "    Val Batch 045/99 | Loss: 0.0628 | Batch Acc: 98.28%\n",
            "    Val Batch 050/99 | Loss: 0.0766 | Batch Acc: 96.55%\n",
            "    Val Batch 055/99 | Loss: 0.3258 | Batch Acc: 89.66%\n",
            "    Val Batch 060/99 | Loss: 0.3068 | Batch Acc: 86.21%\n",
            "    Val Batch 065/99 | Loss: 0.4878 | Batch Acc: 79.31%\n",
            "    Val Batch 070/99 | Loss: 0.1139 | Batch Acc: 94.83%\n",
            "    Val Batch 075/99 | Loss: 0.2372 | Batch Acc: 93.10%\n",
            "    Val Batch 080/99 | Loss: 0.2257 | Batch Acc: 93.10%\n",
            "    Val Batch 085/99 | Loss: 0.0561 | Batch Acc: 98.28%\n",
            "    Val Batch 090/99 | Loss: 0.2093 | Batch Acc: 96.55%\n",
            "    Val Batch 095/99 | Loss: 0.1587 | Batch Acc: 96.55%\n",
            "    Val Batch 099/99 | Loss: 0.2168 | Batch Acc: 90.91%\n",
            "\n",
            "  Validation Summary | Epoch 3\n",
            "  Avg Loss: 0.3494 | Accuracy: 85.78%\n",
            "  Current Best Acc: 85.78%\n",
            "\n",
            "Epoch 4/5\n",
            "  Batch 010/901 | Loss: 0.5927 | CLoss: 0.4251 | FLoss: 0.3352 | LR: 1.04e-04\n",
            "  Batch 020/901 | Loss: 0.7483 | CLoss: 0.6182 | FLoss: 0.2603 | LR: 1.04e-04\n",
            "  Batch 030/901 | Loss: 0.4792 | CLoss: 0.3638 | FLoss: 0.2308 | LR: 1.04e-04\n",
            "  Batch 040/901 | Loss: 0.3485 | CLoss: 0.1959 | FLoss: 0.3051 | LR: 1.04e-04\n",
            "  Batch 050/901 | Loss: 0.2795 | CLoss: 0.1864 | FLoss: 0.1860 | LR: 1.04e-04\n",
            "  Batch 060/901 | Loss: 0.8595 | CLoss: 0.6464 | FLoss: 0.4263 | LR: 1.04e-04\n",
            "  Batch 070/901 | Loss: 0.4974 | CLoss: 0.3882 | FLoss: 0.2185 | LR: 1.04e-04\n",
            "  Batch 080/901 | Loss: 0.7179 | CLoss: 0.5846 | FLoss: 0.2667 | LR: 1.04e-04\n",
            "  Batch 090/901 | Loss: 0.6653 | CLoss: 0.5437 | FLoss: 0.2433 | LR: 1.04e-04\n",
            "  Batch 100/901 | Loss: 0.4255 | CLoss: 0.3243 | FLoss: 0.2025 | LR: 1.04e-04\n",
            "  Batch 110/901 | Loss: 0.5659 | CLoss: 0.4724 | FLoss: 0.1870 | LR: 1.04e-04\n",
            "  Batch 120/901 | Loss: 0.7805 | CLoss: 0.5775 | FLoss: 0.4060 | LR: 1.04e-04\n",
            "  Batch 130/901 | Loss: 1.0761 | CLoss: 0.8335 | FLoss: 0.4853 | LR: 1.04e-04\n",
            "  Batch 140/901 | Loss: 0.6893 | CLoss: 0.5610 | FLoss: 0.2567 | LR: 1.04e-04\n",
            "  Batch 150/901 | Loss: 0.7061 | CLoss: 0.5302 | FLoss: 0.3518 | LR: 1.04e-04\n",
            "  Batch 160/901 | Loss: 0.7400 | CLoss: 0.5073 | FLoss: 0.4653 | LR: 1.04e-04\n",
            "  Batch 170/901 | Loss: 0.5408 | CLoss: 0.3761 | FLoss: 0.3295 | LR: 1.04e-04\n",
            "  Batch 180/901 | Loss: 0.4255 | CLoss: 0.3618 | FLoss: 0.1276 | LR: 1.04e-04\n",
            "  Batch 190/901 | Loss: 0.4541 | CLoss: 0.3262 | FLoss: 0.2557 | LR: 1.04e-04\n",
            "  Batch 200/901 | Loss: 0.6535 | CLoss: 0.5539 | FLoss: 0.1992 | LR: 1.04e-04\n",
            "  Batch 210/901 | Loss: 0.4526 | CLoss: 0.3429 | FLoss: 0.2195 | LR: 1.04e-04\n",
            "  Batch 220/901 | Loss: 0.9164 | CLoss: 0.7457 | FLoss: 0.3414 | LR: 1.04e-04\n",
            "  Batch 230/901 | Loss: 0.4875 | CLoss: 0.2901 | FLoss: 0.3948 | LR: 1.04e-04\n",
            "  Batch 240/901 | Loss: 0.4623 | CLoss: 0.3590 | FLoss: 0.2066 | LR: 1.04e-04\n",
            "  Batch 250/901 | Loss: 1.3804 | CLoss: 1.0953 | FLoss: 0.5701 | LR: 1.04e-04\n",
            "  Batch 260/901 | Loss: 0.5724 | CLoss: 0.4152 | FLoss: 0.3143 | LR: 1.04e-04\n",
            "  Batch 270/901 | Loss: 1.0911 | CLoss: 0.9241 | FLoss: 0.3339 | LR: 1.04e-04\n",
            "  Batch 280/901 | Loss: 0.8352 | CLoss: 0.6199 | FLoss: 0.4305 | LR: 1.04e-04\n",
            "  Batch 290/901 | Loss: 0.7319 | CLoss: 0.6029 | FLoss: 0.2581 | LR: 1.04e-04\n",
            "  Batch 300/901 | Loss: 0.9870 | CLoss: 0.7956 | FLoss: 0.3827 | LR: 1.04e-04\n",
            "  Batch 310/901 | Loss: 0.5089 | CLoss: 0.3826 | FLoss: 0.2527 | LR: 1.04e-04\n",
            "  Batch 320/901 | Loss: 0.6628 | CLoss: 0.5134 | FLoss: 0.2989 | LR: 1.04e-04\n",
            "  Batch 330/901 | Loss: 0.4537 | CLoss: 0.3911 | FLoss: 0.1254 | LR: 1.04e-04\n",
            "  Batch 340/901 | Loss: 0.4970 | CLoss: 0.4100 | FLoss: 0.1742 | LR: 1.04e-04\n",
            "  Batch 350/901 | Loss: 0.7222 | CLoss: 0.5590 | FLoss: 0.3264 | LR: 1.04e-04\n",
            "  Batch 360/901 | Loss: 0.7398 | CLoss: 0.5757 | FLoss: 0.3282 | LR: 1.04e-04\n",
            "  Batch 370/901 | Loss: 0.3928 | CLoss: 0.2091 | FLoss: 0.3675 | LR: 1.04e-04\n",
            "  Batch 380/901 | Loss: 0.8086 | CLoss: 0.6479 | FLoss: 0.3215 | LR: 1.04e-04\n",
            "  Batch 390/901 | Loss: 0.3404 | CLoss: 0.2693 | FLoss: 0.1423 | LR: 1.04e-04\n",
            "  Batch 400/901 | Loss: 0.6908 | CLoss: 0.5424 | FLoss: 0.2967 | LR: 1.04e-04\n",
            "  Batch 410/901 | Loss: 0.8843 | CLoss: 0.6640 | FLoss: 0.4406 | LR: 1.04e-04\n",
            "  Batch 420/901 | Loss: 0.4752 | CLoss: 0.2783 | FLoss: 0.3938 | LR: 1.04e-04\n",
            "  Batch 430/901 | Loss: 0.4154 | CLoss: 0.3054 | FLoss: 0.2198 | LR: 1.04e-04\n",
            "  Batch 440/901 | Loss: 0.8841 | CLoss: 0.8023 | FLoss: 0.1636 | LR: 1.04e-04\n",
            "  Batch 450/901 | Loss: 0.4713 | CLoss: 0.3886 | FLoss: 0.1654 | LR: 1.04e-04\n",
            "  Batch 460/901 | Loss: 0.4212 | CLoss: 0.3472 | FLoss: 0.1481 | LR: 1.04e-04\n",
            "  Batch 470/901 | Loss: 0.6535 | CLoss: 0.5245 | FLoss: 0.2579 | LR: 1.04e-04\n",
            "  Batch 480/901 | Loss: 0.4216 | CLoss: 0.3480 | FLoss: 0.1472 | LR: 1.04e-04\n",
            "  Batch 490/901 | Loss: 0.9644 | CLoss: 0.7452 | FLoss: 0.4386 | LR: 1.04e-04\n",
            "  Batch 500/901 | Loss: 0.9101 | CLoss: 0.7651 | FLoss: 0.2901 | LR: 1.04e-04\n",
            "  Batch 510/901 | Loss: 0.7547 | CLoss: 0.5805 | FLoss: 0.3482 | LR: 1.04e-04\n",
            "  Batch 520/901 | Loss: 0.7757 | CLoss: 0.6355 | FLoss: 0.2804 | LR: 1.04e-04\n",
            "  Batch 530/901 | Loss: 0.6773 | CLoss: 0.5079 | FLoss: 0.3388 | LR: 1.04e-04\n",
            "  Batch 540/901 | Loss: 0.5135 | CLoss: 0.4024 | FLoss: 0.2223 | LR: 1.04e-04\n",
            "  Batch 550/901 | Loss: 0.8762 | CLoss: 0.6472 | FLoss: 0.4580 | LR: 1.04e-04\n",
            "  Batch 560/901 | Loss: 0.4471 | CLoss: 0.3220 | FLoss: 0.2501 | LR: 1.04e-04\n",
            "  Batch 570/901 | Loss: 0.7301 | CLoss: 0.5048 | FLoss: 0.4506 | LR: 1.04e-04\n",
            "  Batch 580/901 | Loss: 0.6565 | CLoss: 0.5419 | FLoss: 0.2291 | LR: 1.04e-04\n",
            "  Batch 590/901 | Loss: 0.6989 | CLoss: 0.5855 | FLoss: 0.2268 | LR: 1.04e-04\n",
            "  Batch 600/901 | Loss: 1.0443 | CLoss: 0.8123 | FLoss: 0.4640 | LR: 1.04e-04\n",
            "  Batch 610/901 | Loss: 0.5178 | CLoss: 0.3880 | FLoss: 0.2596 | LR: 1.04e-04\n",
            "  Batch 620/901 | Loss: 0.5899 | CLoss: 0.5211 | FLoss: 0.1376 | LR: 1.04e-04\n",
            "  Batch 630/901 | Loss: 1.1469 | CLoss: 0.9293 | FLoss: 0.4353 | LR: 1.04e-04\n",
            "  Batch 640/901 | Loss: 0.5339 | CLoss: 0.3659 | FLoss: 0.3358 | LR: 1.04e-04\n",
            "  Batch 650/901 | Loss: 0.9628 | CLoss: 0.7850 | FLoss: 0.3556 | LR: 1.04e-04\n",
            "  Batch 660/901 | Loss: 0.5313 | CLoss: 0.4507 | FLoss: 0.1611 | LR: 1.04e-04\n",
            "  Batch 670/901 | Loss: 0.5764 | CLoss: 0.4048 | FLoss: 0.3434 | LR: 1.04e-04\n",
            "  Batch 680/901 | Loss: 0.8569 | CLoss: 0.7547 | FLoss: 0.2042 | LR: 1.04e-04\n",
            "  Batch 690/901 | Loss: 0.3296 | CLoss: 0.2437 | FLoss: 0.1718 | LR: 1.04e-04\n",
            "  Batch 700/901 | Loss: 0.7984 | CLoss: 0.5996 | FLoss: 0.3976 | LR: 1.04e-04\n",
            "  Batch 710/901 | Loss: 0.8551 | CLoss: 0.7286 | FLoss: 0.2530 | LR: 1.04e-04\n",
            "  Batch 720/901 | Loss: 0.4978 | CLoss: 0.3778 | FLoss: 0.2400 | LR: 1.04e-04\n",
            "  Batch 730/901 | Loss: 0.9995 | CLoss: 0.8117 | FLoss: 0.3756 | LR: 1.04e-04\n",
            "  Batch 740/901 | Loss: 0.6640 | CLoss: 0.5164 | FLoss: 0.2953 | LR: 1.04e-04\n",
            "  Batch 750/901 | Loss: 0.2664 | CLoss: 0.2138 | FLoss: 0.1052 | LR: 1.04e-04\n",
            "  Batch 760/901 | Loss: 0.8600 | CLoss: 0.7089 | FLoss: 0.3021 | LR: 1.04e-04\n",
            "  Batch 770/901 | Loss: 0.7387 | CLoss: 0.5957 | FLoss: 0.2859 | LR: 1.04e-04\n",
            "  Batch 780/901 | Loss: 0.5530 | CLoss: 0.3997 | FLoss: 0.3066 | LR: 1.04e-04\n",
            "  Batch 790/901 | Loss: 0.5587 | CLoss: 0.3944 | FLoss: 0.3287 | LR: 1.04e-04\n",
            "  Batch 800/901 | Loss: 0.9769 | CLoss: 0.7383 | FLoss: 0.4772 | LR: 1.04e-04\n",
            "  Batch 810/901 | Loss: 0.4121 | CLoss: 0.3358 | FLoss: 0.1527 | LR: 1.04e-04\n",
            "  Batch 820/901 | Loss: 0.3273 | CLoss: 0.2753 | FLoss: 0.1039 | LR: 1.04e-04\n",
            "  Batch 830/901 | Loss: 0.7787 | CLoss: 0.5980 | FLoss: 0.3614 | LR: 1.04e-04\n",
            "  Batch 840/901 | Loss: 0.4773 | CLoss: 0.3950 | FLoss: 0.1645 | LR: 1.04e-04\n",
            "  Batch 850/901 | Loss: 0.4558 | CLoss: 0.3215 | FLoss: 0.2687 | LR: 1.04e-04\n",
            "  Batch 860/901 | Loss: 0.4726 | CLoss: 0.3050 | FLoss: 0.3351 | LR: 1.04e-04\n",
            "  Batch 870/901 | Loss: 0.9533 | CLoss: 0.6882 | FLoss: 0.5303 | LR: 1.04e-04\n",
            "  Batch 880/901 | Loss: 0.5392 | CLoss: 0.4475 | FLoss: 0.1834 | LR: 1.04e-04\n",
            "  Batch 890/901 | Loss: 0.5672 | CLoss: 0.4295 | FLoss: 0.2754 | LR: 1.04e-04\n",
            "  Batch 900/901 | Loss: 0.5912 | CLoss: 0.4604 | FLoss: 0.2617 | LR: 1.04e-04\n",
            "  Batch 901/901 | Loss: 0.3365 | CLoss: 0.1371 | FLoss: 0.3988 | LR: 1.04e-04\n",
            "\n",
            "  Training Summary | Epoch 4\n",
            "  Avg Loss: 0.6435\n",
            "  Last Batch Loss: 0.3365\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.6068 | Batch Acc: 75.86%\n",
            "    Val Batch 010/99 | Loss: 0.4438 | Batch Acc: 86.21%\n",
            "    Val Batch 015/99 | Loss: 0.3180 | Batch Acc: 89.66%\n",
            "    Val Batch 020/99 | Loss: 1.1060 | Batch Acc: 29.31%\n",
            "    Val Batch 025/99 | Loss: 0.3892 | Batch Acc: 81.03%\n",
            "    Val Batch 030/99 | Loss: 0.4836 | Batch Acc: 81.03%\n",
            "    Val Batch 035/99 | Loss: 0.1892 | Batch Acc: 93.10%\n",
            "    Val Batch 040/99 | Loss: 0.4218 | Batch Acc: 96.55%\n",
            "    Val Batch 045/99 | Loss: 0.1731 | Batch Acc: 94.83%\n",
            "    Val Batch 050/99 | Loss: 0.1255 | Batch Acc: 94.83%\n",
            "    Val Batch 055/99 | Loss: 0.1766 | Batch Acc: 93.10%\n",
            "    Val Batch 060/99 | Loss: 0.1606 | Batch Acc: 93.10%\n",
            "    Val Batch 065/99 | Loss: 0.4785 | Batch Acc: 84.48%\n",
            "    Val Batch 070/99 | Loss: 0.1708 | Batch Acc: 94.83%\n",
            "    Val Batch 075/99 | Loss: 0.1982 | Batch Acc: 94.83%\n",
            "    Val Batch 080/99 | Loss: 0.0296 | Batch Acc: 100.00%\n",
            "    Val Batch 085/99 | Loss: 0.2527 | Batch Acc: 94.83%\n",
            "    Val Batch 090/99 | Loss: 0.0043 | Batch Acc: 100.00%\n",
            "    Val Batch 095/99 | Loss: 0.1687 | Batch Acc: 96.55%\n",
            "    Val Batch 099/99 | Loss: 0.0529 | Batch Acc: 100.00%\n",
            "\n",
            "  Validation Summary | Epoch 4\n",
            "  Avg Loss: 0.3408 | Accuracy: 86.62%\n",
            "  Current Best Acc: 86.62%\n",
            "\n",
            "Epoch 5/5\n",
            "  Batch 010/901 | Loss: 0.4868 | CLoss: 0.3124 | FLoss: 0.3487 | LR: 2.86e-05\n",
            "  Batch 020/901 | Loss: 1.0775 | CLoss: 0.9245 | FLoss: 0.3061 | LR: 2.86e-05\n",
            "  Batch 030/901 | Loss: 1.1166 | CLoss: 0.8956 | FLoss: 0.4419 | LR: 2.86e-05\n",
            "  Batch 040/901 | Loss: 0.7745 | CLoss: 0.6300 | FLoss: 0.2890 | LR: 2.86e-05\n",
            "  Batch 050/901 | Loss: 0.2896 | CLoss: 0.2072 | FLoss: 0.1647 | LR: 2.86e-05\n",
            "  Batch 060/901 | Loss: 0.1143 | CLoss: 0.0846 | FLoss: 0.0594 | LR: 2.86e-05\n",
            "  Batch 070/901 | Loss: 0.5671 | CLoss: 0.4527 | FLoss: 0.2288 | LR: 2.86e-05\n",
            "  Batch 080/901 | Loss: 0.2838 | CLoss: 0.2098 | FLoss: 0.1479 | LR: 2.86e-05\n",
            "  Batch 090/901 | Loss: 0.5136 | CLoss: 0.3624 | FLoss: 0.3023 | LR: 2.86e-05\n",
            "  Batch 100/901 | Loss: 0.5651 | CLoss: 0.4138 | FLoss: 0.3027 | LR: 2.86e-05\n",
            "  Batch 110/901 | Loss: 0.8075 | CLoss: 0.6729 | FLoss: 0.2692 | LR: 2.86e-05\n",
            "  Batch 120/901 | Loss: 0.5688 | CLoss: 0.4542 | FLoss: 0.2292 | LR: 2.86e-05\n",
            "  Batch 130/901 | Loss: 0.4145 | CLoss: 0.2790 | FLoss: 0.2711 | LR: 2.86e-05\n",
            "  Batch 140/901 | Loss: 0.3141 | CLoss: 0.1874 | FLoss: 0.2534 | LR: 2.86e-05\n",
            "  Batch 150/901 | Loss: 0.5813 | CLoss: 0.4082 | FLoss: 0.3462 | LR: 2.86e-05\n",
            "  Batch 160/901 | Loss: 0.8471 | CLoss: 0.6349 | FLoss: 0.4245 | LR: 2.86e-05\n",
            "  Batch 170/901 | Loss: 0.5822 | CLoss: 0.4744 | FLoss: 0.2157 | LR: 2.86e-05\n",
            "  Batch 180/901 | Loss: 1.0648 | CLoss: 0.9316 | FLoss: 0.2663 | LR: 2.86e-05\n",
            "  Batch 190/901 | Loss: 0.4373 | CLoss: 0.2447 | FLoss: 0.3852 | LR: 2.86e-05\n",
            "  Batch 200/901 | Loss: 0.6487 | CLoss: 0.5570 | FLoss: 0.1834 | LR: 2.86e-05\n",
            "  Batch 210/901 | Loss: 0.4376 | CLoss: 0.3702 | FLoss: 0.1348 | LR: 2.86e-05\n",
            "  Batch 220/901 | Loss: 0.4412 | CLoss: 0.2994 | FLoss: 0.2837 | LR: 2.86e-05\n",
            "  Batch 230/901 | Loss: 0.3271 | CLoss: 0.2510 | FLoss: 0.1523 | LR: 2.86e-05\n",
            "  Batch 240/901 | Loss: 0.6869 | CLoss: 0.5100 | FLoss: 0.3538 | LR: 2.86e-05\n",
            "  Batch 250/901 | Loss: 0.9775 | CLoss: 0.7217 | FLoss: 0.5118 | LR: 2.86e-05\n",
            "  Batch 260/901 | Loss: 0.4167 | CLoss: 0.3657 | FLoss: 0.1020 | LR: 2.86e-05\n",
            "  Batch 270/901 | Loss: 0.8516 | CLoss: 0.7067 | FLoss: 0.2899 | LR: 2.86e-05\n",
            "  Batch 280/901 | Loss: 0.4125 | CLoss: 0.3268 | FLoss: 0.1713 | LR: 2.86e-05\n",
            "  Batch 290/901 | Loss: 0.9335 | CLoss: 0.6993 | FLoss: 0.4683 | LR: 2.86e-05\n",
            "  Batch 300/901 | Loss: 0.6015 | CLoss: 0.4845 | FLoss: 0.2340 | LR: 2.86e-05\n",
            "  Batch 310/901 | Loss: 0.4288 | CLoss: 0.3776 | FLoss: 0.1025 | LR: 2.86e-05\n",
            "  Batch 320/901 | Loss: 1.0227 | CLoss: 0.8210 | FLoss: 0.4034 | LR: 2.86e-05\n",
            "  Batch 330/901 | Loss: 0.6512 | CLoss: 0.4780 | FLoss: 0.3464 | LR: 2.86e-05\n",
            "  Batch 340/901 | Loss: 0.3224 | CLoss: 0.2400 | FLoss: 0.1649 | LR: 2.86e-05\n",
            "  Batch 350/901 | Loss: 0.2944 | CLoss: 0.2648 | FLoss: 0.0591 | LR: 2.86e-05\n",
            "  Batch 360/901 | Loss: 0.4385 | CLoss: 0.3198 | FLoss: 0.2375 | LR: 2.86e-05\n",
            "  Batch 370/901 | Loss: 0.6721 | CLoss: 0.5444 | FLoss: 0.2554 | LR: 2.86e-05\n",
            "  Batch 380/901 | Loss: 0.5560 | CLoss: 0.4567 | FLoss: 0.1986 | LR: 2.86e-05\n",
            "  Batch 390/901 | Loss: 0.8402 | CLoss: 0.6648 | FLoss: 0.3507 | LR: 2.86e-05\n",
            "  Batch 400/901 | Loss: 0.4563 | CLoss: 0.3431 | FLoss: 0.2263 | LR: 2.86e-05\n",
            "  Batch 410/901 | Loss: 0.8484 | CLoss: 0.6900 | FLoss: 0.3168 | LR: 2.86e-05\n",
            "  Batch 420/901 | Loss: 0.4003 | CLoss: 0.3176 | FLoss: 0.1653 | LR: 2.86e-05\n",
            "  Batch 430/901 | Loss: 0.3602 | CLoss: 0.2948 | FLoss: 0.1307 | LR: 2.86e-05\n",
            "  Batch 440/901 | Loss: 0.6056 | CLoss: 0.4880 | FLoss: 0.2352 | LR: 2.86e-05\n",
            "  Batch 450/901 | Loss: 0.6865 | CLoss: 0.5675 | FLoss: 0.2381 | LR: 2.86e-05\n",
            "  Batch 460/901 | Loss: 0.5836 | CLoss: 0.4953 | FLoss: 0.1764 | LR: 2.86e-05\n",
            "  Batch 470/901 | Loss: 0.6606 | CLoss: 0.5349 | FLoss: 0.2515 | LR: 2.86e-05\n",
            "  Batch 480/901 | Loss: 0.6662 | CLoss: 0.5063 | FLoss: 0.3197 | LR: 2.86e-05\n",
            "  Batch 490/901 | Loss: 1.0272 | CLoss: 0.8128 | FLoss: 0.4288 | LR: 2.86e-05\n",
            "  Batch 500/901 | Loss: 0.6716 | CLoss: 0.5030 | FLoss: 0.3372 | LR: 2.86e-05\n",
            "  Batch 510/901 | Loss: 0.5427 | CLoss: 0.4104 | FLoss: 0.2647 | LR: 2.86e-05\n",
            "  Batch 520/901 | Loss: 0.7893 | CLoss: 0.5773 | FLoss: 0.4241 | LR: 2.86e-05\n",
            "  Batch 530/901 | Loss: 0.7038 | CLoss: 0.5052 | FLoss: 0.3971 | LR: 2.86e-05\n",
            "  Batch 540/901 | Loss: 0.6462 | CLoss: 0.4933 | FLoss: 0.3059 | LR: 2.86e-05\n",
            "  Batch 550/901 | Loss: 0.9712 | CLoss: 0.8223 | FLoss: 0.2979 | LR: 2.86e-05\n",
            "  Batch 560/901 | Loss: 0.5149 | CLoss: 0.4180 | FLoss: 0.1938 | LR: 2.86e-05\n",
            "  Batch 570/901 | Loss: 0.3970 | CLoss: 0.3093 | FLoss: 0.1754 | LR: 2.86e-05\n",
            "  Batch 580/901 | Loss: 0.3743 | CLoss: 0.2683 | FLoss: 0.2119 | LR: 2.86e-05\n",
            "  Batch 590/901 | Loss: 0.8493 | CLoss: 0.6640 | FLoss: 0.3706 | LR: 2.86e-05\n",
            "  Batch 600/901 | Loss: 0.7823 | CLoss: 0.6917 | FLoss: 0.1812 | LR: 2.86e-05\n",
            "  Batch 610/901 | Loss: 0.8556 | CLoss: 0.7056 | FLoss: 0.2999 | LR: 2.86e-05\n",
            "  Batch 620/901 | Loss: 0.5169 | CLoss: 0.4039 | FLoss: 0.2260 | LR: 2.86e-05\n",
            "  Batch 630/901 | Loss: 0.8599 | CLoss: 0.6567 | FLoss: 0.4063 | LR: 2.86e-05\n",
            "  Batch 640/901 | Loss: 0.6448 | CLoss: 0.4848 | FLoss: 0.3200 | LR: 2.86e-05\n",
            "  Batch 650/901 | Loss: 0.7288 | CLoss: 0.5784 | FLoss: 0.3009 | LR: 2.86e-05\n",
            "  Batch 660/901 | Loss: 0.5527 | CLoss: 0.4135 | FLoss: 0.2784 | LR: 2.86e-05\n",
            "  Batch 670/901 | Loss: 0.3277 | CLoss: 0.1880 | FLoss: 0.2794 | LR: 2.86e-05\n",
            "  Batch 680/901 | Loss: 0.8108 | CLoss: 0.6407 | FLoss: 0.3403 | LR: 2.86e-05\n",
            "  Batch 690/901 | Loss: 0.4203 | CLoss: 0.3056 | FLoss: 0.2294 | LR: 2.86e-05\n",
            "  Batch 700/901 | Loss: 0.5184 | CLoss: 0.3975 | FLoss: 0.2418 | LR: 2.86e-05\n",
            "  Batch 710/901 | Loss: 0.2504 | CLoss: 0.1451 | FLoss: 0.2105 | LR: 2.86e-05\n",
            "  Batch 720/901 | Loss: 0.4685 | CLoss: 0.3388 | FLoss: 0.2594 | LR: 2.86e-05\n",
            "  Batch 730/901 | Loss: 0.5764 | CLoss: 0.4450 | FLoss: 0.2629 | LR: 2.86e-05\n",
            "  Batch 740/901 | Loss: 0.3746 | CLoss: 0.2600 | FLoss: 0.2291 | LR: 2.86e-05\n",
            "  Batch 750/901 | Loss: 0.7797 | CLoss: 0.6947 | FLoss: 0.1700 | LR: 2.86e-05\n",
            "  Batch 760/901 | Loss: 0.6458 | CLoss: 0.5144 | FLoss: 0.2629 | LR: 2.86e-05\n",
            "  Batch 770/901 | Loss: 0.3218 | CLoss: 0.2128 | FLoss: 0.2178 | LR: 2.86e-05\n",
            "  Batch 780/901 | Loss: 0.3301 | CLoss: 0.2247 | FLoss: 0.2109 | LR: 2.86e-05\n",
            "  Batch 790/901 | Loss: 0.7902 | CLoss: 0.5603 | FLoss: 0.4599 | LR: 2.86e-05\n",
            "  Batch 800/901 | Loss: 0.4804 | CLoss: 0.3846 | FLoss: 0.1916 | LR: 2.86e-05\n",
            "  Batch 810/901 | Loss: 0.5704 | CLoss: 0.4974 | FLoss: 0.1459 | LR: 2.86e-05\n",
            "  Batch 820/901 | Loss: 0.7973 | CLoss: 0.6045 | FLoss: 0.3855 | LR: 2.86e-05\n",
            "  Batch 830/901 | Loss: 0.7312 | CLoss: 0.5972 | FLoss: 0.2680 | LR: 2.86e-05\n",
            "  Batch 840/901 | Loss: 0.6658 | CLoss: 0.5467 | FLoss: 0.2383 | LR: 2.86e-05\n",
            "  Batch 850/901 | Loss: 0.5671 | CLoss: 0.3680 | FLoss: 0.3982 | LR: 2.86e-05\n",
            "  Batch 860/901 | Loss: 0.5907 | CLoss: 0.4754 | FLoss: 0.2306 | LR: 2.86e-05\n",
            "  Batch 870/901 | Loss: 0.7383 | CLoss: 0.5729 | FLoss: 0.3308 | LR: 2.86e-05\n",
            "  Batch 880/901 | Loss: 0.9903 | CLoss: 0.7699 | FLoss: 0.4409 | LR: 2.86e-05\n",
            "  Batch 890/901 | Loss: 0.9326 | CLoss: 0.7658 | FLoss: 0.3336 | LR: 2.86e-05\n",
            "  Batch 900/901 | Loss: 0.4287 | CLoss: 0.3635 | FLoss: 0.1305 | LR: 2.86e-05\n",
            "  Batch 901/901 | Loss: 0.4244 | CLoss: 0.2959 | FLoss: 0.2569 | LR: 2.86e-05\n",
            "\n",
            "  Training Summary | Epoch 5\n",
            "  Avg Loss: 0.5973\n",
            "  Last Batch Loss: 0.4244\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.4066 | Batch Acc: 84.48%\n",
            "    Val Batch 010/99 | Loss: 0.3201 | Batch Acc: 87.93%\n",
            "    Val Batch 015/99 | Loss: 0.2930 | Batch Acc: 87.93%\n",
            "    Val Batch 020/99 | Loss: 1.0374 | Batch Acc: 24.14%\n",
            "    Val Batch 025/99 | Loss: 0.1906 | Batch Acc: 93.10%\n",
            "    Val Batch 030/99 | Loss: 0.4332 | Batch Acc: 86.21%\n",
            "    Val Batch 035/99 | Loss: 0.2391 | Batch Acc: 94.83%\n",
            "    Val Batch 040/99 | Loss: 0.4405 | Batch Acc: 94.83%\n",
            "    Val Batch 045/99 | Loss: 0.1053 | Batch Acc: 94.83%\n",
            "    Val Batch 050/99 | Loss: 0.1504 | Batch Acc: 96.55%\n",
            "    Val Batch 055/99 | Loss: 0.3275 | Batch Acc: 89.66%\n",
            "    Val Batch 060/99 | Loss: 0.1876 | Batch Acc: 93.10%\n",
            "    Val Batch 065/99 | Loss: 0.2725 | Batch Acc: 89.66%\n",
            "    Val Batch 070/99 | Loss: 0.1134 | Batch Acc: 98.28%\n",
            "    Val Batch 075/99 | Loss: 0.0890 | Batch Acc: 94.83%\n",
            "    Val Batch 080/99 | Loss: 0.0715 | Batch Acc: 96.55%\n",
            "    Val Batch 085/99 | Loss: 0.1310 | Batch Acc: 96.55%\n",
            "    Val Batch 090/99 | Loss: 0.0297 | Batch Acc: 98.28%\n",
            "    Val Batch 095/99 | Loss: 0.0875 | Batch Acc: 96.55%\n",
            "    Val Batch 099/99 | Loss: 0.0020 | Batch Acc: 100.00%\n",
            "\n",
            "  Validation Summary | Epoch 5\n",
            "  Avg Loss: 0.3037 | Accuracy: 87.43%\n",
            "  Current Best Acc: 87.43%\n",
            "\n",
            "========================================\n",
            "=== Fold 9 Completed ===\n",
            "Best Validation Accuracy: 87.43%\n",
            "\n",
            "========================================\n",
            "=== Fold 10/10 ====================\n",
            "========================================\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "  Batch 010/900 | Loss: 0.8906 | CLoss: 0.6668 | FLoss: 0.4475 | LR: 3.00e-04\n",
            "  Batch 020/900 | Loss: 0.5988 | CLoss: 0.3851 | FLoss: 0.4274 | LR: 3.00e-04\n",
            "  Batch 030/900 | Loss: 1.0592 | CLoss: 0.7491 | FLoss: 0.6202 | LR: 3.00e-04\n",
            "  Batch 040/900 | Loss: 0.8985 | CLoss: 0.7176 | FLoss: 0.3619 | LR: 3.00e-04\n",
            "  Batch 050/900 | Loss: 0.7900 | CLoss: 0.6389 | FLoss: 0.3021 | LR: 3.00e-04\n",
            "  Batch 060/900 | Loss: 0.7048 | CLoss: 0.4588 | FLoss: 0.4920 | LR: 3.00e-04\n",
            "  Batch 070/900 | Loss: 1.0374 | CLoss: 0.8734 | FLoss: 0.3280 | LR: 3.00e-04\n",
            "  Batch 080/900 | Loss: 0.5464 | CLoss: 0.3828 | FLoss: 0.3272 | LR: 3.00e-04\n",
            "  Batch 090/900 | Loss: 0.5626 | CLoss: 0.4182 | FLoss: 0.2888 | LR: 3.00e-04\n",
            "  Batch 100/900 | Loss: 0.8252 | CLoss: 0.6541 | FLoss: 0.3420 | LR: 3.00e-04\n",
            "  Batch 110/900 | Loss: 0.8143 | CLoss: 0.6732 | FLoss: 0.2823 | LR: 3.00e-04\n",
            "  Batch 120/900 | Loss: 0.4626 | CLoss: 0.3915 | FLoss: 0.1422 | LR: 3.00e-04\n",
            "  Batch 130/900 | Loss: 0.5584 | CLoss: 0.4053 | FLoss: 0.3063 | LR: 3.00e-04\n",
            "  Batch 140/900 | Loss: 0.8890 | CLoss: 0.7128 | FLoss: 0.3524 | LR: 3.00e-04\n",
            "  Batch 150/900 | Loss: 0.3388 | CLoss: 0.1969 | FLoss: 0.2839 | LR: 3.00e-04\n",
            "  Batch 160/900 | Loss: 0.7421 | CLoss: 0.5154 | FLoss: 0.4533 | LR: 3.00e-04\n",
            "  Batch 170/900 | Loss: 0.5383 | CLoss: 0.4017 | FLoss: 0.2732 | LR: 3.00e-04\n",
            "  Batch 180/900 | Loss: 0.9338 | CLoss: 0.7565 | FLoss: 0.3546 | LR: 3.00e-04\n",
            "  Batch 190/900 | Loss: 1.0040 | CLoss: 0.6887 | FLoss: 0.6307 | LR: 3.00e-04\n",
            "  Batch 200/900 | Loss: 0.9511 | CLoss: 0.5899 | FLoss: 0.7224 | LR: 3.00e-04\n",
            "  Batch 210/900 | Loss: 1.1808 | CLoss: 0.9674 | FLoss: 0.4268 | LR: 3.00e-04\n",
            "  Batch 220/900 | Loss: 0.7318 | CLoss: 0.5205 | FLoss: 0.4226 | LR: 3.00e-04\n",
            "  Batch 230/900 | Loss: 0.9787 | CLoss: 0.6829 | FLoss: 0.5917 | LR: 3.00e-04\n",
            "  Batch 240/900 | Loss: 0.6337 | CLoss: 0.5108 | FLoss: 0.2459 | LR: 3.00e-04\n",
            "  Batch 250/900 | Loss: 1.0562 | CLoss: 0.8801 | FLoss: 0.3523 | LR: 3.00e-04\n",
            "  Batch 260/900 | Loss: 0.8814 | CLoss: 0.7358 | FLoss: 0.2911 | LR: 3.00e-04\n",
            "  Batch 270/900 | Loss: 0.6101 | CLoss: 0.5073 | FLoss: 0.2056 | LR: 3.00e-04\n",
            "  Batch 280/900 | Loss: 0.4047 | CLoss: 0.2429 | FLoss: 0.3237 | LR: 3.00e-04\n",
            "  Batch 290/900 | Loss: 0.9867 | CLoss: 0.8045 | FLoss: 0.3645 | LR: 3.00e-04\n",
            "  Batch 300/900 | Loss: 0.8196 | CLoss: 0.6073 | FLoss: 0.4245 | LR: 3.00e-04\n",
            "  Batch 310/900 | Loss: 0.5957 | CLoss: 0.4762 | FLoss: 0.2390 | LR: 3.00e-04\n",
            "  Batch 320/900 | Loss: 0.7640 | CLoss: 0.5639 | FLoss: 0.4003 | LR: 3.00e-04\n",
            "  Batch 330/900 | Loss: 0.7687 | CLoss: 0.5911 | FLoss: 0.3552 | LR: 3.00e-04\n",
            "  Batch 340/900 | Loss: 0.9383 | CLoss: 0.7400 | FLoss: 0.3966 | LR: 3.00e-04\n",
            "  Batch 350/900 | Loss: 0.6925 | CLoss: 0.4624 | FLoss: 0.4602 | LR: 3.00e-04\n",
            "  Batch 360/900 | Loss: 0.9941 | CLoss: 0.7960 | FLoss: 0.3961 | LR: 3.00e-04\n",
            "  Batch 370/900 | Loss: 0.8041 | CLoss: 0.6513 | FLoss: 0.3055 | LR: 3.00e-04\n",
            "  Batch 380/900 | Loss: 0.5841 | CLoss: 0.4631 | FLoss: 0.2420 | LR: 3.00e-04\n",
            "  Batch 390/900 | Loss: 0.7096 | CLoss: 0.5296 | FLoss: 0.3601 | LR: 3.00e-04\n",
            "  Batch 400/900 | Loss: 0.2498 | CLoss: 0.1901 | FLoss: 0.1194 | LR: 3.00e-04\n",
            "  Batch 410/900 | Loss: 0.7561 | CLoss: 0.5863 | FLoss: 0.3396 | LR: 3.00e-04\n",
            "  Batch 420/900 | Loss: 0.5325 | CLoss: 0.3828 | FLoss: 0.2995 | LR: 3.00e-04\n",
            "  Batch 430/900 | Loss: 1.0178 | CLoss: 0.7727 | FLoss: 0.4902 | LR: 3.00e-04\n",
            "  Batch 440/900 | Loss: 0.7576 | CLoss: 0.5039 | FLoss: 0.5074 | LR: 3.00e-04\n",
            "  Batch 450/900 | Loss: 1.2572 | CLoss: 1.0375 | FLoss: 0.4394 | LR: 3.00e-04\n",
            "  Batch 460/900 | Loss: 0.7271 | CLoss: 0.5903 | FLoss: 0.2735 | LR: 3.00e-04\n",
            "  Batch 470/900 | Loss: 0.6591 | CLoss: 0.5343 | FLoss: 0.2497 | LR: 3.00e-04\n",
            "  Batch 480/900 | Loss: 0.7160 | CLoss: 0.5077 | FLoss: 0.4166 | LR: 3.00e-04\n",
            "  Batch 490/900 | Loss: 0.5149 | CLoss: 0.3763 | FLoss: 0.2771 | LR: 3.00e-04\n",
            "  Batch 500/900 | Loss: 0.7610 | CLoss: 0.5561 | FLoss: 0.4097 | LR: 3.00e-04\n",
            "  Batch 510/900 | Loss: 1.1254 | CLoss: 0.9161 | FLoss: 0.4186 | LR: 3.00e-04\n",
            "  Batch 520/900 | Loss: 0.5966 | CLoss: 0.4917 | FLoss: 0.2098 | LR: 3.00e-04\n",
            "  Batch 530/900 | Loss: 0.7446 | CLoss: 0.6190 | FLoss: 0.2511 | LR: 3.00e-04\n",
            "  Batch 540/900 | Loss: 0.8397 | CLoss: 0.5962 | FLoss: 0.4870 | LR: 3.00e-04\n",
            "  Batch 550/900 | Loss: 1.1552 | CLoss: 0.8780 | FLoss: 0.5544 | LR: 3.00e-04\n",
            "  Batch 560/900 | Loss: 0.7284 | CLoss: 0.5382 | FLoss: 0.3804 | LR: 3.00e-04\n",
            "  Batch 570/900 | Loss: 0.6780 | CLoss: 0.5513 | FLoss: 0.2534 | LR: 3.00e-04\n",
            "  Batch 580/900 | Loss: 0.9756 | CLoss: 0.8247 | FLoss: 0.3019 | LR: 3.00e-04\n",
            "  Batch 590/900 | Loss: 1.2639 | CLoss: 1.0273 | FLoss: 0.4731 | LR: 3.00e-04\n",
            "  Batch 600/900 | Loss: 1.1444 | CLoss: 0.9146 | FLoss: 0.4596 | LR: 3.00e-04\n",
            "  Batch 610/900 | Loss: 0.7257 | CLoss: 0.5333 | FLoss: 0.3847 | LR: 3.00e-04\n",
            "  Batch 620/900 | Loss: 1.0623 | CLoss: 0.8417 | FLoss: 0.4411 | LR: 3.00e-04\n",
            "  Batch 630/900 | Loss: 0.7570 | CLoss: 0.6280 | FLoss: 0.2581 | LR: 3.00e-04\n",
            "  Batch 640/900 | Loss: 0.8372 | CLoss: 0.6127 | FLoss: 0.4491 | LR: 3.00e-04\n",
            "  Batch 650/900 | Loss: 0.7846 | CLoss: 0.6244 | FLoss: 0.3203 | LR: 3.00e-04\n",
            "  Batch 660/900 | Loss: 1.3238 | CLoss: 1.0619 | FLoss: 0.5238 | LR: 3.00e-04\n",
            "  Batch 670/900 | Loss: 0.5895 | CLoss: 0.3949 | FLoss: 0.3893 | LR: 3.00e-04\n",
            "  Batch 680/900 | Loss: 0.8183 | CLoss: 0.6208 | FLoss: 0.3950 | LR: 3.00e-04\n",
            "  Batch 690/900 | Loss: 1.1143 | CLoss: 0.8898 | FLoss: 0.4490 | LR: 3.00e-04\n",
            "  Batch 700/900 | Loss: 0.8294 | CLoss: 0.7112 | FLoss: 0.2363 | LR: 3.00e-04\n",
            "  Batch 710/900 | Loss: 0.5200 | CLoss: 0.3521 | FLoss: 0.3356 | LR: 3.00e-04\n",
            "  Batch 720/900 | Loss: 0.7775 | CLoss: 0.5415 | FLoss: 0.4719 | LR: 3.00e-04\n",
            "  Batch 730/900 | Loss: 0.5414 | CLoss: 0.4222 | FLoss: 0.2385 | LR: 3.00e-04\n",
            "  Batch 740/900 | Loss: 0.7509 | CLoss: 0.4967 | FLoss: 0.5084 | LR: 3.00e-04\n",
            "  Batch 750/900 | Loss: 0.7097 | CLoss: 0.5147 | FLoss: 0.3899 | LR: 3.00e-04\n",
            "  Batch 760/900 | Loss: 0.7272 | CLoss: 0.5712 | FLoss: 0.3120 | LR: 3.00e-04\n",
            "  Batch 770/900 | Loss: 0.5122 | CLoss: 0.4089 | FLoss: 0.2066 | LR: 3.00e-04\n",
            "  Batch 780/900 | Loss: 0.8475 | CLoss: 0.6976 | FLoss: 0.2998 | LR: 3.00e-04\n",
            "  Batch 790/900 | Loss: 0.6790 | CLoss: 0.5310 | FLoss: 0.2960 | LR: 3.00e-04\n",
            "  Batch 800/900 | Loss: 1.1083 | CLoss: 0.8329 | FLoss: 0.5509 | LR: 3.00e-04\n",
            "  Batch 810/900 | Loss: 0.5669 | CLoss: 0.4267 | FLoss: 0.2804 | LR: 3.00e-04\n",
            "  Batch 820/900 | Loss: 0.9436 | CLoss: 0.7449 | FLoss: 0.3973 | LR: 3.00e-04\n",
            "  Batch 830/900 | Loss: 0.7697 | CLoss: 0.5975 | FLoss: 0.3444 | LR: 3.00e-04\n",
            "  Batch 840/900 | Loss: 0.7834 | CLoss: 0.6325 | FLoss: 0.3019 | LR: 3.00e-04\n",
            "  Batch 850/900 | Loss: 0.7897 | CLoss: 0.6263 | FLoss: 0.3268 | LR: 3.00e-04\n",
            "  Batch 860/900 | Loss: 0.7242 | CLoss: 0.5968 | FLoss: 0.2547 | LR: 3.00e-04\n",
            "  Batch 870/900 | Loss: 0.5990 | CLoss: 0.4143 | FLoss: 0.3694 | LR: 3.00e-04\n",
            "  Batch 880/900 | Loss: 1.0283 | CLoss: 0.8751 | FLoss: 0.3062 | LR: 3.00e-04\n",
            "  Batch 890/900 | Loss: 1.1990 | CLoss: 0.9328 | FLoss: 0.5324 | LR: 3.00e-04\n",
            "  Batch 900/900 | Loss: 0.9390 | CLoss: 0.6656 | FLoss: 0.5469 | LR: 3.00e-04\n",
            "\n",
            "  Training Summary | Epoch 1\n",
            "  Avg Loss: 0.7760\n",
            "  Last Batch Loss: 0.9390\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.8968 | Batch Acc: 60.34%\n",
            "    Val Batch 010/99 | Loss: 0.1886 | Batch Acc: 94.83%\n",
            "    Val Batch 015/99 | Loss: 0.2490 | Batch Acc: 93.10%\n",
            "    Val Batch 020/99 | Loss: 0.4317 | Batch Acc: 86.21%\n",
            "    Val Batch 025/99 | Loss: 0.3793 | Batch Acc: 89.66%\n",
            "    Val Batch 030/99 | Loss: 0.3029 | Batch Acc: 91.38%\n",
            "    Val Batch 035/99 | Loss: 0.2842 | Batch Acc: 87.93%\n",
            "    Val Batch 040/99 | Loss: 0.3233 | Batch Acc: 82.76%\n",
            "    Val Batch 045/99 | Loss: 0.3430 | Batch Acc: 79.31%\n",
            "    Val Batch 050/99 | Loss: 0.2996 | Batch Acc: 89.66%\n",
            "    Val Batch 055/99 | Loss: 0.2913 | Batch Acc: 87.93%\n",
            "    Val Batch 060/99 | Loss: 0.1379 | Batch Acc: 96.55%\n",
            "    Val Batch 065/99 | Loss: 0.2996 | Batch Acc: 91.38%\n",
            "    Val Batch 070/99 | Loss: 0.2570 | Batch Acc: 93.10%\n",
            "    Val Batch 075/99 | Loss: 0.1894 | Batch Acc: 94.83%\n",
            "    Val Batch 080/99 | Loss: 0.4159 | Batch Acc: 87.93%\n",
            "    Val Batch 085/99 | Loss: 0.2010 | Batch Acc: 94.83%\n",
            "    Val Batch 090/99 | Loss: 0.2839 | Batch Acc: 89.66%\n",
            "    Val Batch 095/99 | Loss: 0.3494 | Batch Acc: 87.93%\n",
            "    Val Batch 099/99 | Loss: 0.3041 | Batch Acc: 91.43%\n",
            "\n",
            "  Validation Summary | Epoch 1\n",
            "  Avg Loss: 0.3235 | Accuracy: 88.49%\n",
            "  Current Best Acc: 88.49%\n",
            "\n",
            "Epoch 2/5\n",
            "  Batch 010/900 | Loss: 1.1425 | CLoss: 0.9589 | FLoss: 0.3672 | LR: 2.71e-04\n",
            "  Batch 020/900 | Loss: 0.7089 | CLoss: 0.5829 | FLoss: 0.2519 | LR: 2.71e-04\n",
            "  Batch 030/900 | Loss: 0.8494 | CLoss: 0.7170 | FLoss: 0.2647 | LR: 2.71e-04\n",
            "  Batch 040/900 | Loss: 0.9589 | CLoss: 0.8327 | FLoss: 0.2523 | LR: 2.71e-04\n",
            "  Batch 050/900 | Loss: 0.7681 | CLoss: 0.5889 | FLoss: 0.3584 | LR: 2.71e-04\n",
            "  Batch 060/900 | Loss: 1.2310 | CLoss: 1.0075 | FLoss: 0.4471 | LR: 2.71e-04\n",
            "  Batch 070/900 | Loss: 1.0019 | CLoss: 0.7957 | FLoss: 0.4123 | LR: 2.71e-04\n",
            "  Batch 080/900 | Loss: 0.6170 | CLoss: 0.5046 | FLoss: 0.2249 | LR: 2.71e-04\n",
            "  Batch 090/900 | Loss: 0.7709 | CLoss: 0.6129 | FLoss: 0.3159 | LR: 2.71e-04\n",
            "  Batch 100/900 | Loss: 0.4861 | CLoss: 0.3321 | FLoss: 0.3080 | LR: 2.71e-04\n",
            "  Batch 110/900 | Loss: 0.6549 | CLoss: 0.4986 | FLoss: 0.3125 | LR: 2.71e-04\n",
            "  Batch 120/900 | Loss: 0.8841 | CLoss: 0.7435 | FLoss: 0.2812 | LR: 2.71e-04\n",
            "  Batch 130/900 | Loss: 0.6605 | CLoss: 0.4485 | FLoss: 0.4240 | LR: 2.71e-04\n",
            "  Batch 140/900 | Loss: 0.7245 | CLoss: 0.6027 | FLoss: 0.2437 | LR: 2.71e-04\n",
            "  Batch 150/900 | Loss: 0.7353 | CLoss: 0.5899 | FLoss: 0.2908 | LR: 2.71e-04\n",
            "  Batch 160/900 | Loss: 0.6000 | CLoss: 0.5046 | FLoss: 0.1908 | LR: 2.71e-04\n",
            "  Batch 170/900 | Loss: 0.7135 | CLoss: 0.5980 | FLoss: 0.2310 | LR: 2.71e-04\n",
            "  Batch 180/900 | Loss: 0.6418 | CLoss: 0.4997 | FLoss: 0.2843 | LR: 2.71e-04\n",
            "  Batch 190/900 | Loss: 1.0664 | CLoss: 0.8882 | FLoss: 0.3563 | LR: 2.71e-04\n",
            "  Batch 200/900 | Loss: 0.9221 | CLoss: 0.7000 | FLoss: 0.4441 | LR: 2.71e-04\n",
            "  Batch 210/900 | Loss: 0.8507 | CLoss: 0.6770 | FLoss: 0.3474 | LR: 2.71e-04\n",
            "  Batch 220/900 | Loss: 0.6231 | CLoss: 0.4882 | FLoss: 0.2698 | LR: 2.71e-04\n",
            "  Batch 230/900 | Loss: 0.9325 | CLoss: 0.6507 | FLoss: 0.5634 | LR: 2.71e-04\n",
            "  Batch 240/900 | Loss: 0.8350 | CLoss: 0.6792 | FLoss: 0.3116 | LR: 2.71e-04\n",
            "  Batch 250/900 | Loss: 0.5042 | CLoss: 0.3666 | FLoss: 0.2751 | LR: 2.71e-04\n",
            "  Batch 260/900 | Loss: 0.5101 | CLoss: 0.3744 | FLoss: 0.2714 | LR: 2.71e-04\n",
            "  Batch 270/900 | Loss: 0.6745 | CLoss: 0.4891 | FLoss: 0.3707 | LR: 2.71e-04\n",
            "  Batch 280/900 | Loss: 0.8828 | CLoss: 0.7411 | FLoss: 0.2834 | LR: 2.71e-04\n",
            "  Batch 290/900 | Loss: 1.0396 | CLoss: 0.8222 | FLoss: 0.4349 | LR: 2.71e-04\n",
            "  Batch 300/900 | Loss: 1.1501 | CLoss: 0.8968 | FLoss: 0.5066 | LR: 2.71e-04\n",
            "  Batch 310/900 | Loss: 0.7362 | CLoss: 0.5712 | FLoss: 0.3300 | LR: 2.71e-04\n",
            "  Batch 320/900 | Loss: 0.8342 | CLoss: 0.6392 | FLoss: 0.3900 | LR: 2.71e-04\n",
            "  Batch 330/900 | Loss: 0.4008 | CLoss: 0.2981 | FLoss: 0.2053 | LR: 2.71e-04\n",
            "  Batch 340/900 | Loss: 0.5877 | CLoss: 0.4221 | FLoss: 0.3312 | LR: 2.71e-04\n",
            "  Batch 350/900 | Loss: 0.8277 | CLoss: 0.6294 | FLoss: 0.3966 | LR: 2.71e-04\n",
            "  Batch 360/900 | Loss: 0.4651 | CLoss: 0.3274 | FLoss: 0.2754 | LR: 2.71e-04\n",
            "  Batch 370/900 | Loss: 0.7128 | CLoss: 0.5887 | FLoss: 0.2481 | LR: 2.71e-04\n",
            "  Batch 380/900 | Loss: 0.8542 | CLoss: 0.6893 | FLoss: 0.3299 | LR: 2.71e-04\n",
            "  Batch 390/900 | Loss: 0.5371 | CLoss: 0.4278 | FLoss: 0.2185 | LR: 2.71e-04\n",
            "  Batch 400/900 | Loss: 0.4399 | CLoss: 0.3900 | FLoss: 0.0999 | LR: 2.71e-04\n",
            "  Batch 410/900 | Loss: 1.2757 | CLoss: 1.0189 | FLoss: 0.5138 | LR: 2.71e-04\n",
            "  Batch 420/900 | Loss: 0.5840 | CLoss: 0.3976 | FLoss: 0.3728 | LR: 2.71e-04\n",
            "  Batch 430/900 | Loss: 0.8544 | CLoss: 0.6765 | FLoss: 0.3557 | LR: 2.71e-04\n",
            "  Batch 440/900 | Loss: 0.5034 | CLoss: 0.3982 | FLoss: 0.2105 | LR: 2.71e-04\n",
            "  Batch 450/900 | Loss: 0.5330 | CLoss: 0.3948 | FLoss: 0.2763 | LR: 2.71e-04\n",
            "  Batch 460/900 | Loss: 0.7099 | CLoss: 0.5181 | FLoss: 0.3835 | LR: 2.71e-04\n",
            "  Batch 470/900 | Loss: 1.2929 | CLoss: 0.9598 | FLoss: 0.6661 | LR: 2.71e-04\n",
            "  Batch 480/900 | Loss: 0.4897 | CLoss: 0.3928 | FLoss: 0.1938 | LR: 2.71e-04\n",
            "  Batch 490/900 | Loss: 0.5983 | CLoss: 0.4590 | FLoss: 0.2786 | LR: 2.71e-04\n",
            "  Batch 500/900 | Loss: 0.4975 | CLoss: 0.3540 | FLoss: 0.2869 | LR: 2.71e-04\n",
            "  Batch 510/900 | Loss: 0.6046 | CLoss: 0.4330 | FLoss: 0.3433 | LR: 2.71e-04\n",
            "  Batch 520/900 | Loss: 0.5449 | CLoss: 0.3633 | FLoss: 0.3632 | LR: 2.71e-04\n",
            "  Batch 530/900 | Loss: 1.0265 | CLoss: 0.8291 | FLoss: 0.3948 | LR: 2.71e-04\n",
            "  Batch 540/900 | Loss: 1.3110 | CLoss: 1.0020 | FLoss: 0.6179 | LR: 2.71e-04\n",
            "  Batch 550/900 | Loss: 1.1577 | CLoss: 0.9695 | FLoss: 0.3762 | LR: 2.71e-04\n",
            "  Batch 560/900 | Loss: 0.8314 | CLoss: 0.6896 | FLoss: 0.2837 | LR: 2.71e-04\n",
            "  Batch 570/900 | Loss: 0.8503 | CLoss: 0.6228 | FLoss: 0.4550 | LR: 2.71e-04\n",
            "  Batch 580/900 | Loss: 0.8608 | CLoss: 0.6756 | FLoss: 0.3704 | LR: 2.71e-04\n",
            "  Batch 590/900 | Loss: 0.7130 | CLoss: 0.6007 | FLoss: 0.2246 | LR: 2.71e-04\n",
            "  Batch 600/900 | Loss: 0.6432 | CLoss: 0.4029 | FLoss: 0.4807 | LR: 2.71e-04\n",
            "  Batch 610/900 | Loss: 0.5674 | CLoss: 0.4623 | FLoss: 0.2102 | LR: 2.71e-04\n",
            "  Batch 620/900 | Loss: 0.2524 | CLoss: 0.1869 | FLoss: 0.1312 | LR: 2.71e-04\n",
            "  Batch 630/900 | Loss: 0.3625 | CLoss: 0.2705 | FLoss: 0.1841 | LR: 2.71e-04\n",
            "  Batch 640/900 | Loss: 0.8763 | CLoss: 0.6772 | FLoss: 0.3981 | LR: 2.71e-04\n",
            "  Batch 650/900 | Loss: 0.5131 | CLoss: 0.4475 | FLoss: 0.1314 | LR: 2.71e-04\n",
            "  Batch 660/900 | Loss: 0.8485 | CLoss: 0.6588 | FLoss: 0.3794 | LR: 2.71e-04\n",
            "  Batch 670/900 | Loss: 0.7686 | CLoss: 0.5474 | FLoss: 0.4425 | LR: 2.71e-04\n",
            "  Batch 680/900 | Loss: 0.9278 | CLoss: 0.7693 | FLoss: 0.3170 | LR: 2.71e-04\n",
            "  Batch 690/900 | Loss: 0.4824 | CLoss: 0.3716 | FLoss: 0.2215 | LR: 2.71e-04\n",
            "  Batch 700/900 | Loss: 0.7759 | CLoss: 0.5359 | FLoss: 0.4800 | LR: 2.71e-04\n",
            "  Batch 710/900 | Loss: 0.6472 | CLoss: 0.4225 | FLoss: 0.4494 | LR: 2.71e-04\n",
            "  Batch 720/900 | Loss: 0.3980 | CLoss: 0.3059 | FLoss: 0.1841 | LR: 2.71e-04\n",
            "  Batch 730/900 | Loss: 1.4796 | CLoss: 1.1809 | FLoss: 0.5972 | LR: 2.71e-04\n",
            "  Batch 740/900 | Loss: 0.8014 | CLoss: 0.6222 | FLoss: 0.3583 | LR: 2.71e-04\n",
            "  Batch 750/900 | Loss: 0.7900 | CLoss: 0.6231 | FLoss: 0.3340 | LR: 2.71e-04\n",
            "  Batch 760/900 | Loss: 0.5084 | CLoss: 0.3329 | FLoss: 0.3509 | LR: 2.71e-04\n",
            "  Batch 770/900 | Loss: 0.6214 | CLoss: 0.4791 | FLoss: 0.2844 | LR: 2.71e-04\n",
            "  Batch 780/900 | Loss: 0.5928 | CLoss: 0.4636 | FLoss: 0.2585 | LR: 2.71e-04\n",
            "  Batch 790/900 | Loss: 1.6348 | CLoss: 1.3884 | FLoss: 0.4928 | LR: 2.71e-04\n",
            "  Batch 800/900 | Loss: 0.9063 | CLoss: 0.7544 | FLoss: 0.3038 | LR: 2.71e-04\n",
            "  Batch 810/900 | Loss: 0.6277 | CLoss: 0.4803 | FLoss: 0.2949 | LR: 2.71e-04\n",
            "  Batch 820/900 | Loss: 0.7458 | CLoss: 0.4960 | FLoss: 0.4995 | LR: 2.71e-04\n",
            "  Batch 830/900 | Loss: 0.9651 | CLoss: 0.8007 | FLoss: 0.3288 | LR: 2.71e-04\n",
            "  Batch 840/900 | Loss: 0.6910 | CLoss: 0.5248 | FLoss: 0.3324 | LR: 2.71e-04\n",
            "  Batch 850/900 | Loss: 0.6383 | CLoss: 0.4513 | FLoss: 0.3739 | LR: 2.71e-04\n",
            "  Batch 860/900 | Loss: 1.0907 | CLoss: 0.8556 | FLoss: 0.4702 | LR: 2.71e-04\n",
            "  Batch 870/900 | Loss: 0.6726 | CLoss: 0.4672 | FLoss: 0.4110 | LR: 2.71e-04\n",
            "  Batch 880/900 | Loss: 1.0273 | CLoss: 0.8840 | FLoss: 0.2867 | LR: 2.71e-04\n",
            "  Batch 890/900 | Loss: 0.8038 | CLoss: 0.5742 | FLoss: 0.4592 | LR: 2.71e-04\n",
            "  Batch 900/900 | Loss: 0.4442 | CLoss: 0.3874 | FLoss: 0.1136 | LR: 2.71e-04\n",
            "\n",
            "  Training Summary | Epoch 2\n",
            "  Avg Loss: 0.7476\n",
            "  Last Batch Loss: 0.4442\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.8123 | Batch Acc: 67.24%\n",
            "    Val Batch 010/99 | Loss: 0.1313 | Batch Acc: 96.55%\n",
            "    Val Batch 015/99 | Loss: 0.3590 | Batch Acc: 86.21%\n",
            "    Val Batch 020/99 | Loss: 0.6435 | Batch Acc: 65.52%\n",
            "    Val Batch 025/99 | Loss: 0.8663 | Batch Acc: 74.14%\n",
            "    Val Batch 030/99 | Loss: 0.4795 | Batch Acc: 81.03%\n",
            "    Val Batch 035/99 | Loss: 0.2239 | Batch Acc: 91.38%\n",
            "    Val Batch 040/99 | Loss: 0.2109 | Batch Acc: 100.00%\n",
            "    Val Batch 045/99 | Loss: 0.4589 | Batch Acc: 93.10%\n",
            "    Val Batch 050/99 | Loss: 0.3975 | Batch Acc: 91.38%\n",
            "    Val Batch 055/99 | Loss: 0.0232 | Batch Acc: 100.00%\n",
            "    Val Batch 060/99 | Loss: 0.1293 | Batch Acc: 93.10%\n",
            "    Val Batch 065/99 | Loss: 0.5472 | Batch Acc: 79.31%\n",
            "    Val Batch 070/99 | Loss: 0.0952 | Batch Acc: 98.28%\n",
            "    Val Batch 075/99 | Loss: 0.1694 | Batch Acc: 94.83%\n",
            "    Val Batch 080/99 | Loss: 0.2184 | Batch Acc: 94.83%\n",
            "    Val Batch 085/99 | Loss: 0.4050 | Batch Acc: 84.48%\n",
            "    Val Batch 090/99 | Loss: 0.4152 | Batch Acc: 86.21%\n",
            "    Val Batch 095/99 | Loss: 0.2655 | Batch Acc: 93.10%\n",
            "    Val Batch 099/99 | Loss: 0.1938 | Batch Acc: 94.29%\n",
            "\n",
            "  Validation Summary | Epoch 2\n",
            "  Avg Loss: 0.3180 | Accuracy: 88.84%\n",
            "  Current Best Acc: 88.84%\n",
            "\n",
            "Epoch 3/5\n",
            "  Batch 010/900 | Loss: 1.0505 | CLoss: 0.7954 | FLoss: 0.5101 | LR: 1.96e-04\n",
            "  Batch 020/900 | Loss: 0.3965 | CLoss: 0.3135 | FLoss: 0.1661 | LR: 1.96e-04\n",
            "  Batch 030/900 | Loss: 1.0331 | CLoss: 0.8684 | FLoss: 0.3294 | LR: 1.96e-04\n",
            "  Batch 040/900 | Loss: 0.7021 | CLoss: 0.5330 | FLoss: 0.3381 | LR: 1.96e-04\n",
            "  Batch 050/900 | Loss: 0.7021 | CLoss: 0.6164 | FLoss: 0.1716 | LR: 1.96e-04\n",
            "  Batch 060/900 | Loss: 0.9064 | CLoss: 0.6775 | FLoss: 0.4578 | LR: 1.96e-04\n",
            "  Batch 070/900 | Loss: 1.0774 | CLoss: 0.8745 | FLoss: 0.4058 | LR: 1.96e-04\n",
            "  Batch 080/900 | Loss: 0.7062 | CLoss: 0.5844 | FLoss: 0.2436 | LR: 1.96e-04\n",
            "  Batch 090/900 | Loss: 0.6011 | CLoss: 0.4588 | FLoss: 0.2845 | LR: 1.96e-04\n",
            "  Batch 100/900 | Loss: 0.7277 | CLoss: 0.5450 | FLoss: 0.3653 | LR: 1.96e-04\n",
            "  Batch 110/900 | Loss: 0.3550 | CLoss: 0.2620 | FLoss: 0.1860 | LR: 1.96e-04\n",
            "  Batch 120/900 | Loss: 0.4095 | CLoss: 0.3075 | FLoss: 0.2039 | LR: 1.96e-04\n",
            "  Batch 130/900 | Loss: 0.4900 | CLoss: 0.3710 | FLoss: 0.2379 | LR: 1.96e-04\n",
            "  Batch 140/900 | Loss: 0.6499 | CLoss: 0.4977 | FLoss: 0.3045 | LR: 1.96e-04\n",
            "  Batch 150/900 | Loss: 0.7962 | CLoss: 0.6256 | FLoss: 0.3412 | LR: 1.96e-04\n",
            "  Batch 160/900 | Loss: 0.6607 | CLoss: 0.5216 | FLoss: 0.2784 | LR: 1.96e-04\n",
            "  Batch 170/900 | Loss: 0.8946 | CLoss: 0.6687 | FLoss: 0.4518 | LR: 1.96e-04\n",
            "  Batch 180/900 | Loss: 0.6487 | CLoss: 0.5401 | FLoss: 0.2171 | LR: 1.96e-04\n",
            "  Batch 190/900 | Loss: 1.2306 | CLoss: 1.0137 | FLoss: 0.4336 | LR: 1.96e-04\n",
            "  Batch 200/900 | Loss: 1.1587 | CLoss: 0.9331 | FLoss: 0.4512 | LR: 1.96e-04\n",
            "  Batch 210/900 | Loss: 0.6971 | CLoss: 0.5019 | FLoss: 0.3905 | LR: 1.96e-04\n",
            "  Batch 220/900 | Loss: 0.7993 | CLoss: 0.6197 | FLoss: 0.3593 | LR: 1.96e-04\n",
            "  Batch 230/900 | Loss: 0.5750 | CLoss: 0.4686 | FLoss: 0.2129 | LR: 1.96e-04\n",
            "  Batch 240/900 | Loss: 0.5953 | CLoss: 0.4639 | FLoss: 0.2627 | LR: 1.96e-04\n",
            "  Batch 250/900 | Loss: 0.7454 | CLoss: 0.6224 | FLoss: 0.2459 | LR: 1.96e-04\n",
            "  Batch 260/900 | Loss: 1.1042 | CLoss: 0.8123 | FLoss: 0.5837 | LR: 1.96e-04\n",
            "  Batch 270/900 | Loss: 0.6921 | CLoss: 0.5419 | FLoss: 0.3004 | LR: 1.96e-04\n",
            "  Batch 280/900 | Loss: 0.9427 | CLoss: 0.7686 | FLoss: 0.3481 | LR: 1.96e-04\n",
            "  Batch 290/900 | Loss: 0.4721 | CLoss: 0.3442 | FLoss: 0.2557 | LR: 1.96e-04\n",
            "  Batch 300/900 | Loss: 0.7467 | CLoss: 0.5854 | FLoss: 0.3226 | LR: 1.96e-04\n",
            "  Batch 310/900 | Loss: 0.3917 | CLoss: 0.3144 | FLoss: 0.1546 | LR: 1.96e-04\n",
            "  Batch 320/900 | Loss: 0.6612 | CLoss: 0.4676 | FLoss: 0.3872 | LR: 1.96e-04\n",
            "  Batch 330/900 | Loss: 1.2846 | CLoss: 1.0853 | FLoss: 0.3985 | LR: 1.96e-04\n",
            "  Batch 340/900 | Loss: 1.1185 | CLoss: 0.8643 | FLoss: 0.5083 | LR: 1.96e-04\n",
            "  Batch 350/900 | Loss: 0.4211 | CLoss: 0.3189 | FLoss: 0.2043 | LR: 1.96e-04\n",
            "  Batch 360/900 | Loss: 0.5135 | CLoss: 0.3418 | FLoss: 0.3435 | LR: 1.96e-04\n",
            "  Batch 370/900 | Loss: 0.6251 | CLoss: 0.5112 | FLoss: 0.2278 | LR: 1.96e-04\n",
            "  Batch 380/900 | Loss: 0.6575 | CLoss: 0.5377 | FLoss: 0.2396 | LR: 1.96e-04\n",
            "  Batch 390/900 | Loss: 0.8998 | CLoss: 0.7377 | FLoss: 0.3241 | LR: 1.96e-04\n",
            "  Batch 400/900 | Loss: 0.5079 | CLoss: 0.4458 | FLoss: 0.1242 | LR: 1.96e-04\n",
            "  Batch 410/900 | Loss: 0.8437 | CLoss: 0.6389 | FLoss: 0.4096 | LR: 1.96e-04\n",
            "  Batch 420/900 | Loss: 0.4240 | CLoss: 0.3543 | FLoss: 0.1395 | LR: 1.96e-04\n",
            "  Batch 430/900 | Loss: 0.4942 | CLoss: 0.4162 | FLoss: 0.1561 | LR: 1.96e-04\n",
            "  Batch 440/900 | Loss: 0.6323 | CLoss: 0.5397 | FLoss: 0.1851 | LR: 1.96e-04\n",
            "  Batch 450/900 | Loss: 0.7198 | CLoss: 0.5507 | FLoss: 0.3381 | LR: 1.96e-04\n",
            "  Batch 460/900 | Loss: 0.6197 | CLoss: 0.4962 | FLoss: 0.2469 | LR: 1.96e-04\n",
            "  Batch 470/900 | Loss: 0.4474 | CLoss: 0.3041 | FLoss: 0.2865 | LR: 1.96e-04\n",
            "  Batch 480/900 | Loss: 0.4400 | CLoss: 0.2909 | FLoss: 0.2982 | LR: 1.96e-04\n",
            "  Batch 490/900 | Loss: 0.8645 | CLoss: 0.6480 | FLoss: 0.4331 | LR: 1.96e-04\n",
            "  Batch 500/900 | Loss: 0.8565 | CLoss: 0.6081 | FLoss: 0.4968 | LR: 1.96e-04\n",
            "  Batch 510/900 | Loss: 0.4267 | CLoss: 0.2561 | FLoss: 0.3413 | LR: 1.96e-04\n",
            "  Batch 520/900 | Loss: 0.4737 | CLoss: 0.3330 | FLoss: 0.2813 | LR: 1.96e-04\n",
            "  Batch 530/900 | Loss: 0.7555 | CLoss: 0.5401 | FLoss: 0.4308 | LR: 1.96e-04\n",
            "  Batch 540/900 | Loss: 0.6534 | CLoss: 0.5843 | FLoss: 0.1382 | LR: 1.96e-04\n",
            "  Batch 550/900 | Loss: 0.7927 | CLoss: 0.6186 | FLoss: 0.3482 | LR: 1.96e-04\n",
            "  Batch 560/900 | Loss: 0.4884 | CLoss: 0.3836 | FLoss: 0.2094 | LR: 1.96e-04\n",
            "  Batch 570/900 | Loss: 0.3297 | CLoss: 0.2136 | FLoss: 0.2321 | LR: 1.96e-04\n",
            "  Batch 580/900 | Loss: 0.9127 | CLoss: 0.7668 | FLoss: 0.2919 | LR: 1.96e-04\n",
            "  Batch 590/900 | Loss: 0.8758 | CLoss: 0.6670 | FLoss: 0.4177 | LR: 1.96e-04\n",
            "  Batch 600/900 | Loss: 0.7223 | CLoss: 0.5754 | FLoss: 0.2936 | LR: 1.96e-04\n",
            "  Batch 610/900 | Loss: 0.6647 | CLoss: 0.5235 | FLoss: 0.2824 | LR: 1.96e-04\n",
            "  Batch 620/900 | Loss: 0.4586 | CLoss: 0.2671 | FLoss: 0.3832 | LR: 1.96e-04\n",
            "  Batch 630/900 | Loss: 0.6674 | CLoss: 0.5356 | FLoss: 0.2635 | LR: 1.96e-04\n",
            "  Batch 640/900 | Loss: 0.8609 | CLoss: 0.6569 | FLoss: 0.4079 | LR: 1.96e-04\n",
            "  Batch 650/900 | Loss: 0.8195 | CLoss: 0.5870 | FLoss: 0.4651 | LR: 1.96e-04\n",
            "  Batch 660/900 | Loss: 0.4732 | CLoss: 0.3447 | FLoss: 0.2570 | LR: 1.96e-04\n",
            "  Batch 670/900 | Loss: 0.5815 | CLoss: 0.4278 | FLoss: 0.3075 | LR: 1.96e-04\n",
            "  Batch 680/900 | Loss: 0.7862 | CLoss: 0.5931 | FLoss: 0.3862 | LR: 1.96e-04\n",
            "  Batch 690/900 | Loss: 0.8486 | CLoss: 0.6275 | FLoss: 0.4420 | LR: 1.96e-04\n",
            "  Batch 700/900 | Loss: 1.0549 | CLoss: 0.8118 | FLoss: 0.4861 | LR: 1.96e-04\n",
            "  Batch 710/900 | Loss: 0.6664 | CLoss: 0.5088 | FLoss: 0.3151 | LR: 1.96e-04\n",
            "  Batch 720/900 | Loss: 0.5083 | CLoss: 0.4346 | FLoss: 0.1473 | LR: 1.96e-04\n",
            "  Batch 730/900 | Loss: 0.3128 | CLoss: 0.2205 | FLoss: 0.1846 | LR: 1.96e-04\n",
            "  Batch 740/900 | Loss: 0.5764 | CLoss: 0.4602 | FLoss: 0.2323 | LR: 1.96e-04\n",
            "  Batch 750/900 | Loss: 0.8060 | CLoss: 0.5756 | FLoss: 0.4608 | LR: 1.96e-04\n",
            "  Batch 760/900 | Loss: 0.6319 | CLoss: 0.4313 | FLoss: 0.4011 | LR: 1.96e-04\n",
            "  Batch 770/900 | Loss: 0.5277 | CLoss: 0.3656 | FLoss: 0.3242 | LR: 1.96e-04\n",
            "  Batch 780/900 | Loss: 0.7226 | CLoss: 0.5211 | FLoss: 0.4029 | LR: 1.96e-04\n",
            "  Batch 790/900 | Loss: 0.7925 | CLoss: 0.6222 | FLoss: 0.3405 | LR: 1.96e-04\n",
            "  Batch 800/900 | Loss: 0.9223 | CLoss: 0.6159 | FLoss: 0.6129 | LR: 1.96e-04\n",
            "  Batch 810/900 | Loss: 0.3362 | CLoss: 0.2291 | FLoss: 0.2143 | LR: 1.96e-04\n",
            "  Batch 820/900 | Loss: 0.2836 | CLoss: 0.2039 | FLoss: 0.1595 | LR: 1.96e-04\n",
            "  Batch 830/900 | Loss: 0.7431 | CLoss: 0.5844 | FLoss: 0.3174 | LR: 1.96e-04\n",
            "  Batch 840/900 | Loss: 0.5811 | CLoss: 0.4837 | FLoss: 0.1947 | LR: 1.96e-04\n",
            "  Batch 850/900 | Loss: 0.7274 | CLoss: 0.6378 | FLoss: 0.1793 | LR: 1.96e-04\n",
            "  Batch 860/900 | Loss: 1.0168 | CLoss: 0.7992 | FLoss: 0.4352 | LR: 1.96e-04\n",
            "  Batch 870/900 | Loss: 0.7465 | CLoss: 0.5928 | FLoss: 0.3073 | LR: 1.96e-04\n",
            "  Batch 880/900 | Loss: 0.5405 | CLoss: 0.4708 | FLoss: 0.1394 | LR: 1.96e-04\n",
            "  Batch 890/900 | Loss: 0.6836 | CLoss: 0.5500 | FLoss: 0.2672 | LR: 1.96e-04\n",
            "  Batch 900/900 | Loss: 0.6308 | CLoss: 0.4852 | FLoss: 0.2913 | LR: 1.96e-04\n",
            "\n",
            "  Training Summary | Epoch 3\n",
            "  Avg Loss: 0.6979\n",
            "  Last Batch Loss: 0.6308\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 1.1171 | Batch Acc: 51.72%\n",
            "    Val Batch 010/99 | Loss: 0.4143 | Batch Acc: 86.21%\n",
            "    Val Batch 015/99 | Loss: 0.5231 | Batch Acc: 84.48%\n",
            "    Val Batch 020/99 | Loss: 0.7049 | Batch Acc: 70.69%\n",
            "    Val Batch 025/99 | Loss: 0.1727 | Batch Acc: 91.38%\n",
            "    Val Batch 030/99 | Loss: 0.2625 | Batch Acc: 93.10%\n",
            "    Val Batch 035/99 | Loss: 0.1711 | Batch Acc: 93.10%\n",
            "    Val Batch 040/99 | Loss: 0.3650 | Batch Acc: 96.55%\n",
            "    Val Batch 045/99 | Loss: 0.2583 | Batch Acc: 94.83%\n",
            "    Val Batch 050/99 | Loss: 0.1978 | Batch Acc: 93.10%\n",
            "    Val Batch 055/99 | Loss: 0.1419 | Batch Acc: 91.38%\n",
            "    Val Batch 060/99 | Loss: 0.2358 | Batch Acc: 89.66%\n",
            "    Val Batch 065/99 | Loss: 0.3087 | Batch Acc: 91.38%\n",
            "    Val Batch 070/99 | Loss: 0.0635 | Batch Acc: 100.00%\n",
            "    Val Batch 075/99 | Loss: 0.1048 | Batch Acc: 94.83%\n",
            "    Val Batch 080/99 | Loss: 0.3310 | Batch Acc: 87.93%\n",
            "    Val Batch 085/99 | Loss: 0.3166 | Batch Acc: 86.21%\n",
            "    Val Batch 090/99 | Loss: 0.3648 | Batch Acc: 91.38%\n",
            "    Val Batch 095/99 | Loss: 0.1647 | Batch Acc: 96.55%\n",
            "    Val Batch 099/99 | Loss: 0.1160 | Batch Acc: 97.14%\n",
            "\n",
            "  Validation Summary | Epoch 3\n",
            "  Avg Loss: 0.3109 | Accuracy: 89.04%\n",
            "  Current Best Acc: 89.04%\n",
            "\n",
            "Epoch 4/5\n",
            "  Batch 010/900 | Loss: 0.7627 | CLoss: 0.5681 | FLoss: 0.3892 | LR: 1.04e-04\n",
            "  Batch 020/900 | Loss: 1.0090 | CLoss: 0.7149 | FLoss: 0.5882 | LR: 1.04e-04\n",
            "  Batch 030/900 | Loss: 0.6404 | CLoss: 0.3970 | FLoss: 0.4869 | LR: 1.04e-04\n",
            "  Batch 040/900 | Loss: 0.8071 | CLoss: 0.6402 | FLoss: 0.3339 | LR: 1.04e-04\n",
            "  Batch 050/900 | Loss: 0.8401 | CLoss: 0.5925 | FLoss: 0.4952 | LR: 1.04e-04\n",
            "  Batch 060/900 | Loss: 0.5816 | CLoss: 0.4383 | FLoss: 0.2865 | LR: 1.04e-04\n",
            "  Batch 070/900 | Loss: 0.7501 | CLoss: 0.6217 | FLoss: 0.2569 | LR: 1.04e-04\n",
            "  Batch 080/900 | Loss: 0.5743 | CLoss: 0.4296 | FLoss: 0.2894 | LR: 1.04e-04\n",
            "  Batch 090/900 | Loss: 0.7335 | CLoss: 0.6021 | FLoss: 0.2627 | LR: 1.04e-04\n",
            "  Batch 100/900 | Loss: 0.8522 | CLoss: 0.6785 | FLoss: 0.3475 | LR: 1.04e-04\n",
            "  Batch 110/900 | Loss: 0.5599 | CLoss: 0.4470 | FLoss: 0.2258 | LR: 1.04e-04\n",
            "  Batch 120/900 | Loss: 0.4079 | CLoss: 0.2908 | FLoss: 0.2342 | LR: 1.04e-04\n",
            "  Batch 130/900 | Loss: 0.4998 | CLoss: 0.3623 | FLoss: 0.2749 | LR: 1.04e-04\n",
            "  Batch 140/900 | Loss: 0.9019 | CLoss: 0.7758 | FLoss: 0.2522 | LR: 1.04e-04\n",
            "  Batch 150/900 | Loss: 0.8139 | CLoss: 0.6709 | FLoss: 0.2859 | LR: 1.04e-04\n",
            "  Batch 160/900 | Loss: 0.6862 | CLoss: 0.5372 | FLoss: 0.2980 | LR: 1.04e-04\n",
            "  Batch 170/900 | Loss: 0.4493 | CLoss: 0.3066 | FLoss: 0.2854 | LR: 1.04e-04\n",
            "  Batch 180/900 | Loss: 0.5638 | CLoss: 0.4443 | FLoss: 0.2391 | LR: 1.04e-04\n",
            "  Batch 190/900 | Loss: 0.3741 | CLoss: 0.2363 | FLoss: 0.2755 | LR: 1.04e-04\n",
            "  Batch 200/900 | Loss: 0.6628 | CLoss: 0.5828 | FLoss: 0.1599 | LR: 1.04e-04\n",
            "  Batch 210/900 | Loss: 0.5467 | CLoss: 0.4627 | FLoss: 0.1680 | LR: 1.04e-04\n",
            "  Batch 220/900 | Loss: 0.6790 | CLoss: 0.4889 | FLoss: 0.3803 | LR: 1.04e-04\n",
            "  Batch 230/900 | Loss: 0.9677 | CLoss: 0.8245 | FLoss: 0.2863 | LR: 1.04e-04\n",
            "  Batch 240/900 | Loss: 0.3476 | CLoss: 0.2144 | FLoss: 0.2662 | LR: 1.04e-04\n",
            "  Batch 250/900 | Loss: 0.5673 | CLoss: 0.4034 | FLoss: 0.3278 | LR: 1.04e-04\n",
            "  Batch 260/900 | Loss: 0.3523 | CLoss: 0.2703 | FLoss: 0.1641 | LR: 1.04e-04\n",
            "  Batch 270/900 | Loss: 0.4203 | CLoss: 0.2023 | FLoss: 0.4359 | LR: 1.04e-04\n",
            "  Batch 280/900 | Loss: 0.9584 | CLoss: 0.7524 | FLoss: 0.4119 | LR: 1.04e-04\n",
            "  Batch 290/900 | Loss: 0.3597 | CLoss: 0.2690 | FLoss: 0.1814 | LR: 1.04e-04\n",
            "  Batch 300/900 | Loss: 0.7213 | CLoss: 0.5423 | FLoss: 0.3581 | LR: 1.04e-04\n",
            "  Batch 310/900 | Loss: 0.9082 | CLoss: 0.7422 | FLoss: 0.3320 | LR: 1.04e-04\n",
            "  Batch 320/900 | Loss: 0.5130 | CLoss: 0.3970 | FLoss: 0.2319 | LR: 1.04e-04\n",
            "  Batch 330/900 | Loss: 0.4116 | CLoss: 0.3106 | FLoss: 0.2020 | LR: 1.04e-04\n",
            "  Batch 340/900 | Loss: 0.7568 | CLoss: 0.6073 | FLoss: 0.2991 | LR: 1.04e-04\n",
            "  Batch 350/900 | Loss: 0.3146 | CLoss: 0.2364 | FLoss: 0.1565 | LR: 1.04e-04\n",
            "  Batch 360/900 | Loss: 0.8230 | CLoss: 0.7019 | FLoss: 0.2422 | LR: 1.04e-04\n",
            "  Batch 370/900 | Loss: 0.5334 | CLoss: 0.3494 | FLoss: 0.3679 | LR: 1.04e-04\n",
            "  Batch 380/900 | Loss: 0.7438 | CLoss: 0.6624 | FLoss: 0.1629 | LR: 1.04e-04\n",
            "  Batch 390/900 | Loss: 0.5590 | CLoss: 0.4696 | FLoss: 0.1788 | LR: 1.04e-04\n",
            "  Batch 400/900 | Loss: 0.6276 | CLoss: 0.5315 | FLoss: 0.1923 | LR: 1.04e-04\n",
            "  Batch 410/900 | Loss: 0.6883 | CLoss: 0.5326 | FLoss: 0.3113 | LR: 1.04e-04\n",
            "  Batch 420/900 | Loss: 0.5338 | CLoss: 0.3563 | FLoss: 0.3551 | LR: 1.04e-04\n",
            "  Batch 430/900 | Loss: 0.4367 | CLoss: 0.3564 | FLoss: 0.1607 | LR: 1.04e-04\n",
            "  Batch 440/900 | Loss: 0.5732 | CLoss: 0.4551 | FLoss: 0.2362 | LR: 1.04e-04\n",
            "  Batch 450/900 | Loss: 0.7196 | CLoss: 0.5835 | FLoss: 0.2723 | LR: 1.04e-04\n",
            "  Batch 460/900 | Loss: 0.6103 | CLoss: 0.5068 | FLoss: 0.2071 | LR: 1.04e-04\n",
            "  Batch 470/900 | Loss: 0.9962 | CLoss: 0.7912 | FLoss: 0.4101 | LR: 1.04e-04\n",
            "  Batch 480/900 | Loss: 0.3844 | CLoss: 0.2205 | FLoss: 0.3277 | LR: 1.04e-04\n",
            "  Batch 490/900 | Loss: 0.8937 | CLoss: 0.7765 | FLoss: 0.2343 | LR: 1.04e-04\n",
            "  Batch 500/900 | Loss: 0.4381 | CLoss: 0.3184 | FLoss: 0.2392 | LR: 1.04e-04\n",
            "  Batch 510/900 | Loss: 0.9561 | CLoss: 0.7731 | FLoss: 0.3659 | LR: 1.04e-04\n",
            "  Batch 520/900 | Loss: 0.5232 | CLoss: 0.3752 | FLoss: 0.2960 | LR: 1.04e-04\n",
            "  Batch 530/900 | Loss: 1.1822 | CLoss: 0.9681 | FLoss: 0.4284 | LR: 1.04e-04\n",
            "  Batch 540/900 | Loss: 0.6858 | CLoss: 0.4815 | FLoss: 0.4086 | LR: 1.04e-04\n",
            "  Batch 550/900 | Loss: 0.3745 | CLoss: 0.3118 | FLoss: 0.1254 | LR: 1.04e-04\n",
            "  Batch 560/900 | Loss: 0.9226 | CLoss: 0.7212 | FLoss: 0.4028 | LR: 1.04e-04\n",
            "  Batch 570/900 | Loss: 0.5166 | CLoss: 0.4130 | FLoss: 0.2073 | LR: 1.04e-04\n",
            "  Batch 580/900 | Loss: 0.4716 | CLoss: 0.3812 | FLoss: 0.1809 | LR: 1.04e-04\n",
            "  Batch 590/900 | Loss: 0.5307 | CLoss: 0.3574 | FLoss: 0.3467 | LR: 1.04e-04\n",
            "  Batch 600/900 | Loss: 0.6016 | CLoss: 0.4649 | FLoss: 0.2733 | LR: 1.04e-04\n",
            "  Batch 610/900 | Loss: 0.6499 | CLoss: 0.4913 | FLoss: 0.3173 | LR: 1.04e-04\n",
            "  Batch 620/900 | Loss: 0.6024 | CLoss: 0.5016 | FLoss: 0.2016 | LR: 1.04e-04\n",
            "  Batch 630/900 | Loss: 0.5227 | CLoss: 0.4458 | FLoss: 0.1540 | LR: 1.04e-04\n",
            "  Batch 640/900 | Loss: 0.6223 | CLoss: 0.4806 | FLoss: 0.2834 | LR: 1.04e-04\n",
            "  Batch 650/900 | Loss: 0.6947 | CLoss: 0.6084 | FLoss: 0.1725 | LR: 1.04e-04\n",
            "  Batch 660/900 | Loss: 0.3716 | CLoss: 0.2752 | FLoss: 0.1928 | LR: 1.04e-04\n",
            "  Batch 670/900 | Loss: 0.5787 | CLoss: 0.3913 | FLoss: 0.3748 | LR: 1.04e-04\n",
            "  Batch 680/900 | Loss: 0.6995 | CLoss: 0.5391 | FLoss: 0.3208 | LR: 1.04e-04\n",
            "  Batch 690/900 | Loss: 0.3037 | CLoss: 0.2169 | FLoss: 0.1736 | LR: 1.04e-04\n",
            "  Batch 700/900 | Loss: 0.6527 | CLoss: 0.5119 | FLoss: 0.2816 | LR: 1.04e-04\n",
            "  Batch 710/900 | Loss: 1.0745 | CLoss: 0.8575 | FLoss: 0.4340 | LR: 1.04e-04\n",
            "  Batch 720/900 | Loss: 0.6737 | CLoss: 0.5341 | FLoss: 0.2793 | LR: 1.04e-04\n",
            "  Batch 730/900 | Loss: 0.6096 | CLoss: 0.4622 | FLoss: 0.2946 | LR: 1.04e-04\n",
            "  Batch 740/900 | Loss: 0.2594 | CLoss: 0.1968 | FLoss: 0.1253 | LR: 1.04e-04\n",
            "  Batch 750/900 | Loss: 0.5714 | CLoss: 0.3433 | FLoss: 0.4561 | LR: 1.04e-04\n",
            "  Batch 760/900 | Loss: 0.3714 | CLoss: 0.2499 | FLoss: 0.2429 | LR: 1.04e-04\n",
            "  Batch 770/900 | Loss: 0.6766 | CLoss: 0.5303 | FLoss: 0.2927 | LR: 1.04e-04\n",
            "  Batch 780/900 | Loss: 0.6010 | CLoss: 0.4690 | FLoss: 0.2640 | LR: 1.04e-04\n",
            "  Batch 790/900 | Loss: 1.2752 | CLoss: 1.0051 | FLoss: 0.5402 | LR: 1.04e-04\n",
            "  Batch 800/900 | Loss: 0.6728 | CLoss: 0.5696 | FLoss: 0.2064 | LR: 1.04e-04\n",
            "  Batch 810/900 | Loss: 0.3848 | CLoss: 0.3048 | FLoss: 0.1601 | LR: 1.04e-04\n",
            "  Batch 820/900 | Loss: 0.9436 | CLoss: 0.8138 | FLoss: 0.2596 | LR: 1.04e-04\n",
            "  Batch 830/900 | Loss: 0.9538 | CLoss: 0.7844 | FLoss: 0.3387 | LR: 1.04e-04\n",
            "  Batch 840/900 | Loss: 0.8752 | CLoss: 0.7153 | FLoss: 0.3197 | LR: 1.04e-04\n",
            "  Batch 850/900 | Loss: 0.7496 | CLoss: 0.5609 | FLoss: 0.3774 | LR: 1.04e-04\n",
            "  Batch 860/900 | Loss: 0.3111 | CLoss: 0.2110 | FLoss: 0.2001 | LR: 1.04e-04\n",
            "  Batch 870/900 | Loss: 0.5271 | CLoss: 0.3846 | FLoss: 0.2852 | LR: 1.04e-04\n",
            "  Batch 880/900 | Loss: 0.7090 | CLoss: 0.5105 | FLoss: 0.3969 | LR: 1.04e-04\n",
            "  Batch 890/900 | Loss: 0.3248 | CLoss: 0.1912 | FLoss: 0.2673 | LR: 1.04e-04\n",
            "  Batch 900/900 | Loss: 0.5044 | CLoss: 0.3825 | FLoss: 0.2439 | LR: 1.04e-04\n",
            "\n",
            "  Training Summary | Epoch 4\n",
            "  Avg Loss: 0.6376\n",
            "  Last Batch Loss: 0.5044\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.7503 | Batch Acc: 67.24%\n",
            "    Val Batch 010/99 | Loss: 0.3182 | Batch Acc: 89.66%\n",
            "    Val Batch 015/99 | Loss: 0.4046 | Batch Acc: 87.93%\n",
            "    Val Batch 020/99 | Loss: 0.5585 | Batch Acc: 63.79%\n",
            "    Val Batch 025/99 | Loss: 0.2698 | Batch Acc: 91.38%\n",
            "    Val Batch 030/99 | Loss: 0.2290 | Batch Acc: 89.66%\n",
            "    Val Batch 035/99 | Loss: 0.3470 | Batch Acc: 86.21%\n",
            "    Val Batch 040/99 | Loss: 0.3960 | Batch Acc: 98.28%\n",
            "    Val Batch 045/99 | Loss: 0.2651 | Batch Acc: 98.28%\n",
            "    Val Batch 050/99 | Loss: 0.3577 | Batch Acc: 87.93%\n",
            "    Val Batch 055/99 | Loss: 0.0944 | Batch Acc: 94.83%\n",
            "    Val Batch 060/99 | Loss: 0.0388 | Batch Acc: 100.00%\n",
            "    Val Batch 065/99 | Loss: 0.1925 | Batch Acc: 93.10%\n",
            "    Val Batch 070/99 | Loss: 0.1518 | Batch Acc: 96.55%\n",
            "    Val Batch 075/99 | Loss: 0.2471 | Batch Acc: 94.83%\n",
            "    Val Batch 080/99 | Loss: 0.2370 | Batch Acc: 93.10%\n",
            "    Val Batch 085/99 | Loss: 0.3999 | Batch Acc: 87.93%\n",
            "    Val Batch 090/99 | Loss: 0.4204 | Batch Acc: 91.38%\n",
            "    Val Batch 095/99 | Loss: 0.0441 | Batch Acc: 98.28%\n",
            "    Val Batch 099/99 | Loss: 0.0203 | Batch Acc: 100.00%\n",
            "\n",
            "  Validation Summary | Epoch 4\n",
            "  Avg Loss: 0.2693 | Accuracy: 90.47%\n",
            "  Current Best Acc: 90.47%\n",
            "\n",
            "Epoch 5/5\n",
            "  Batch 010/900 | Loss: 0.4711 | CLoss: 0.3799 | FLoss: 0.1824 | LR: 2.86e-05\n",
            "  Batch 020/900 | Loss: 0.6106 | CLoss: 0.4563 | FLoss: 0.3087 | LR: 2.86e-05\n",
            "  Batch 030/900 | Loss: 0.3664 | CLoss: 0.2549 | FLoss: 0.2232 | LR: 2.86e-05\n",
            "  Batch 040/900 | Loss: 0.5326 | CLoss: 0.4302 | FLoss: 0.2047 | LR: 2.86e-05\n",
            "  Batch 050/900 | Loss: 0.6886 | CLoss: 0.5161 | FLoss: 0.3450 | LR: 2.86e-05\n",
            "  Batch 060/900 | Loss: 0.6368 | CLoss: 0.5421 | FLoss: 0.1895 | LR: 2.86e-05\n",
            "  Batch 070/900 | Loss: 0.6536 | CLoss: 0.5235 | FLoss: 0.2602 | LR: 2.86e-05\n",
            "  Batch 080/900 | Loss: 0.5477 | CLoss: 0.4250 | FLoss: 0.2454 | LR: 2.86e-05\n",
            "  Batch 090/900 | Loss: 0.5254 | CLoss: 0.4035 | FLoss: 0.2438 | LR: 2.86e-05\n",
            "  Batch 100/900 | Loss: 0.5113 | CLoss: 0.4087 | FLoss: 0.2052 | LR: 2.86e-05\n",
            "  Batch 110/900 | Loss: 0.6097 | CLoss: 0.5105 | FLoss: 0.1985 | LR: 2.86e-05\n",
            "  Batch 120/900 | Loss: 0.7279 | CLoss: 0.5391 | FLoss: 0.3776 | LR: 2.86e-05\n",
            "  Batch 130/900 | Loss: 0.6266 | CLoss: 0.4764 | FLoss: 0.3005 | LR: 2.86e-05\n",
            "  Batch 140/900 | Loss: 0.6680 | CLoss: 0.5330 | FLoss: 0.2700 | LR: 2.86e-05\n",
            "  Batch 150/900 | Loss: 0.8084 | CLoss: 0.6255 | FLoss: 0.3658 | LR: 2.86e-05\n",
            "  Batch 160/900 | Loss: 0.7340 | CLoss: 0.5882 | FLoss: 0.2915 | LR: 2.86e-05\n",
            "  Batch 170/900 | Loss: 0.7482 | CLoss: 0.6106 | FLoss: 0.2752 | LR: 2.86e-05\n",
            "  Batch 180/900 | Loss: 0.7728 | CLoss: 0.5980 | FLoss: 0.3495 | LR: 2.86e-05\n",
            "  Batch 190/900 | Loss: 0.3939 | CLoss: 0.3190 | FLoss: 0.1497 | LR: 2.86e-05\n",
            "  Batch 200/900 | Loss: 0.7196 | CLoss: 0.5783 | FLoss: 0.2827 | LR: 2.86e-05\n",
            "  Batch 210/900 | Loss: 0.3777 | CLoss: 0.3007 | FLoss: 0.1539 | LR: 2.86e-05\n",
            "  Batch 220/900 | Loss: 0.4127 | CLoss: 0.2798 | FLoss: 0.2657 | LR: 2.86e-05\n",
            "  Batch 230/900 | Loss: 0.7301 | CLoss: 0.5575 | FLoss: 0.3451 | LR: 2.86e-05\n",
            "  Batch 240/900 | Loss: 0.6325 | CLoss: 0.5399 | FLoss: 0.1852 | LR: 2.86e-05\n",
            "  Batch 250/900 | Loss: 0.4342 | CLoss: 0.3043 | FLoss: 0.2599 | LR: 2.86e-05\n",
            "  Batch 260/900 | Loss: 0.8025 | CLoss: 0.6600 | FLoss: 0.2849 | LR: 2.86e-05\n",
            "  Batch 270/900 | Loss: 0.5065 | CLoss: 0.3936 | FLoss: 0.2258 | LR: 2.86e-05\n",
            "  Batch 280/900 | Loss: 0.4494 | CLoss: 0.3467 | FLoss: 0.2054 | LR: 2.86e-05\n",
            "  Batch 290/900 | Loss: 0.4951 | CLoss: 0.3562 | FLoss: 0.2779 | LR: 2.86e-05\n",
            "  Batch 300/900 | Loss: 0.6039 | CLoss: 0.4583 | FLoss: 0.2912 | LR: 2.86e-05\n",
            "  Batch 310/900 | Loss: 0.5122 | CLoss: 0.3892 | FLoss: 0.2460 | LR: 2.86e-05\n",
            "  Batch 320/900 | Loss: 0.3046 | CLoss: 0.2430 | FLoss: 0.1231 | LR: 2.86e-05\n",
            "  Batch 330/900 | Loss: 0.4420 | CLoss: 0.3127 | FLoss: 0.2585 | LR: 2.86e-05\n",
            "  Batch 340/900 | Loss: 0.5804 | CLoss: 0.4784 | FLoss: 0.2039 | LR: 2.86e-05\n",
            "  Batch 350/900 | Loss: 0.2254 | CLoss: 0.1946 | FLoss: 0.0616 | LR: 2.86e-05\n",
            "  Batch 360/900 | Loss: 0.6947 | CLoss: 0.5263 | FLoss: 0.3367 | LR: 2.86e-05\n",
            "  Batch 370/900 | Loss: 0.3198 | CLoss: 0.2464 | FLoss: 0.1469 | LR: 2.86e-05\n",
            "  Batch 380/900 | Loss: 0.4832 | CLoss: 0.3034 | FLoss: 0.3596 | LR: 2.86e-05\n",
            "  Batch 390/900 | Loss: 0.5992 | CLoss: 0.5016 | FLoss: 0.1953 | LR: 2.86e-05\n",
            "  Batch 400/900 | Loss: 0.3882 | CLoss: 0.2593 | FLoss: 0.2579 | LR: 2.86e-05\n",
            "  Batch 410/900 | Loss: 0.5604 | CLoss: 0.4683 | FLoss: 0.1842 | LR: 2.86e-05\n",
            "  Batch 420/900 | Loss: 0.9314 | CLoss: 0.7471 | FLoss: 0.3686 | LR: 2.86e-05\n",
            "  Batch 430/900 | Loss: 0.6482 | CLoss: 0.5526 | FLoss: 0.1912 | LR: 2.86e-05\n",
            "  Batch 440/900 | Loss: 0.5730 | CLoss: 0.4977 | FLoss: 0.1506 | LR: 2.86e-05\n",
            "  Batch 450/900 | Loss: 0.5001 | CLoss: 0.3747 | FLoss: 0.2507 | LR: 2.86e-05\n",
            "  Batch 460/900 | Loss: 0.4632 | CLoss: 0.2903 | FLoss: 0.3459 | LR: 2.86e-05\n",
            "  Batch 470/900 | Loss: 0.3869 | CLoss: 0.2376 | FLoss: 0.2987 | LR: 2.86e-05\n",
            "  Batch 480/900 | Loss: 0.6148 | CLoss: 0.4739 | FLoss: 0.2819 | LR: 2.86e-05\n",
            "  Batch 490/900 | Loss: 0.8422 | CLoss: 0.6993 | FLoss: 0.2858 | LR: 2.86e-05\n",
            "  Batch 500/900 | Loss: 0.4171 | CLoss: 0.3300 | FLoss: 0.1742 | LR: 2.86e-05\n",
            "  Batch 510/900 | Loss: 0.4231 | CLoss: 0.3159 | FLoss: 0.2143 | LR: 2.86e-05\n",
            "  Batch 520/900 | Loss: 0.5964 | CLoss: 0.4369 | FLoss: 0.3190 | LR: 2.86e-05\n",
            "  Batch 530/900 | Loss: 0.5895 | CLoss: 0.4923 | FLoss: 0.1942 | LR: 2.86e-05\n",
            "  Batch 540/900 | Loss: 0.4087 | CLoss: 0.3460 | FLoss: 0.1254 | LR: 2.86e-05\n",
            "  Batch 550/900 | Loss: 0.6722 | CLoss: 0.4890 | FLoss: 0.3663 | LR: 2.86e-05\n",
            "  Batch 560/900 | Loss: 1.1720 | CLoss: 0.8885 | FLoss: 0.5669 | LR: 2.86e-05\n",
            "  Batch 570/900 | Loss: 0.7483 | CLoss: 0.5633 | FLoss: 0.3698 | LR: 2.86e-05\n",
            "  Batch 580/900 | Loss: 0.8343 | CLoss: 0.6519 | FLoss: 0.3649 | LR: 2.86e-05\n",
            "  Batch 590/900 | Loss: 0.6355 | CLoss: 0.5407 | FLoss: 0.1897 | LR: 2.86e-05\n",
            "  Batch 600/900 | Loss: 0.6209 | CLoss: 0.4970 | FLoss: 0.2478 | LR: 2.86e-05\n",
            "  Batch 610/900 | Loss: 0.3409 | CLoss: 0.2706 | FLoss: 0.1406 | LR: 2.86e-05\n",
            "  Batch 620/900 | Loss: 0.5381 | CLoss: 0.4163 | FLoss: 0.2436 | LR: 2.86e-05\n",
            "  Batch 630/900 | Loss: 0.6615 | CLoss: 0.5030 | FLoss: 0.3171 | LR: 2.86e-05\n",
            "  Batch 640/900 | Loss: 0.3604 | CLoss: 0.2462 | FLoss: 0.2283 | LR: 2.86e-05\n",
            "  Batch 650/900 | Loss: 0.3656 | CLoss: 0.2881 | FLoss: 0.1549 | LR: 2.86e-05\n",
            "  Batch 660/900 | Loss: 0.6040 | CLoss: 0.4793 | FLoss: 0.2496 | LR: 2.86e-05\n",
            "  Batch 670/900 | Loss: 0.2905 | CLoss: 0.1993 | FLoss: 0.1823 | LR: 2.86e-05\n",
            "  Batch 680/900 | Loss: 0.5651 | CLoss: 0.4820 | FLoss: 0.1661 | LR: 2.86e-05\n",
            "  Batch 690/900 | Loss: 0.4765 | CLoss: 0.4009 | FLoss: 0.1512 | LR: 2.86e-05\n",
            "  Batch 700/900 | Loss: 0.5891 | CLoss: 0.4590 | FLoss: 0.2601 | LR: 2.86e-05\n",
            "  Batch 710/900 | Loss: 0.8335 | CLoss: 0.6321 | FLoss: 0.4028 | LR: 2.86e-05\n",
            "  Batch 720/900 | Loss: 0.5904 | CLoss: 0.4149 | FLoss: 0.3510 | LR: 2.86e-05\n",
            "  Batch 730/900 | Loss: 0.5251 | CLoss: 0.3976 | FLoss: 0.2548 | LR: 2.86e-05\n",
            "  Batch 740/900 | Loss: 0.4454 | CLoss: 0.3533 | FLoss: 0.1842 | LR: 2.86e-05\n",
            "  Batch 750/900 | Loss: 0.7617 | CLoss: 0.5772 | FLoss: 0.3689 | LR: 2.86e-05\n",
            "  Batch 760/900 | Loss: 0.5843 | CLoss: 0.3698 | FLoss: 0.4290 | LR: 2.86e-05\n",
            "  Batch 770/900 | Loss: 0.4028 | CLoss: 0.2811 | FLoss: 0.2432 | LR: 2.86e-05\n",
            "  Batch 780/900 | Loss: 1.0242 | CLoss: 0.8624 | FLoss: 0.3236 | LR: 2.86e-05\n",
            "  Batch 790/900 | Loss: 1.1799 | CLoss: 0.9717 | FLoss: 0.4164 | LR: 2.86e-05\n",
            "  Batch 800/900 | Loss: 0.3442 | CLoss: 0.2887 | FLoss: 0.1108 | LR: 2.86e-05\n",
            "  Batch 810/900 | Loss: 0.5988 | CLoss: 0.4324 | FLoss: 0.3329 | LR: 2.86e-05\n",
            "  Batch 820/900 | Loss: 0.5447 | CLoss: 0.4011 | FLoss: 0.2871 | LR: 2.86e-05\n",
            "  Batch 830/900 | Loss: 0.6046 | CLoss: 0.5029 | FLoss: 0.2034 | LR: 2.86e-05\n",
            "  Batch 840/900 | Loss: 0.3966 | CLoss: 0.3471 | FLoss: 0.0991 | LR: 2.86e-05\n",
            "  Batch 850/900 | Loss: 0.6801 | CLoss: 0.5354 | FLoss: 0.2894 | LR: 2.86e-05\n",
            "  Batch 860/900 | Loss: 0.6693 | CLoss: 0.5086 | FLoss: 0.3214 | LR: 2.86e-05\n",
            "  Batch 870/900 | Loss: 0.4273 | CLoss: 0.3370 | FLoss: 0.1806 | LR: 2.86e-05\n",
            "  Batch 880/900 | Loss: 0.2519 | CLoss: 0.2063 | FLoss: 0.0911 | LR: 2.86e-05\n",
            "  Batch 890/900 | Loss: 0.4448 | CLoss: 0.3715 | FLoss: 0.1465 | LR: 2.86e-05\n",
            "  Batch 900/900 | Loss: 0.5691 | CLoss: 0.4609 | FLoss: 0.2164 | LR: 2.86e-05\n",
            "\n",
            "  Training Summary | Epoch 5\n",
            "  Avg Loss: 0.6057\n",
            "  Last Batch Loss: 0.5691\n",
            "\n",
            "  Validating...\n",
            "    Val Batch 005/99 | Loss: 0.7867 | Batch Acc: 65.52%\n",
            "    Val Batch 010/99 | Loss: 0.2393 | Batch Acc: 93.10%\n",
            "    Val Batch 015/99 | Loss: 0.3023 | Batch Acc: 93.10%\n",
            "    Val Batch 020/99 | Loss: 0.5045 | Batch Acc: 67.24%\n",
            "    Val Batch 025/99 | Loss: 0.5535 | Batch Acc: 86.21%\n",
            "    Val Batch 030/99 | Loss: 0.2601 | Batch Acc: 89.66%\n",
            "    Val Batch 035/99 | Loss: 0.1183 | Batch Acc: 98.28%\n",
            "    Val Batch 040/99 | Loss: 0.3229 | Batch Acc: 100.00%\n",
            "    Val Batch 045/99 | Loss: 0.2800 | Batch Acc: 96.55%\n",
            "    Val Batch 050/99 | Loss: 0.1884 | Batch Acc: 96.55%\n",
            "    Val Batch 055/99 | Loss: 0.0741 | Batch Acc: 98.28%\n",
            "    Val Batch 060/99 | Loss: 0.0618 | Batch Acc: 94.83%\n",
            "    Val Batch 065/99 | Loss: 0.4860 | Batch Acc: 82.76%\n",
            "    Val Batch 070/99 | Loss: 0.1645 | Batch Acc: 94.83%\n",
            "    Val Batch 075/99 | Loss: 0.0527 | Batch Acc: 100.00%\n",
            "    Val Batch 080/99 | Loss: 0.2882 | Batch Acc: 89.66%\n",
            "    Val Batch 085/99 | Loss: 0.3117 | Batch Acc: 93.10%\n",
            "    Val Batch 090/99 | Loss: 0.1083 | Batch Acc: 94.83%\n",
            "    Val Batch 095/99 | Loss: 0.0514 | Batch Acc: 98.28%\n",
            "    Val Batch 099/99 | Loss: 0.1547 | Batch Acc: 97.14%\n",
            "\n",
            "  Validation Summary | Epoch 5\n",
            "  Avg Loss: 0.2539 | Accuracy: 91.33%\n",
            "  Current Best Acc: 91.33%\n",
            "\n",
            "========================================\n",
            "=== Fold 10 Completed ===\n",
            "Best Validation Accuracy: 91.33%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import random\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# Configuration\n",
        "device = \"cuda:0\"\n",
        "num_epochs = 5\n",
        "num_folds = 10\n",
        "batch_size = 58\n",
        "feature_dim = 2048\n",
        "num_classes = 13\n",
        "root_dir = './spectrograms'\n",
        "csv_file = \"./spectrograms_balanced_no_sirens.csv\"\n",
        "\n",
        "# Get class names\n",
        "full_annotations = pd.read_csv(csv_file)\n",
        "class_names = sorted(full_annotations['classID'].unique())\n",
        "\n",
        "# Dataset Class\n",
        "class UrbanSoundDataset(Dataset):\n",
        "    def __init__(self, root_dir, folds, csv_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "\n",
        "        if isinstance(folds, int):\n",
        "            folds = [folds]\n",
        "        self.file_list = self.annotations[self.annotations['fold'].isin(folds)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.file_list.iloc[idx]\n",
        "        img_path = os.path.join(self.root_dir, f'fold{row[\"fold\"]}', row['spec_file_name'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = row['classID']\n",
        "\n",
        "        if self.transform:\n",
        "            xi = self.transform(image)\n",
        "            xj = self.transform(image)\n",
        "            return xi, xj, label\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Model Components\n",
        "class ProjectionHead(torch.nn.Module):\n",
        "    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class SimCLR(torch.nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection = ProjectionHead()\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.projection(features)\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim=2048, num_classes=13):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Loss Function\n",
        "class NTXentLoss(torch.nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        N = z_i.size(0)\n",
        "        z = torch.cat([z_i, z_j], dim=0)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        sim = torch.mm(z, z.T) / self.temperature\n",
        "\n",
        "        # Create labels: positives are the N off-diagonal elements\n",
        "        labels = torch.cat([\n",
        "            torch.arange(N, 2*N, device=z.device),\n",
        "            torch.arange(0, N, device=z.device)\n",
        "        ])\n",
        "\n",
        "        # Mask out self-similarity\n",
        "        mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
        "        sim = sim.masked_fill(mask, -1e9)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.criterion(sim, labels)\n",
        "        return loss\n",
        "\n",
        "#tried but does work require more memeory with batch size 64\n",
        "# def mixup_data(x1, x2, y1, y2, alpha=0.2):\n",
        "#     '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "#     lam = np.random.beta(alpha, alpha)\n",
        "#     batch_size = x1.size(0)\n",
        "#     index = torch.randperm(batch_size).cuda()\n",
        "\n",
        "#     mixed_x = lam * x1 + (1 - lam) * x2\n",
        "#     y_a, y_b = y1, y2\n",
        "#     return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Training Function with Mixup and CosineAnnealingLR\n",
        "def train():\n",
        "    # Initialize backbone\n",
        "    backbone = models.resnet50(pretrained=True)\n",
        "    backbone.fc = torch.nn.Identity()\n",
        "    simclr = SimCLR(backbone).to(device)\n",
        "    classifier = Classifier().to(device)\n",
        "    criterion = NTXentLoss()\n",
        "\n",
        "    # Data transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # K-fold cross-validation\n",
        "    for fold in range(6, num_folds+1):\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"=== Fold {fold}/{num_folds} {'='*20}\")\n",
        "        print(f\"{'='*40}\\n\")\n",
        "\n",
        "        # Data loaders\n",
        "        train_ds = UrbanSoundDataset(root_dir, [f for f in range(1,11) if f != fold],\n",
        "                                   csv_file, transform)\n",
        "        val_ds = UrbanSoundDataset(root_dir, [fold], csv_file, transform)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "        # Model components\n",
        "\n",
        "        #load the model till 4th fold and contnious training\n",
        "\n",
        "\n",
        "        if fold == 6:\n",
        "            checkpoint_dir = 'checkpoints'\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, 'fold_5_checkpoint.pth')\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "            simclr.load_state_dict(checkpoint['simclr'])\n",
        "            classifier.load_state_dict(checkpoint['classifier'])\n",
        "\n",
        "            continue\n",
        "        else:\n",
        "          # Cosine Annealing learning rate scheduler\n",
        "          optimizer = torch.optim.Adam(list(simclr.parameters()) + list(classifier.parameters()), lr=3e-4)\n",
        "\n",
        "          scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "          # Metrics storage\n",
        "          train_losses, val_losses = [], []\n",
        "          val_accuracies, all_preds, all_labels = [], [], []\n",
        "\n",
        "          for epoch in range(num_epochs):\n",
        "              print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "              # Training Phase\n",
        "              simclr.train()\n",
        "              classifier.train()\n",
        "              epoch_loss = 0\n",
        "              batch_count = 0\n",
        "\n",
        "              for batch_idx, (xi, xj, labels) in enumerate(train_loader):\n",
        "                  xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  zi, zj = simclr(xi), simclr(xj)\n",
        "                  loss_contrastive = criterion(zi, zj)\n",
        "\n",
        "                  # Classification\n",
        "                  features = simclr.backbone(xi)\n",
        "                  logits = classifier(features)\n",
        "                  loss_classification = torch.nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "                  # Total loss\n",
        "                  loss = loss_contrastive + 0.5 * loss_classification\n",
        "\n",
        "                  # Backward pass\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Progress tracking\n",
        "                  epoch_loss += loss.item()\n",
        "                  batch_count += 1\n",
        "\n",
        "                  # Print batch updates\n",
        "                  if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
        "                      current_lr = optimizer.param_groups[0]['lr']\n",
        "                      print(f\"  Batch {batch_idx + 1:03d}/{len(train_loader)} | \"\n",
        "                            f\"Loss: {loss.item():.4f} | \"\n",
        "                            f\"CLoss: {loss_contrastive.item():.4f} | \"\n",
        "                            f\"FLoss: {loss_classification.item():.4f} | \"\n",
        "                            f\"LR: {current_lr:.2e}\")\n",
        "\n",
        "              # Epoch statistics\n",
        "              avg_train_loss = epoch_loss / batch_count\n",
        "              train_losses.append(avg_train_loss)\n",
        "              print(f\"\\n  Training Summary | Epoch {epoch+1}\")\n",
        "              print(f\"  Avg Loss: {avg_train_loss:.4f}\")\n",
        "              print(f\"  Last Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "              scheduler.step()\n",
        "\n",
        "              # Validation Phase\n",
        "              simclr.eval()\n",
        "              classifier.eval()\n",
        "              val_loss, correct, total = 0, 0, 0\n",
        "\n",
        "              print(\"\\n  Validating...\")\n",
        "              with torch.no_grad():\n",
        "                  for batch_idx, (xi, _, labels) in enumerate(val_loader):\n",
        "                      xi, labels = xi.to(device), labels.to(device)\n",
        "\n",
        "                      # Forward pass\n",
        "                      features = simclr.backbone(xi)\n",
        "                      logits = classifier(features)\n",
        "\n",
        "                      # Loss calculation\n",
        "                      loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "                      val_loss += loss.item()\n",
        "\n",
        "                      # Accuracy calculation\n",
        "                      preds = torch.argmax(logits, dim=1)\n",
        "                      correct += (preds == labels).sum().item()\n",
        "                      total += labels.size(0)\n",
        "\n",
        "                      # Store predictions\n",
        "                      all_preds.extend(preds.cpu().numpy())\n",
        "                      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                      # Validation batch updates\n",
        "                      if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(val_loader):\n",
        "                          acc = 100 * (preds == labels).sum().item() / labels.size(0)\n",
        "                          print(f\"    Val Batch {batch_idx + 1:03d}/{len(val_loader)} | \"\n",
        "                                f\"Loss: {loss.item():.4f} | \"\n",
        "                                f\"Batch Acc: {acc:.2f}%\")\n",
        "\n",
        "              # Validation statistics\n",
        "              avg_val_loss = val_loss / len(val_loader)\n",
        "              val_losses.append(avg_val_loss)\n",
        "              val_acc = 100 * correct / total\n",
        "              val_accuracies.append(val_acc)\n",
        "\n",
        "              print(f\"\\n  Validation Summary | Epoch {epoch+1}\")\n",
        "              print(f\"  Avg Loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
        "              print(f\"  Current Best Acc: {max(val_accuracies):.2f}%\")\n",
        "\n",
        "              # Step the scheduler\n",
        "\n",
        "          # Fold Completion\n",
        "          print(f\"\\n{'='*40}\")\n",
        "          print(f\"=== Fold {fold} Completed ===\")\n",
        "          print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
        "          checkpoint_dir = 'checkpoints'  # Or provide an absolute path like '/path/to/save'\n",
        "          os.makedirs(checkpoint_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "          torch.save({\n",
        "                'simclr': simclr.state_dict(),\n",
        "                'classifier': classifier.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, os.path.join(checkpoint_dir, f'fold_{fold}_checkpoint.pth'))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spec = pd.read_csv('/content/spectrograms_balanced_no_sirens.csv')\n",
        "spec.sort_values('classID').groupby('fold').apply(lambda x: x['classID'].unique())\n",
        "spec.sort_values('classID')['class'].unique()\n",
        "# spec[spec['classID'] == 11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVMcxLqn75pX",
        "outputId": "ee904f8f-71f7-4e78-d3e9-1404ae758a01"
      },
      "id": "OVMcxLqn75pX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-61890aece928>:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  spec.sort_values('classID').groupby('fold').apply(lambda x: x['classID'].unique())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['air_conditioner', 'ambulance', 'car_horn', 'children_playing',\n",
              "       'dog_bark', 'drilling', 'engine_idling', 'firetruck', 'gun_shot',\n",
              "       'jackhammer', 'police', 'street_music', 'traffic'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c2c9d11-4ce6-41c4-8dee-5dded2647d4f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2c9d11-4ce6-41c4-8dee-5dded2647d4f",
        "outputId": "64031450-eb97-4612-d674-4606c0f7daf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint from ./checkpoints/fold_10_checkpoint.pth\n",
            "Checkpoint keys: dict_keys(['simclr', 'classifier', 'optimizer'])\n",
            "Processing audio: 75743-0-0-11.wav\n",
            "spec_file_name    75743-0-orig.png\n",
            "orig_file_name    75743-0-0-11.wav\n",
            "fsID                         75743\n",
            "fold                             9\n",
            "classID                          0\n",
            "class              air_conditioner\n",
            "augmentation              original\n",
            "Name: 47838, dtype: object\n",
            "/content/spectrograms/fold9/75743-0-orig.png\n",
            "[9.9959117e-01 4.3854552e-08 1.7155821e-07 4.3200230e-06 1.6912589e-05\n",
            " 1.0995423e-05 1.5801904e-04 1.8663379e-05 1.8172510e-07 1.9268055e-05\n",
            " 1.7953918e-06 1.7802566e-04 3.9542076e-07]\n",
            "\n",
            "Top 3 predictions:\n",
            "[ 0 11  6]\n",
            "1. air_conditioner: 99.96%\n",
            "2. street_music: 0.02%\n",
            "3. engine_idling: 0.02%\n",
            "\n",
            "Final prediction: air_conditioner with 99.96% confidence\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import IPython.display as ipd\n",
        "import pandas as pd\n",
        "\n",
        "# Model Components (same as in training script)\n",
        "class ProjectionHead(torch.nn.Module):\n",
        "    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class SimCLR(torch.nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection = ProjectionHead()\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.projection(features)\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim=2048, num_classes=13):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def process_audio(audio_path, sr=22050, duration=None, n_mels=224):\n",
        "    \"\"\"Process audio file to mel spectrogram\"\"\"\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
        "\n",
        "    # Create mel spectrogram\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048,\n",
        "                                      hop_length=512, n_mels=n_mels)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    # Convert to image\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)\n",
        "\n",
        "    # Save temporarily and load as PIL\n",
        "    temp_path = 'temp_spec.png'\n",
        "    plt.savefig(temp_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "    img = Image.open(temp_path).convert('RGB')\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "\n",
        "    return img, y, sr\n",
        "\n",
        "class AudioClassifier:\n",
        "    def __init__(self, checkpoint_path, class_names, device=None):\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize model components\n",
        "        backbone = models.resnet50(pretrained=False)\n",
        "        backbone.fc = torch.nn.Identity()\n",
        "\n",
        "        self.simclr = SimCLR(backbone).to(self.device)\n",
        "        self.classifier = Classifier().to(self.device)\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # Load checkpoint\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        # Debug: print keys\n",
        "        print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
        "\n",
        "        self.simclr.load_state_dict(checkpoint['simclr'])\n",
        "        self.classifier.load_state_dict(checkpoint['classifier'])\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        self.simclr.eval()\n",
        "        self.classifier.eval()\n",
        "\n",
        "        # Define transforms (similar to validation transforms)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def predict(self, audio_path, verbose=True):\n",
        "        \"\"\"Predict class for audio file\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"Processing audio: {audio_path}\")\n",
        "\n",
        "        # Process audio to spectrogram image\n",
        "        # img, audio, sr = process_audio(audio_path)\n",
        "        #extract the the img\n",
        "        r = pd.read_csv('./spectrograms_balanced_no_sirens.csv')\n",
        "        img = r[r['orig_file_name'] == audio_path].iloc[0]\n",
        "        print(img)\n",
        "        path= '/content/spectrograms/'+ 'fold'+str(img['fold']) + '/' + img['spec_file_name']\n",
        "        print(path)\n",
        "        img = Image.open(path).convert('RGB')\n",
        "\n",
        "        # Apply transforms\n",
        "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            # Extract features\n",
        "            features = self.simclr.backbone(img_tensor)\n",
        "\n",
        "            # Get logits from classifier\n",
        "            logits = self.classifier(features)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "            print(probs)\n",
        "\n",
        "        if verbose:\n",
        "            # Print top 3 predictions\n",
        "            top_indices = np.argsort(probs)[::-1][:3]  # Sort in descending order\n",
        "            print(\"\\nTop 3 predictions:\")\n",
        "            print(top_indices)\n",
        "\n",
        "            for i, idx in enumerate(top_indices):\n",
        "                print(f\"{i+1}. {self.class_names[idx]}: {probs[idx]*100:.2f}%\")\n",
        "\n",
        "        return probs\n",
        "\n",
        "def visualize_prediction(probs, class_names):\n",
        "    \"\"\"Visualize prediction with audio playback\"\"\"\n",
        "    # Play audio\n",
        "    # print(\"Audio sample:\")\n",
        "    # display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "    # Plot waveform and probabilities\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Waveform\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio)\n",
        "    # plt.title(\"Waveform\")\n",
        "    # plt.xlabel(\"Time (s)\")\n",
        "    # plt.ylabel(\"Amplitude\")\n",
        "\n",
        "    # Class probabilities\n",
        "    plt.subplot(1, 2, 2)\n",
        "    indices = np.argsort(probs)[::-1]\n",
        "    plt.barh(range(len(class_names)), [probs[i] for i in indices])\n",
        "    plt.yticks(range(len(class_names)), [class_names[i] for i in indices])\n",
        "    plt.title(\"Class Probabilities\")\n",
        "    plt.xlabel(\"Probability\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define class names (same as in training)\n",
        "    class_names = ['air_conditioner', 'ambulance', 'car_horn', 'children_playing',\n",
        "       'dog_bark', 'drilling', 'engine_idling', 'firetruck', 'gun_shot',\n",
        "       'jackhammer', 'police', 'street_music', 'traffic']\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = AudioClassifier(\n",
        "        checkpoint_path='./checkpoints/fold_10_checkpoint.pth',\n",
        "        class_names=class_names\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    audio_path = \"75743-0-0-11.wav\"  # Replace with your audio file\n",
        "    probs = classifier.predict(audio_path)\n",
        "\n",
        "    # Visualize\n",
        "    # visualize_prediction(probs, class_names)\n",
        "\n",
        "    # Get top prediction\n",
        "    top_idx = np.argmax(probs)\n",
        "    print(f\"\\nFinal prediction: {class_names[top_idx]} with {probs[top_idx]*100:.2f}% confidence\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def predict_on_dataframe(df, model_path, class_names, image_dir='./spectrograms', save_results=True):\n",
        "    \"\"\"\n",
        "    Predict classes for a set of images using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the spectrogram file names and class names\n",
        "        model_path: Path to the trained model checkpoint\n",
        "        class_names: List of class names\n",
        "        image_dir: Directory where spectrogram images are stored\n",
        "        save_results: Whether to save the predictions to a CSV file\n",
        "\n",
        "    Returns:\n",
        "        predictions_df: DataFrame containing the predictions\n",
        "    \"\"\"\n",
        "    backbone = models.resnet50(pretrained=False)\n",
        "    backbone.fc = torch.nn.Identity()\n",
        "\n",
        "    simclr = SimCLR(backbone).to(\"cuda:0\")\n",
        "    classifier = Classifier().to(\"cuda:0\")\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=\"cuda:0\")\n",
        "    simclr.load_state_dict(checkpoint['simclr'])\n",
        "    classifier.load_state_dict(checkpoint['classifier'])\n",
        "\n",
        "    simclr.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization for ResNet\n",
        "    ])\n",
        "\n",
        "    # Initialize list to store results\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        img_name = row['spec_file_name']\n",
        "        true_class = row['classID']\n",
        "        fold = row['fold']\n",
        "\n",
        "        img_path = os.path.join(image_dir, f\"fold{fold}\", img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = transform(image).unsqueeze(0).to(\"cuda:0\")\n",
        "\n",
        "        features = simclr.backbone(image)\n",
        "\n",
        "        logits = classifier(features)\n",
        "\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        pred_class = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs.max().item()\n",
        "\n",
        "\n",
        "        results.append({\n",
        "            'spec_file_name': img_name,\n",
        "            'true_class': true_class,\n",
        "            'pred_class': pred_class,\n",
        "            'confidence': confidence,\n",
        "            'probabilities': probs.cpu().detach().numpy().tolist()\n",
        "        })\n",
        "\n",
        "\n",
        "    predictions_df = pd.DataFrame(results)\n",
        "\n",
        "    if save_results:\n",
        "        predictions_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    class_names  = ['air_conditioner', 'ambulance', 'car_horn', 'children_playing',\n",
        "       'dog_bark', 'drilling', 'engine_idling', 'firetruck', 'gun_shot',\n",
        "       'jackhammer', 'police', 'street_music', 'traffic']\n",
        "\n",
        "    model_checkpoint_path = '/content/checkpoints/fold_10_checkpoint.pth'\n",
        "\n",
        "\n",
        "    predictions_df = predict_on_dataframe(test_df, model_checkpoint_path, class_names)\n",
        "        # Optionally print or inspect results\n",
        "    print(predictions_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1xS77Ogrf4k",
        "outputId": "eca1fe94-29f9-453b-b637-fa3b10345f2a"
      },
      "id": "C1xS77Ogrf4k",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100%|| 11584/11584 [04:03<00:00, 47.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          spec_file_name  true_class  pred_class  confidence  \\\n",
            "0  7803284754-13-pa1.png          12           8    0.332234   \n",
            "1   7802093679-12-n1.png          10          10    0.999359   \n",
            "2  7801911300-12-sa2.png          10           8    0.509390   \n",
            "3  7798879923-10-ts0.png           1           1    0.829199   \n",
            "4   7798630186-10-n0.png           1          10    0.723252   \n",
            "\n",
            "                                       probabilities  \n",
            "0  [[7.309515240194742e-06, 0.18852150440216064, ...  \n",
            "1  [[1.3022838174947537e-05, 3.881257271132199e-0...  \n",
            "2  [[3.341061756145791e-06, 0.38381245732307434, ...  \n",
            "3  [[4.111320706101651e-08, 0.8291988372802734, 0...  \n",
            "4  [[4.7775876737432554e-05, 0.05374174565076828,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_top_n_accuracy_per_class(predictions_df, top_percent=1):\n",
        "    \"\"\"\n",
        "    Calculate Top N% accuracy for each class based on maximum probabilities.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: DataFrame containing predictions with 'pred_class', 'true_class', and 'probabilities' columns\n",
        "        top_percent: The percentage (1, 5, 10) for Top N% accuracy\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing the Top N% accuracy for each class\n",
        "    \"\"\"\n",
        "    top_n_accuracy = []\n",
        "\n",
        "    print(\"Example probabilities from the DataFrame:\")\n",
        "    print(predictions_df['probabilities'].head())\n",
        "\n",
        "    for class_name, group in predictions_df.groupby('true_class'):\n",
        "        class_size = len(group)\n",
        "\n",
        "        top_n_count = max(1, int(class_size * top_percent / 100))\n",
        "        print(f\"Class: {class_name}, Class Size: {class_size}, Top N Count: {top_n_count}\")\n",
        "        group['max_prob'] = group['probabilities'].apply(lambda x: max(x[0]) if isinstance(x, list) and isinstance(x[0], list) else max(x)).astype(float)\n",
        "\n",
        "        print(\"Example max_prob values from the DataFrame:\")\n",
        "        print(group[['spec_file_name', 'max_prob']].head())\n",
        "\n",
        "        if top_n_count > 0:\n",
        "            top_n_group = group.nlargest(top_n_count, 'max_prob')\n",
        "\n",
        "            correct_predictions = (top_n_group['pred_class'] == top_n_group['true_class']).sum()\n",
        "            accuracy = (correct_predictions / top_n_count) * 100\n",
        "\n",
        "            top_n_accuracy.append({\n",
        "                'Class': class_name,\n",
        "                f'Top {top_percent}% Accuracy': f\"{accuracy:.2f}%\"\n",
        "            })\n",
        "        else:\n",
        "            top_n_accuracy.append({\n",
        "                'Class': class_name,\n",
        "                f'Top {top_percent}% Accuracy': \"Not enough samples\"\n",
        "            })\n",
        "\n",
        "    accuracy_df = pd.DataFrame(top_n_accuracy)\n",
        "\n",
        "    accuracy_df = accuracy_df.sort_values(by='Class', ascending=True)\n",
        "\n",
        "    return accuracy_df\n",
        "\n",
        "top_5_accuracy_df = calculate_top_n_accuracy_per_class(predictions_df, top_percent=1)\n",
        "\n",
        "print(top_5_accuracy_df.to_string(index=False))\n",
        "\n",
        "\n",
        "top_5_accuracy_df = calculate_top_n_accuracy_per_class(predictions_df, top_percent=5)\n",
        "\n",
        "print(top_5_accuracy_df.to_string(index=False))\n",
        "\n",
        "top_10_accuracy_df = calculate_top_n_accuracy_per_class(predictions_df, top_percent=10)\n",
        "print(top_10_accuracy_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YFwEzmQx92h",
        "outputId": "1f62891a-6282-45f3-a6d6-cffa53b5bf81"
      },
      "id": "9YFwEzmQx92h",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example probabilities from the DataFrame:\n",
            "0    [[7.309515240194742e-06, 0.18852150440216064, ...\n",
            "1    [[1.3022838174947537e-05, 3.881257271132199e-0...\n",
            "2    [[3.341061756145791e-06, 0.38381245732307434, ...\n",
            "3    [[4.111320706101651e-08, 0.8291988372802734, 0...\n",
            "4    [[4.7775876737432554e-05, 0.05374174565076828,...\n",
            "Name: probabilities, dtype: object\n",
            "Class: 0, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "14  100852-0-sa0.png  0.751515\n",
            "45   75743-0-ps1.png  0.336204\n",
            "55  100852-0-ts0.png  0.709551\n",
            "69  153261-0-ps1.png  0.995751\n",
            "92  146709-0-sa0.png  0.977597\n",
            "Class: 1, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "            spec_file_name  max_prob\n",
            "3    7798879923-10-ts0.png  0.829199\n",
            "4     7798630186-10-n0.png  0.723252\n",
            "22    7798628988-10-n1.png  0.568961\n",
            "50  7798720632-10-orig.png  0.941548\n",
            "58   7798715376-10-sa2.png  0.729578\n",
            "Class: 2, Class Size: 808, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "38   175855-1-pa0.png  0.929720\n",
            "85   125520-1-ps0.png  0.946549\n",
            "101   199769-1-n1.png  0.794779\n",
            "115   24074-1-ts1.png  0.999864\n",
            "146    24074-1-n0.png  0.935002\n",
            "Class: 3, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "28   182739-2-pa0.png  0.997514\n",
            "37    54173-2-pa0.png  0.988613\n",
            "63   155280-2-ps1.png  0.818560\n",
            "81    76266-2-sa2.png  0.707142\n",
            "124   81787-2-pa0.png  0.769142\n",
            "Class: 4, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "11    190996-3-n1.png  0.958859\n",
            "12  193394-3-orig.png  0.982504\n",
            "16   183992-3-sa2.png  0.982052\n",
            "19   183989-3-ts1.png  0.512118\n",
            "20  174994-3-orig.png  0.838769\n",
            "Class: 5, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "6    77751-4-orig.png  0.347882\n",
            "23   185801-4-ts1.png  0.999977\n",
            "29  104817-4-orig.png  0.998685\n",
            "32   205874-4-sa2.png  0.829660\n",
            "43  137815-4-orig.png  0.999773\n",
            "Class: 6, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "24  102857-5-pa0.png  0.556184\n",
            "36  166101-5-ts0.png  0.677731\n",
            "57  176787-5-ps1.png  0.997161\n",
            "59   209992-5-n0.png  0.684680\n",
            "68  176787-5-sa0.png  0.560709\n",
            "Class: 7, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "           spec_file_name  max_prob\n",
            "15   7799027323-11-n1.png  0.989812\n",
            "18  7799308235-11-ps1.png  0.873502\n",
            "21  7799325988-11-sa0.png  0.851128\n",
            "44  7799368183-11-sa2.png  0.718792\n",
            "56   7799091287-11-n1.png  0.963644\n",
            "Class: 8, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "9   135544-6-pa0.png  0.743271\n",
            "13    46656-6-n0.png  0.831086\n",
            "41  197320-6-ps0.png  0.948876\n",
            "49  174287-6-ts1.png  0.844945\n",
            "52  200460-6-sa1.png  0.955984\n",
            "Class: 9, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "31  188824-7-ps1.png  0.362932\n",
            "40   105029-7-n1.png  0.997396\n",
            "53  105029-7-sa1.png  0.314966\n",
            "62  54697-7-orig.png  0.826094\n",
            "70  165039-7-sa2.png  0.995014\n",
            "Class: 10, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "            spec_file_name  max_prob\n",
            "1     7802093679-12-n1.png  0.999359\n",
            "2    7801911300-12-sa2.png  0.509390\n",
            "17   7801789557-12-pa1.png  0.395554\n",
            "27  7801479501-12-orig.png  0.663544\n",
            "47   7800102421-12-pa0.png  0.763966\n",
            "Class: 11, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "7    172519-9-ps1.png  0.667371\n",
            "8    202334-9-pa1.png  0.507498\n",
            "10  203424-9-orig.png  0.810626\n",
            "26     94182-9-n0.png  0.491926\n",
            "35   14386-9-orig.png  0.958644\n",
            "Class: 12, Class Size: 898, Top N Count: 8\n",
            "Example max_prob values from the DataFrame:\n",
            "           spec_file_name  max_prob\n",
            "0   7803284754-13-pa1.png  0.332234\n",
            "5   7803291358-13-sa0.png  0.940155\n",
            "25  7803171446-13-pa0.png  0.732547\n",
            "33   7803418323-13-n0.png  0.785088\n",
            "34   7802315383-13-n1.png  0.888527\n",
            " Class Top 1% Accuracy\n",
            "     0          50.00%\n",
            "     1         100.00%\n",
            "     2         100.00%\n",
            "     3         100.00%\n",
            "     4         100.00%\n",
            "     5         100.00%\n",
            "     6         100.00%\n",
            "     7         100.00%\n",
            "     8          50.00%\n",
            "     9         100.00%\n",
            "    10         100.00%\n",
            "    11         100.00%\n",
            "    12          87.50%\n",
            "Example probabilities from the DataFrame:\n",
            "0    [[7.309515240194742e-06, 0.18852150440216064, ...\n",
            "1    [[1.3022838174947537e-05, 3.881257271132199e-0...\n",
            "2    [[3.341061756145791e-06, 0.38381245732307434, ...\n",
            "3    [[4.111320706101651e-08, 0.8291988372802734, 0...\n",
            "4    [[4.7775876737432554e-05, 0.05374174565076828,...\n",
            "Name: probabilities, dtype: object\n",
            "Class: 0, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "14  100852-0-sa0.png  0.751515\n",
            "45   75743-0-ps1.png  0.336204\n",
            "55  100852-0-ts0.png  0.709551\n",
            "69  153261-0-ps1.png  0.995751\n",
            "92  146709-0-sa0.png  0.977597\n",
            "Class: 1, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "            spec_file_name  max_prob\n",
            "3    7798879923-10-ts0.png  0.829199\n",
            "4     7798630186-10-n0.png  0.723252\n",
            "22    7798628988-10-n1.png  0.568961\n",
            "50  7798720632-10-orig.png  0.941548\n",
            "58   7798715376-10-sa2.png  0.729578\n",
            "Class: 2, Class Size: 808, Top N Count: 40\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "38   175855-1-pa0.png  0.929720\n",
            "85   125520-1-ps0.png  0.946549\n",
            "101   199769-1-n1.png  0.794779\n",
            "115   24074-1-ts1.png  0.999864\n",
            "146    24074-1-n0.png  0.935002\n",
            "Class: 3, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "28   182739-2-pa0.png  0.997514\n",
            "37    54173-2-pa0.png  0.988613\n",
            "63   155280-2-ps1.png  0.818560\n",
            "81    76266-2-sa2.png  0.707142\n",
            "124   81787-2-pa0.png  0.769142\n",
            "Class: 4, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "11    190996-3-n1.png  0.958859\n",
            "12  193394-3-orig.png  0.982504\n",
            "16   183992-3-sa2.png  0.982052\n",
            "19   183989-3-ts1.png  0.512118\n",
            "20  174994-3-orig.png  0.838769\n",
            "Class: 5, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "6    77751-4-orig.png  0.347882\n",
            "23   185801-4-ts1.png  0.999977\n",
            "29  104817-4-orig.png  0.998685\n",
            "32   205874-4-sa2.png  0.829660\n",
            "43  137815-4-orig.png  0.999773\n",
            "Class: 6, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "24  102857-5-pa0.png  0.556184\n",
            "36  166101-5-ts0.png  0.677731\n",
            "57  176787-5-ps1.png  0.997161\n",
            "59   209992-5-n0.png  0.684680\n",
            "68  176787-5-sa0.png  0.560709\n",
            "Class: 7, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "           spec_file_name  max_prob\n",
            "15   7799027323-11-n1.png  0.989812\n",
            "18  7799308235-11-ps1.png  0.873502\n",
            "21  7799325988-11-sa0.png  0.851128\n",
            "44  7799368183-11-sa2.png  0.718792\n",
            "56   7799091287-11-n1.png  0.963644\n",
            "Class: 8, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "9   135544-6-pa0.png  0.743271\n",
            "13    46656-6-n0.png  0.831086\n",
            "41  197320-6-ps0.png  0.948876\n",
            "49  174287-6-ts1.png  0.844945\n",
            "52  200460-6-sa1.png  0.955984\n",
            "Class: 9, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "31  188824-7-ps1.png  0.362932\n",
            "40   105029-7-n1.png  0.997396\n",
            "53  105029-7-sa1.png  0.314966\n",
            "62  54697-7-orig.png  0.826094\n",
            "70  165039-7-sa2.png  0.995014\n",
            "Class: 10, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "            spec_file_name  max_prob\n",
            "1     7802093679-12-n1.png  0.999359\n",
            "2    7801911300-12-sa2.png  0.509390\n",
            "17   7801789557-12-pa1.png  0.395554\n",
            "27  7801479501-12-orig.png  0.663544\n",
            "47   7800102421-12-pa0.png  0.763966\n",
            "Class: 11, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "7    172519-9-ps1.png  0.667371\n",
            "8    202334-9-pa1.png  0.507498\n",
            "10  203424-9-orig.png  0.810626\n",
            "26     94182-9-n0.png  0.491926\n",
            "35   14386-9-orig.png  0.958644\n",
            "Class: 12, Class Size: 898, Top N Count: 44\n",
            "Example max_prob values from the DataFrame:\n",
            "           spec_file_name  max_prob\n",
            "0   7803284754-13-pa1.png  0.332234\n",
            "5   7803291358-13-sa0.png  0.940155\n",
            "25  7803171446-13-pa0.png  0.732547\n",
            "33   7803418323-13-n0.png  0.785088\n",
            "34   7802315383-13-n1.png  0.888527\n",
            " Class Top 5% Accuracy\n",
            "     0          34.09%\n",
            "     1         100.00%\n",
            "     2         100.00%\n",
            "     3         100.00%\n",
            "     4         100.00%\n",
            "     5         100.00%\n",
            "     6          95.45%\n",
            "     7          95.45%\n",
            "     8          72.73%\n",
            "     9         100.00%\n",
            "    10         100.00%\n",
            "    11          95.45%\n",
            "    12          68.18%\n",
            "Example probabilities from the DataFrame:\n",
            "0    [[7.309515240194742e-06, 0.18852150440216064, ...\n",
            "1    [[1.3022838174947537e-05, 3.881257271132199e-0...\n",
            "2    [[3.341061756145791e-06, 0.38381245732307434, ...\n",
            "3    [[4.111320706101651e-08, 0.8291988372802734, 0...\n",
            "4    [[4.7775876737432554e-05, 0.05374174565076828,...\n",
            "Name: probabilities, dtype: object\n",
            "Class: 0, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "14  100852-0-sa0.png  0.751515\n",
            "45   75743-0-ps1.png  0.336204\n",
            "55  100852-0-ts0.png  0.709551\n",
            "69  153261-0-ps1.png  0.995751\n",
            "92  146709-0-sa0.png  0.977597\n",
            "Class: 1, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "            spec_file_name  max_prob\n",
            "3    7798879923-10-ts0.png  0.829199\n",
            "4     7798630186-10-n0.png  0.723252\n",
            "22    7798628988-10-n1.png  0.568961\n",
            "50  7798720632-10-orig.png  0.941548\n",
            "58   7798715376-10-sa2.png  0.729578\n",
            "Class: 2, Class Size: 808, Top N Count: 80\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "38   175855-1-pa0.png  0.929720\n",
            "85   125520-1-ps0.png  0.946549\n",
            "101   199769-1-n1.png  0.794779\n",
            "115   24074-1-ts1.png  0.999864\n",
            "146    24074-1-n0.png  0.935002\n",
            "Class: 3, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "28   182739-2-pa0.png  0.997514\n",
            "37    54173-2-pa0.png  0.988613\n",
            "63   155280-2-ps1.png  0.818560\n",
            "81    76266-2-sa2.png  0.707142\n",
            "124   81787-2-pa0.png  0.769142\n",
            "Class: 4, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "11    190996-3-n1.png  0.958859\n",
            "12  193394-3-orig.png  0.982504\n",
            "16   183992-3-sa2.png  0.982052\n",
            "19   183989-3-ts1.png  0.512118\n",
            "20  174994-3-orig.png  0.838769\n",
            "Class: 5, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "6    77751-4-orig.png  0.347882\n",
            "23   185801-4-ts1.png  0.999977\n",
            "29  104817-4-orig.png  0.998685\n",
            "32   205874-4-sa2.png  0.829660\n",
            "43  137815-4-orig.png  0.999773\n",
            "Class: 6, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "24  102857-5-pa0.png  0.556184\n",
            "36  166101-5-ts0.png  0.677731\n",
            "57  176787-5-ps1.png  0.997161\n",
            "59   209992-5-n0.png  0.684680\n",
            "68  176787-5-sa0.png  0.560709\n",
            "Class: 7, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "           spec_file_name  max_prob\n",
            "15   7799027323-11-n1.png  0.989812\n",
            "18  7799308235-11-ps1.png  0.873502\n",
            "21  7799325988-11-sa0.png  0.851128\n",
            "44  7799368183-11-sa2.png  0.718792\n",
            "56   7799091287-11-n1.png  0.963644\n",
            "Class: 8, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "9   135544-6-pa0.png  0.743271\n",
            "13    46656-6-n0.png  0.831086\n",
            "41  197320-6-ps0.png  0.948876\n",
            "49  174287-6-ts1.png  0.844945\n",
            "52  200460-6-sa1.png  0.955984\n",
            "Class: 9, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "      spec_file_name  max_prob\n",
            "31  188824-7-ps1.png  0.362932\n",
            "40   105029-7-n1.png  0.997396\n",
            "53  105029-7-sa1.png  0.314966\n",
            "62  54697-7-orig.png  0.826094\n",
            "70  165039-7-sa2.png  0.995014\n",
            "Class: 10, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "            spec_file_name  max_prob\n",
            "1     7802093679-12-n1.png  0.999359\n",
            "2    7801911300-12-sa2.png  0.509390\n",
            "17   7801789557-12-pa1.png  0.395554\n",
            "27  7801479501-12-orig.png  0.663544\n",
            "47   7800102421-12-pa0.png  0.763966\n",
            "Class: 11, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "       spec_file_name  max_prob\n",
            "7    172519-9-ps1.png  0.667371\n",
            "8    202334-9-pa1.png  0.507498\n",
            "10  203424-9-orig.png  0.810626\n",
            "26     94182-9-n0.png  0.491926\n",
            "35   14386-9-orig.png  0.958644\n",
            "Class: 12, Class Size: 898, Top N Count: 89\n",
            "Example max_prob values from the DataFrame:\n",
            "           spec_file_name  max_prob\n",
            "0   7803284754-13-pa1.png  0.332234\n",
            "5   7803291358-13-sa0.png  0.940155\n",
            "25  7803171446-13-pa0.png  0.732547\n",
            "33   7803418323-13-n0.png  0.785088\n",
            "34   7802315383-13-n1.png  0.888527\n",
            " Class Top 10% Accuracy\n",
            "     0           38.20%\n",
            "     1          100.00%\n",
            "     2          100.00%\n",
            "     3          100.00%\n",
            "     4           97.75%\n",
            "     5          100.00%\n",
            "     6           91.01%\n",
            "     7           85.39%\n",
            "     8           57.30%\n",
            "     9          100.00%\n",
            "    10           98.88%\n",
            "    11           76.40%\n",
            "    12           59.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix_and_f1(predictions_df, class_names):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix and calculate F1 score for each class.\n",
        "\n",
        "    Args:\n",
        "        predictions_df: DataFrame containing the predictions with 'true_class' and 'pred_class'\n",
        "        class_names: List of all possible class names\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"Unique true labels in predictions_df:\", predictions_df['true_class'].unique())\n",
        "    print(\"Unique predicted labels in predictions_df:\", predictions_df['pred_class'].unique())\n",
        "\n",
        "    missing_true_labels = predictions_df['true_class'].isnull().sum()\n",
        "    missing_pred_labels = predictions_df['pred_class'].isnull().sum()\n",
        "    if missing_true_labels > 0 or missing_pred_labels > 0:\n",
        "        print(f\"Warning: {missing_true_labels} missing true labels, {missing_pred_labels} missing predicted labels.\")\n",
        "\n",
        "    y_true = predictions_df['true_class'].values\n",
        "    y_pred = predictions_df['pred_class'].values\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))\n",
        "\n",
        "    f1_scores = f1_score(y_true, y_pred, labels=np.arange(len(class_names)), average=None)\n",
        "\n",
        "    f1_df = pd.DataFrame({\n",
        "        'Class': class_names,\n",
        "        'F1 Score': f1_scores\n",
        "    })\n",
        "    print(\"\\nF1 Score per class:\")\n",
        "    print(f1_df)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix_and_f1(predictions_df, class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qyg9HgqH1id1",
        "outputId": "b9650b02-2d67-42e6-fd92-55aa4f2b8480"
      },
      "id": "qyg9HgqH1id1",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique true labels in predictions_df: [12 10  1  5 11  8  4  0  7  6  3  9  2]\n",
            "Unique predicted labels in predictions_df: [ 8 10  1  4  7 11  6  5  3  9  0 12  2]\n",
            "\n",
            "F1 Score per class:\n",
            "               Class  F1 Score\n",
            "0    air_conditioner  0.358025\n",
            "1          ambulance  0.346187\n",
            "2           car_horn  0.463436\n",
            "3   children_playing  0.652062\n",
            "4           dog_bark  0.252822\n",
            "5           drilling  0.718130\n",
            "6      engine_idling  0.485784\n",
            "7          firetruck  0.619222\n",
            "8           gun_shot  0.212550\n",
            "9         jackhammer  0.694842\n",
            "10            police  0.647059\n",
            "11      street_music  0.392670\n",
            "12           traffic  0.437500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "985031ea-d337-4443-a574-b9b43d2669ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "985031ea-d337-4443-a574-b9b43d2669ea",
        "outputId": "3b5eebf2-5646-486a-d33f-7b6753a266ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3efc93cf-970a-49b0-bd5b-cb7d852c3a9c\", \"checkpoint.zip\", 2693614947)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoint.zip')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}