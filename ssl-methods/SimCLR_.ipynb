{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7d0774-b070-4b9c-a7ec-d42beec76056",
   "metadata": {
    "id": "9a7d0774-b070-4b9c-a7ec-d42beec76056"
   },
   "source": [
    "# Enhanced SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aoRtkQmGmLs7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoRtkQmGmLs7",
    "outputId": "22a8d832-0893-4f7f-a825-9f3a7c76b5d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ZClyR2szdlu",
   "metadata": {
    "id": "9ZClyR2szdlu"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/My Drive/file.txt', 'w') as f:\n",
    "  f.write('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AGquYxwRchQY",
   "metadata": {
    "id": "AGquYxwRchQY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec326472-5abf-4e6f-8303-e0e1eea36e67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ec326472-5abf-4e6f-8303-e0e1eea36e67",
    "outputId": "ba98d45d-e1cf-493e-b049-6e0f49b40094"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "    Val Batch 085/101 | Loss: 0.2402 | Batch Acc: 91.38%\n",
      "    Val Batch 090/101 | Loss: 0.1882 | Batch Acc: 87.93%\n",
      "    Val Batch 095/101 | Loss: 0.3688 | Batch Acc: 89.66%\n",
      "    Val Batch 100/101 | Loss: 0.0711 | Batch Acc: 98.28%\n",
      "    Val Batch 101/101 | Loss: 0.2160 | Batch Acc: 87.50%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.8007 | Accuracy: 73.19%\n",
      "  Current Best Acc: 73.19%\n",
      "\n",
      "========================================\n",
      "=== Fold 2 Completed ===\n",
      "Best Validation Accuracy: 73.19%\n",
      "\n",
      "========================================\n",
      "=== Fold 3/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/899 | Loss: 1.9352 | CLoss: 1.0866 | FLoss: 1.6972 | LR: 3.00e-04\n",
      "  Batch 020/899 | Loss: 1.7955 | CLoss: 1.1489 | FLoss: 1.2933 | LR: 3.00e-04\n",
      "  Batch 030/899 | Loss: 1.5172 | CLoss: 1.0266 | FLoss: 0.9812 | LR: 3.00e-04\n",
      "  Batch 040/899 | Loss: 1.3212 | CLoss: 0.9136 | FLoss: 0.8152 | LR: 3.00e-04\n",
      "  Batch 050/899 | Loss: 1.1909 | CLoss: 0.8006 | FLoss: 0.7808 | LR: 3.00e-04\n",
      "  Batch 060/899 | Loss: 1.0375 | CLoss: 0.6210 | FLoss: 0.8330 | LR: 3.00e-04\n",
      "  Batch 070/899 | Loss: 1.4968 | CLoss: 1.0658 | FLoss: 0.8620 | LR: 3.00e-04\n",
      "  Batch 080/899 | Loss: 1.3408 | CLoss: 1.0299 | FLoss: 0.6219 | LR: 3.00e-04\n",
      "  Batch 090/899 | Loss: 1.0746 | CLoss: 0.7939 | FLoss: 0.5614 | LR: 3.00e-04\n",
      "  Batch 100/899 | Loss: 1.0633 | CLoss: 0.7875 | FLoss: 0.5516 | LR: 3.00e-04\n",
      "  Batch 110/899 | Loss: 1.3873 | CLoss: 0.9950 | FLoss: 0.7846 | LR: 3.00e-04\n",
      "  Batch 120/899 | Loss: 1.5283 | CLoss: 1.1406 | FLoss: 0.7754 | LR: 3.00e-04\n",
      "  Batch 130/899 | Loss: 0.9522 | CLoss: 0.7129 | FLoss: 0.4787 | LR: 3.00e-04\n",
      "  Batch 140/899 | Loss: 1.1544 | CLoss: 0.8419 | FLoss: 0.6252 | LR: 3.00e-04\n",
      "  Batch 150/899 | Loss: 1.4562 | CLoss: 1.0880 | FLoss: 0.7363 | LR: 3.00e-04\n",
      "  Batch 160/899 | Loss: 1.1463 | CLoss: 0.8358 | FLoss: 0.6208 | LR: 3.00e-04\n",
      "  Batch 170/899 | Loss: 1.1305 | CLoss: 0.7811 | FLoss: 0.6986 | LR: 3.00e-04\n",
      "  Batch 180/899 | Loss: 1.1872 | CLoss: 0.8333 | FLoss: 0.7079 | LR: 3.00e-04\n",
      "  Batch 190/899 | Loss: 1.3564 | CLoss: 1.0466 | FLoss: 0.6196 | LR: 3.00e-04\n",
      "  Batch 200/899 | Loss: 1.0040 | CLoss: 0.7381 | FLoss: 0.5318 | LR: 3.00e-04\n",
      "  Batch 210/899 | Loss: 1.3255 | CLoss: 1.0663 | FLoss: 0.5184 | LR: 3.00e-04\n",
      "  Batch 220/899 | Loss: 1.3132 | CLoss: 1.0236 | FLoss: 0.5793 | LR: 3.00e-04\n",
      "  Batch 230/899 | Loss: 1.1481 | CLoss: 0.8611 | FLoss: 0.5740 | LR: 3.00e-04\n",
      "  Batch 240/899 | Loss: 0.7701 | CLoss: 0.5321 | FLoss: 0.4761 | LR: 3.00e-04\n",
      "  Batch 250/899 | Loss: 0.9007 | CLoss: 0.6274 | FLoss: 0.5468 | LR: 3.00e-04\n",
      "  Batch 260/899 | Loss: 1.8133 | CLoss: 1.3889 | FLoss: 0.8487 | LR: 3.00e-04\n",
      "  Batch 270/899 | Loss: 0.9430 | CLoss: 0.7072 | FLoss: 0.4717 | LR: 3.00e-04\n",
      "  Batch 280/899 | Loss: 1.2230 | CLoss: 0.8945 | FLoss: 0.6571 | LR: 3.00e-04\n",
      "  Batch 290/899 | Loss: 1.4152 | CLoss: 1.0402 | FLoss: 0.7501 | LR: 3.00e-04\n",
      "  Batch 300/899 | Loss: 0.9792 | CLoss: 0.7449 | FLoss: 0.4685 | LR: 3.00e-04\n",
      "  Batch 310/899 | Loss: 1.0747 | CLoss: 0.8398 | FLoss: 0.4699 | LR: 3.00e-04\n",
      "  Batch 320/899 | Loss: 0.9329 | CLoss: 0.7340 | FLoss: 0.3978 | LR: 3.00e-04\n",
      "  Batch 330/899 | Loss: 1.4259 | CLoss: 1.1276 | FLoss: 0.5966 | LR: 3.00e-04\n",
      "  Batch 340/899 | Loss: 1.4310 | CLoss: 1.0838 | FLoss: 0.6943 | LR: 3.00e-04\n",
      "  Batch 350/899 | Loss: 1.5594 | CLoss: 1.1389 | FLoss: 0.8411 | LR: 3.00e-04\n",
      "  Batch 360/899 | Loss: 1.1428 | CLoss: 0.7961 | FLoss: 0.6934 | LR: 3.00e-04\n",
      "  Batch 370/899 | Loss: 1.6692 | CLoss: 1.3536 | FLoss: 0.6313 | LR: 3.00e-04\n",
      "  Batch 380/899 | Loss: 0.9384 | CLoss: 0.6837 | FLoss: 0.5093 | LR: 3.00e-04\n",
      "  Batch 390/899 | Loss: 1.0695 | CLoss: 0.8194 | FLoss: 0.5001 | LR: 3.00e-04\n",
      "  Batch 400/899 | Loss: 1.4288 | CLoss: 1.0773 | FLoss: 0.7030 | LR: 3.00e-04\n",
      "  Batch 410/899 | Loss: 1.1538 | CLoss: 0.8872 | FLoss: 0.5332 | LR: 3.00e-04\n",
      "  Batch 420/899 | Loss: 1.0137 | CLoss: 0.8198 | FLoss: 0.3878 | LR: 3.00e-04\n",
      "  Batch 430/899 | Loss: 1.2642 | CLoss: 1.0112 | FLoss: 0.5059 | LR: 3.00e-04\n",
      "  Batch 440/899 | Loss: 1.0473 | CLoss: 0.8034 | FLoss: 0.4880 | LR: 3.00e-04\n",
      "  Batch 450/899 | Loss: 1.4858 | CLoss: 1.0852 | FLoss: 0.8012 | LR: 3.00e-04\n",
      "  Batch 460/899 | Loss: 1.0670 | CLoss: 0.8652 | FLoss: 0.4037 | LR: 3.00e-04\n",
      "  Batch 470/899 | Loss: 1.2481 | CLoss: 0.9484 | FLoss: 0.5994 | LR: 3.00e-04\n",
      "  Batch 480/899 | Loss: 1.4064 | CLoss: 1.0898 | FLoss: 0.6331 | LR: 3.00e-04\n",
      "  Batch 490/899 | Loss: 0.8214 | CLoss: 0.5707 | FLoss: 0.5014 | LR: 3.00e-04\n",
      "  Batch 500/899 | Loss: 1.6456 | CLoss: 1.2294 | FLoss: 0.8326 | LR: 3.00e-04\n",
      "  Batch 510/899 | Loss: 1.1903 | CLoss: 0.9586 | FLoss: 0.4634 | LR: 3.00e-04\n",
      "  Batch 520/899 | Loss: 1.0001 | CLoss: 0.7410 | FLoss: 0.5181 | LR: 3.00e-04\n",
      "  Batch 530/899 | Loss: 1.5406 | CLoss: 1.2938 | FLoss: 0.4935 | LR: 3.00e-04\n",
      "  Batch 540/899 | Loss: 0.9439 | CLoss: 0.7430 | FLoss: 0.4019 | LR: 3.00e-04\n",
      "  Batch 550/899 | Loss: 1.7905 | CLoss: 1.4820 | FLoss: 0.6170 | LR: 3.00e-04\n",
      "  Batch 560/899 | Loss: 0.8595 | CLoss: 0.6140 | FLoss: 0.4910 | LR: 3.00e-04\n",
      "  Batch 570/899 | Loss: 1.1197 | CLoss: 0.7619 | FLoss: 0.7156 | LR: 3.00e-04\n",
      "  Batch 580/899 | Loss: 1.9026 | CLoss: 1.4712 | FLoss: 0.8628 | LR: 3.00e-04\n",
      "  Batch 590/899 | Loss: 1.2984 | CLoss: 0.9457 | FLoss: 0.7054 | LR: 3.00e-04\n",
      "  Batch 600/899 | Loss: 1.4255 | CLoss: 1.0131 | FLoss: 0.8247 | LR: 3.00e-04\n",
      "  Batch 610/899 | Loss: 0.9536 | CLoss: 0.7634 | FLoss: 0.3805 | LR: 3.00e-04\n",
      "  Batch 620/899 | Loss: 0.9914 | CLoss: 0.8108 | FLoss: 0.3612 | LR: 3.00e-04\n",
      "  Batch 630/899 | Loss: 0.8669 | CLoss: 0.5669 | FLoss: 0.6001 | LR: 3.00e-04\n",
      "  Batch 640/899 | Loss: 1.2131 | CLoss: 0.9333 | FLoss: 0.5595 | LR: 3.00e-04\n",
      "  Batch 650/899 | Loss: 1.3507 | CLoss: 1.1431 | FLoss: 0.4153 | LR: 3.00e-04\n",
      "  Batch 660/899 | Loss: 1.2800 | CLoss: 0.9782 | FLoss: 0.6037 | LR: 3.00e-04\n",
      "  Batch 670/899 | Loss: 0.8733 | CLoss: 0.6418 | FLoss: 0.4630 | LR: 3.00e-04\n",
      "  Batch 680/899 | Loss: 1.0059 | CLoss: 0.7295 | FLoss: 0.5528 | LR: 3.00e-04\n",
      "  Batch 690/899 | Loss: 1.0830 | CLoss: 0.8095 | FLoss: 0.5470 | LR: 3.00e-04\n",
      "  Batch 700/899 | Loss: 1.7379 | CLoss: 1.3623 | FLoss: 0.7513 | LR: 3.00e-04\n",
      "  Batch 710/899 | Loss: 0.9527 | CLoss: 0.6676 | FLoss: 0.5702 | LR: 3.00e-04\n",
      "  Batch 720/899 | Loss: 0.9837 | CLoss: 0.7292 | FLoss: 0.5089 | LR: 3.00e-04\n",
      "  Batch 730/899 | Loss: 1.1011 | CLoss: 0.7330 | FLoss: 0.7362 | LR: 3.00e-04\n",
      "  Batch 740/899 | Loss: 1.2719 | CLoss: 0.9540 | FLoss: 0.6358 | LR: 3.00e-04\n",
      "  Batch 750/899 | Loss: 1.6806 | CLoss: 1.3155 | FLoss: 0.7302 | LR: 3.00e-04\n",
      "  Batch 760/899 | Loss: 1.0667 | CLoss: 0.8906 | FLoss: 0.3522 | LR: 3.00e-04\n",
      "  Batch 770/899 | Loss: 1.5649 | CLoss: 1.2371 | FLoss: 0.6558 | LR: 3.00e-04\n",
      "  Batch 780/899 | Loss: 0.8058 | CLoss: 0.6417 | FLoss: 0.3281 | LR: 3.00e-04\n",
      "  Batch 790/899 | Loss: 1.0655 | CLoss: 0.8464 | FLoss: 0.4381 | LR: 3.00e-04\n",
      "  Batch 800/899 | Loss: 1.2254 | CLoss: 0.9450 | FLoss: 0.5607 | LR: 3.00e-04\n",
      "  Batch 810/899 | Loss: 1.3057 | CLoss: 1.0414 | FLoss: 0.5285 | LR: 3.00e-04\n",
      "  Batch 820/899 | Loss: 1.0092 | CLoss: 0.8017 | FLoss: 0.4151 | LR: 3.00e-04\n",
      "  Batch 830/899 | Loss: 1.3458 | CLoss: 1.0670 | FLoss: 0.5575 | LR: 3.00e-04\n",
      "  Batch 840/899 | Loss: 0.9903 | CLoss: 0.7305 | FLoss: 0.5198 | LR: 3.00e-04\n",
      "  Batch 850/899 | Loss: 1.2176 | CLoss: 1.0172 | FLoss: 0.4009 | LR: 3.00e-04\n",
      "  Batch 860/899 | Loss: 1.5638 | CLoss: 1.3426 | FLoss: 0.4424 | LR: 3.00e-04\n",
      "  Batch 870/899 | Loss: 1.3075 | CLoss: 1.0562 | FLoss: 0.5027 | LR: 3.00e-04\n",
      "  Batch 880/899 | Loss: 1.0100 | CLoss: 0.7724 | FLoss: 0.4753 | LR: 3.00e-04\n",
      "  Batch 890/899 | Loss: 0.8540 | CLoss: 0.6199 | FLoss: 0.4683 | LR: 3.00e-04\n",
      "  Batch 899/899 | Loss: 0.5975 | CLoss: 0.2677 | FLoss: 0.6595 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 1.2618\n",
      "  Last Batch Loss: 0.5975\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.5608 | Batch Acc: 79.31%\n",
      "    Val Batch 010/101 | Loss: 0.6360 | Batch Acc: 79.31%\n",
      "    Val Batch 015/101 | Loss: 0.5443 | Batch Acc: 82.76%\n",
      "    Val Batch 020/101 | Loss: 0.5182 | Batch Acc: 96.55%\n",
      "    Val Batch 025/101 | Loss: 0.5132 | Batch Acc: 81.03%\n",
      "    Val Batch 030/101 | Loss: 0.8593 | Batch Acc: 63.79%\n",
      "    Val Batch 035/101 | Loss: 0.8085 | Batch Acc: 74.14%\n",
      "    Val Batch 040/101 | Loss: 1.2819 | Batch Acc: 37.93%\n",
      "    Val Batch 045/101 | Loss: 0.9214 | Batch Acc: 41.38%\n",
      "    Val Batch 050/101 | Loss: 0.8767 | Batch Acc: 67.24%\n",
      "    Val Batch 055/101 | Loss: 0.5818 | Batch Acc: 72.41%\n",
      "    Val Batch 060/101 | Loss: 0.8565 | Batch Acc: 58.62%\n",
      "    Val Batch 065/101 | Loss: 0.3274 | Batch Acc: 89.66%\n",
      "    Val Batch 070/101 | Loss: 0.3114 | Batch Acc: 91.38%\n",
      "    Val Batch 075/101 | Loss: 0.2489 | Batch Acc: 89.66%\n",
      "    Val Batch 080/101 | Loss: 0.8201 | Batch Acc: 70.69%\n",
      "    Val Batch 085/101 | Loss: 0.6410 | Batch Acc: 82.76%\n",
      "    Val Batch 090/101 | Loss: 0.3352 | Batch Acc: 89.66%\n",
      "    Val Batch 095/101 | Loss: 0.1860 | Batch Acc: 96.55%\n",
      "    Val Batch 100/101 | Loss: 0.4465 | Batch Acc: 87.93%\n",
      "    Val Batch 101/101 | Loss: 0.0282 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.6029 | Accuracy: 78.28%\n",
      "  Current Best Acc: 78.28%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/899 | Loss: 1.0112 | CLoss: 0.8216 | FLoss: 0.3791 | LR: 2.71e-04\n",
      "  Batch 020/899 | Loss: 0.8604 | CLoss: 0.6675 | FLoss: 0.3857 | LR: 2.71e-04\n",
      "  Batch 030/899 | Loss: 1.1405 | CLoss: 0.8701 | FLoss: 0.5408 | LR: 2.71e-04\n",
      "  Batch 040/899 | Loss: 0.7233 | CLoss: 0.4455 | FLoss: 0.5555 | LR: 2.71e-04\n",
      "  Batch 050/899 | Loss: 1.5138 | CLoss: 1.1578 | FLoss: 0.7119 | LR: 2.71e-04\n",
      "  Batch 060/899 | Loss: 1.1399 | CLoss: 0.8354 | FLoss: 0.6090 | LR: 2.71e-04\n",
      "  Batch 070/899 | Loss: 0.8237 | CLoss: 0.5480 | FLoss: 0.5514 | LR: 2.71e-04\n",
      "  Batch 080/899 | Loss: 1.3353 | CLoss: 1.0620 | FLoss: 0.5467 | LR: 2.71e-04\n",
      "  Batch 090/899 | Loss: 1.0921 | CLoss: 0.7619 | FLoss: 0.6603 | LR: 2.71e-04\n",
      "  Batch 100/899 | Loss: 1.0171 | CLoss: 0.7511 | FLoss: 0.5319 | LR: 2.71e-04\n",
      "  Batch 110/899 | Loss: 1.2908 | CLoss: 1.0399 | FLoss: 0.5019 | LR: 2.71e-04\n",
      "  Batch 120/899 | Loss: 1.1617 | CLoss: 0.9344 | FLoss: 0.4545 | LR: 2.71e-04\n",
      "  Batch 130/899 | Loss: 1.2205 | CLoss: 0.9622 | FLoss: 0.5166 | LR: 2.71e-04\n",
      "  Batch 140/899 | Loss: 1.0328 | CLoss: 0.7997 | FLoss: 0.4662 | LR: 2.71e-04\n",
      "  Batch 150/899 | Loss: 0.9588 | CLoss: 0.6015 | FLoss: 0.7145 | LR: 2.71e-04\n",
      "  Batch 160/899 | Loss: 0.7990 | CLoss: 0.6015 | FLoss: 0.3950 | LR: 2.71e-04\n",
      "  Batch 170/899 | Loss: 1.1538 | CLoss: 0.8754 | FLoss: 0.5568 | LR: 2.71e-04\n",
      "  Batch 180/899 | Loss: 0.4989 | CLoss: 0.3583 | FLoss: 0.2811 | LR: 2.71e-04\n",
      "  Batch 190/899 | Loss: 1.0111 | CLoss: 0.7301 | FLoss: 0.5621 | LR: 2.71e-04\n",
      "  Batch 200/899 | Loss: 1.4161 | CLoss: 1.0904 | FLoss: 0.6513 | LR: 2.71e-04\n",
      "  Batch 210/899 | Loss: 0.7513 | CLoss: 0.4997 | FLoss: 0.5032 | LR: 2.71e-04\n",
      "  Batch 220/899 | Loss: 0.8578 | CLoss: 0.5992 | FLoss: 0.5174 | LR: 2.71e-04\n",
      "  Batch 230/899 | Loss: 1.0063 | CLoss: 0.7301 | FLoss: 0.5522 | LR: 2.71e-04\n",
      "  Batch 240/899 | Loss: 0.9938 | CLoss: 0.7365 | FLoss: 0.5146 | LR: 2.71e-04\n",
      "  Batch 250/899 | Loss: 1.1080 | CLoss: 0.9347 | FLoss: 0.3466 | LR: 2.71e-04\n",
      "  Batch 260/899 | Loss: 1.1369 | CLoss: 0.8197 | FLoss: 0.6344 | LR: 2.71e-04\n",
      "  Batch 270/899 | Loss: 0.8489 | CLoss: 0.6597 | FLoss: 0.3783 | LR: 2.71e-04\n",
      "  Batch 280/899 | Loss: 1.3608 | CLoss: 1.1757 | FLoss: 0.3702 | LR: 2.71e-04\n",
      "  Batch 290/899 | Loss: 1.3527 | CLoss: 1.0030 | FLoss: 0.6994 | LR: 2.71e-04\n",
      "  Batch 300/899 | Loss: 1.0824 | CLoss: 0.7212 | FLoss: 0.7223 | LR: 2.71e-04\n",
      "  Batch 310/899 | Loss: 0.8683 | CLoss: 0.7155 | FLoss: 0.3056 | LR: 2.71e-04\n",
      "  Batch 320/899 | Loss: 0.9028 | CLoss: 0.6504 | FLoss: 0.5049 | LR: 2.71e-04\n",
      "  Batch 330/899 | Loss: 1.4056 | CLoss: 1.1181 | FLoss: 0.5751 | LR: 2.71e-04\n",
      "  Batch 340/899 | Loss: 0.8347 | CLoss: 0.6405 | FLoss: 0.3885 | LR: 2.71e-04\n",
      "  Batch 350/899 | Loss: 1.0145 | CLoss: 0.7747 | FLoss: 0.4798 | LR: 2.71e-04\n",
      "  Batch 360/899 | Loss: 1.4350 | CLoss: 1.1325 | FLoss: 0.6050 | LR: 2.71e-04\n",
      "  Batch 370/899 | Loss: 1.3534 | CLoss: 1.1384 | FLoss: 0.4300 | LR: 2.71e-04\n",
      "  Batch 380/899 | Loss: 1.1811 | CLoss: 0.9904 | FLoss: 0.3813 | LR: 2.71e-04\n",
      "  Batch 390/899 | Loss: 0.9466 | CLoss: 0.6810 | FLoss: 0.5312 | LR: 2.71e-04\n",
      "  Batch 400/899 | Loss: 1.1889 | CLoss: 0.8954 | FLoss: 0.5869 | LR: 2.71e-04\n",
      "  Batch 410/899 | Loss: 0.8329 | CLoss: 0.6743 | FLoss: 0.3172 | LR: 2.71e-04\n",
      "  Batch 420/899 | Loss: 1.7779 | CLoss: 1.3610 | FLoss: 0.8339 | LR: 2.71e-04\n",
      "  Batch 430/899 | Loss: 0.8634 | CLoss: 0.6614 | FLoss: 0.4040 | LR: 2.71e-04\n",
      "  Batch 440/899 | Loss: 1.1139 | CLoss: 0.8915 | FLoss: 0.4449 | LR: 2.71e-04\n",
      "  Batch 450/899 | Loss: 1.5302 | CLoss: 1.1938 | FLoss: 0.6728 | LR: 2.71e-04\n",
      "  Batch 460/899 | Loss: 1.3897 | CLoss: 1.0223 | FLoss: 0.7347 | LR: 2.71e-04\n",
      "  Batch 470/899 | Loss: 1.0723 | CLoss: 0.7934 | FLoss: 0.5578 | LR: 2.71e-04\n",
      "  Batch 480/899 | Loss: 1.3539 | CLoss: 1.0388 | FLoss: 0.6303 | LR: 2.71e-04\n",
      "  Batch 490/899 | Loss: 0.9350 | CLoss: 0.6822 | FLoss: 0.5056 | LR: 2.71e-04\n",
      "  Batch 500/899 | Loss: 1.5988 | CLoss: 1.1081 | FLoss: 0.9814 | LR: 2.71e-04\n",
      "  Batch 510/899 | Loss: 0.9788 | CLoss: 0.7570 | FLoss: 0.4437 | LR: 2.71e-04\n",
      "  Batch 520/899 | Loss: 0.9065 | CLoss: 0.6462 | FLoss: 0.5205 | LR: 2.71e-04\n",
      "  Batch 530/899 | Loss: 1.2897 | CLoss: 1.0533 | FLoss: 0.4728 | LR: 2.71e-04\n",
      "  Batch 540/899 | Loss: 0.9336 | CLoss: 0.7502 | FLoss: 0.3668 | LR: 2.71e-04\n",
      "  Batch 550/899 | Loss: 1.5565 | CLoss: 1.2319 | FLoss: 0.6492 | LR: 2.71e-04\n",
      "  Batch 560/899 | Loss: 0.8161 | CLoss: 0.5669 | FLoss: 0.4985 | LR: 2.71e-04\n",
      "  Batch 570/899 | Loss: 1.1341 | CLoss: 0.8669 | FLoss: 0.5345 | LR: 2.71e-04\n",
      "  Batch 580/899 | Loss: 1.2186 | CLoss: 0.8857 | FLoss: 0.6658 | LR: 2.71e-04\n",
      "  Batch 590/899 | Loss: 1.4991 | CLoss: 1.1418 | FLoss: 0.7145 | LR: 2.71e-04\n",
      "  Batch 600/899 | Loss: 1.2488 | CLoss: 0.9412 | FLoss: 0.6153 | LR: 2.71e-04\n",
      "  Batch 610/899 | Loss: 0.8967 | CLoss: 0.6967 | FLoss: 0.3999 | LR: 2.71e-04\n",
      "  Batch 620/899 | Loss: 0.8376 | CLoss: 0.6844 | FLoss: 0.3065 | LR: 2.71e-04\n",
      "  Batch 630/899 | Loss: 0.6922 | CLoss: 0.4946 | FLoss: 0.3953 | LR: 2.71e-04\n",
      "  Batch 640/899 | Loss: 1.3215 | CLoss: 1.0402 | FLoss: 0.5627 | LR: 2.71e-04\n",
      "  Batch 650/899 | Loss: 1.0119 | CLoss: 0.7374 | FLoss: 0.5490 | LR: 2.71e-04\n",
      "  Batch 660/899 | Loss: 0.8675 | CLoss: 0.5936 | FLoss: 0.5477 | LR: 2.71e-04\n",
      "  Batch 670/899 | Loss: 1.1570 | CLoss: 0.8807 | FLoss: 0.5526 | LR: 2.71e-04\n",
      "  Batch 680/899 | Loss: 0.9503 | CLoss: 0.7243 | FLoss: 0.4520 | LR: 2.71e-04\n",
      "  Batch 690/899 | Loss: 1.3218 | CLoss: 1.0615 | FLoss: 0.5206 | LR: 2.71e-04\n",
      "  Batch 700/899 | Loss: 1.4804 | CLoss: 1.1351 | FLoss: 0.6906 | LR: 2.71e-04\n",
      "  Batch 710/899 | Loss: 1.2985 | CLoss: 1.0358 | FLoss: 0.5253 | LR: 2.71e-04\n",
      "  Batch 720/899 | Loss: 1.3741 | CLoss: 0.9854 | FLoss: 0.7775 | LR: 2.71e-04\n",
      "  Batch 730/899 | Loss: 1.0071 | CLoss: 0.7420 | FLoss: 0.5302 | LR: 2.71e-04\n",
      "  Batch 740/899 | Loss: 0.9274 | CLoss: 0.6950 | FLoss: 0.4647 | LR: 2.71e-04\n",
      "  Batch 750/899 | Loss: 0.7877 | CLoss: 0.6054 | FLoss: 0.3647 | LR: 2.71e-04\n",
      "  Batch 760/899 | Loss: 1.1108 | CLoss: 0.7495 | FLoss: 0.7225 | LR: 2.71e-04\n",
      "  Batch 770/899 | Loss: 0.9509 | CLoss: 0.7205 | FLoss: 0.4608 | LR: 2.71e-04\n",
      "  Batch 780/899 | Loss: 0.9181 | CLoss: 0.6265 | FLoss: 0.5831 | LR: 2.71e-04\n",
      "  Batch 790/899 | Loss: 0.8755 | CLoss: 0.6438 | FLoss: 0.4634 | LR: 2.71e-04\n",
      "  Batch 800/899 | Loss: 0.7154 | CLoss: 0.5128 | FLoss: 0.4051 | LR: 2.71e-04\n",
      "  Batch 810/899 | Loss: 0.8050 | CLoss: 0.6007 | FLoss: 0.4087 | LR: 2.71e-04\n",
      "  Batch 820/899 | Loss: 0.7204 | CLoss: 0.5507 | FLoss: 0.3394 | LR: 2.71e-04\n",
      "  Batch 830/899 | Loss: 0.8655 | CLoss: 0.7105 | FLoss: 0.3101 | LR: 2.71e-04\n",
      "  Batch 840/899 | Loss: 0.8708 | CLoss: 0.6268 | FLoss: 0.4880 | LR: 2.71e-04\n",
      "  Batch 850/899 | Loss: 1.2376 | CLoss: 0.9224 | FLoss: 0.6303 | LR: 2.71e-04\n",
      "  Batch 860/899 | Loss: 1.3216 | CLoss: 1.0555 | FLoss: 0.5323 | LR: 2.71e-04\n",
      "  Batch 870/899 | Loss: 1.1006 | CLoss: 0.8848 | FLoss: 0.4315 | LR: 2.71e-04\n",
      "  Batch 880/899 | Loss: 1.0131 | CLoss: 0.7284 | FLoss: 0.5695 | LR: 2.71e-04\n",
      "  Batch 890/899 | Loss: 0.9621 | CLoss: 0.7356 | FLoss: 0.4530 | LR: 2.71e-04\n",
      "  Batch 899/899 | Loss: 0.7443 | CLoss: 0.3165 | FLoss: 0.8558 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 1.0718\n",
      "  Last Batch Loss: 0.7443\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.7869 | Batch Acc: 72.41%\n",
      "    Val Batch 010/101 | Loss: 0.8616 | Batch Acc: 68.97%\n",
      "    Val Batch 015/101 | Loss: 0.7982 | Batch Acc: 74.14%\n",
      "    Val Batch 020/101 | Loss: 0.7424 | Batch Acc: 50.00%\n",
      "    Val Batch 025/101 | Loss: 2.3821 | Batch Acc: 27.59%\n",
      "    Val Batch 030/101 | Loss: 1.4016 | Batch Acc: 53.45%\n",
      "    Val Batch 035/101 | Loss: 1.2078 | Batch Acc: 65.52%\n",
      "    Val Batch 040/101 | Loss: 1.0978 | Batch Acc: 72.41%\n",
      "    Val Batch 045/101 | Loss: 0.5812 | Batch Acc: 96.55%\n",
      "    Val Batch 050/101 | Loss: 0.8178 | Batch Acc: 74.14%\n",
      "    Val Batch 055/101 | Loss: 1.5379 | Batch Acc: 50.00%\n",
      "    Val Batch 060/101 | Loss: 2.3445 | Batch Acc: 22.41%\n",
      "    Val Batch 065/101 | Loss: 0.3325 | Batch Acc: 87.93%\n",
      "    Val Batch 070/101 | Loss: 0.1893 | Batch Acc: 93.10%\n",
      "    Val Batch 075/101 | Loss: 0.2958 | Batch Acc: 91.38%\n",
      "    Val Batch 080/101 | Loss: 0.3290 | Batch Acc: 87.93%\n",
      "    Val Batch 085/101 | Loss: 0.3730 | Batch Acc: 89.66%\n",
      "    Val Batch 090/101 | Loss: 0.2673 | Batch Acc: 89.66%\n",
      "    Val Batch 095/101 | Loss: 0.2826 | Batch Acc: 91.38%\n",
      "    Val Batch 100/101 | Loss: 0.2753 | Batch Acc: 91.38%\n",
      "    Val Batch 101/101 | Loss: 0.1758 | Batch Acc: 90.00%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.7613 | Accuracy: 74.23%\n",
      "  Current Best Acc: 78.28%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/899 | Loss: 0.9053 | CLoss: 0.7256 | FLoss: 0.3592 | LR: 1.96e-04\n",
      "  Batch 020/899 | Loss: 1.1358 | CLoss: 0.9027 | FLoss: 0.4663 | LR: 1.96e-04\n",
      "  Batch 030/899 | Loss: 1.0152 | CLoss: 0.7778 | FLoss: 0.4749 | LR: 1.96e-04\n",
      "  Batch 040/899 | Loss: 0.9605 | CLoss: 0.6686 | FLoss: 0.5838 | LR: 1.96e-04\n",
      "  Batch 050/899 | Loss: 1.0454 | CLoss: 0.8479 | FLoss: 0.3948 | LR: 1.96e-04\n",
      "  Batch 060/899 | Loss: 1.0168 | CLoss: 0.7404 | FLoss: 0.5529 | LR: 1.96e-04\n",
      "  Batch 070/899 | Loss: 1.2058 | CLoss: 0.9461 | FLoss: 0.5195 | LR: 1.96e-04\n",
      "  Batch 080/899 | Loss: 0.6598 | CLoss: 0.4280 | FLoss: 0.4635 | LR: 1.96e-04\n",
      "  Batch 090/899 | Loss: 0.8714 | CLoss: 0.5752 | FLoss: 0.5925 | LR: 1.96e-04\n",
      "  Batch 100/899 | Loss: 1.0596 | CLoss: 0.8023 | FLoss: 0.5146 | LR: 1.96e-04\n",
      "  Batch 110/899 | Loss: 1.3461 | CLoss: 1.0709 | FLoss: 0.5505 | LR: 1.96e-04\n",
      "  Batch 120/899 | Loss: 1.2223 | CLoss: 0.9178 | FLoss: 0.6089 | LR: 1.96e-04\n",
      "  Batch 130/899 | Loss: 0.7781 | CLoss: 0.5837 | FLoss: 0.3888 | LR: 1.96e-04\n",
      "  Batch 140/899 | Loss: 0.6627 | CLoss: 0.5077 | FLoss: 0.3100 | LR: 1.96e-04\n",
      "  Batch 150/899 | Loss: 0.7773 | CLoss: 0.6061 | FLoss: 0.3426 | LR: 1.96e-04\n",
      "  Batch 160/899 | Loss: 0.6572 | CLoss: 0.4117 | FLoss: 0.4911 | LR: 1.96e-04\n",
      "  Batch 170/899 | Loss: 1.5015 | CLoss: 1.1320 | FLoss: 0.7389 | LR: 1.96e-04\n",
      "  Batch 180/899 | Loss: 0.5234 | CLoss: 0.3426 | FLoss: 0.3614 | LR: 1.96e-04\n",
      "  Batch 190/899 | Loss: 1.1418 | CLoss: 0.7577 | FLoss: 0.7682 | LR: 1.96e-04\n",
      "  Batch 200/899 | Loss: 0.5874 | CLoss: 0.3684 | FLoss: 0.4381 | LR: 1.96e-04\n",
      "  Batch 210/899 | Loss: 1.0954 | CLoss: 0.8344 | FLoss: 0.5220 | LR: 1.96e-04\n",
      "  Batch 220/899 | Loss: 0.9658 | CLoss: 0.7274 | FLoss: 0.4768 | LR: 1.96e-04\n",
      "  Batch 230/899 | Loss: 1.0202 | CLoss: 0.7986 | FLoss: 0.4431 | LR: 1.96e-04\n",
      "  Batch 240/899 | Loss: 1.1360 | CLoss: 0.7822 | FLoss: 0.7076 | LR: 1.96e-04\n",
      "  Batch 250/899 | Loss: 0.8185 | CLoss: 0.6096 | FLoss: 0.4178 | LR: 1.96e-04\n",
      "  Batch 260/899 | Loss: 0.9958 | CLoss: 0.8264 | FLoss: 0.3389 | LR: 1.96e-04\n",
      "  Batch 270/899 | Loss: 1.1235 | CLoss: 0.8415 | FLoss: 0.5639 | LR: 1.96e-04\n",
      "  Batch 280/899 | Loss: 1.3995 | CLoss: 1.1066 | FLoss: 0.5856 | LR: 1.96e-04\n",
      "  Batch 290/899 | Loss: 0.9725 | CLoss: 0.7972 | FLoss: 0.3505 | LR: 1.96e-04\n",
      "  Batch 300/899 | Loss: 0.9406 | CLoss: 0.7345 | FLoss: 0.4122 | LR: 1.96e-04\n",
      "  Batch 310/899 | Loss: 0.5808 | CLoss: 0.4514 | FLoss: 0.2589 | LR: 1.96e-04\n",
      "  Batch 320/899 | Loss: 1.1968 | CLoss: 0.8281 | FLoss: 0.7375 | LR: 1.96e-04\n",
      "  Batch 330/899 | Loss: 0.9052 | CLoss: 0.7189 | FLoss: 0.3727 | LR: 1.96e-04\n",
      "  Batch 340/899 | Loss: 1.3379 | CLoss: 0.9996 | FLoss: 0.6767 | LR: 1.96e-04\n",
      "  Batch 350/899 | Loss: 1.0151 | CLoss: 0.8316 | FLoss: 0.3669 | LR: 1.96e-04\n",
      "  Batch 360/899 | Loss: 1.2895 | CLoss: 0.9378 | FLoss: 0.7033 | LR: 1.96e-04\n",
      "  Batch 370/899 | Loss: 1.1348 | CLoss: 0.9637 | FLoss: 0.3421 | LR: 1.96e-04\n",
      "  Batch 380/899 | Loss: 1.0982 | CLoss: 0.8515 | FLoss: 0.4933 | LR: 1.96e-04\n",
      "  Batch 390/899 | Loss: 0.7124 | CLoss: 0.4591 | FLoss: 0.5067 | LR: 1.96e-04\n",
      "  Batch 400/899 | Loss: 1.3282 | CLoss: 0.9662 | FLoss: 0.7240 | LR: 1.96e-04\n",
      "  Batch 410/899 | Loss: 0.7372 | CLoss: 0.5251 | FLoss: 0.4242 | LR: 1.96e-04\n",
      "  Batch 420/899 | Loss: 1.0852 | CLoss: 0.8577 | FLoss: 0.4549 | LR: 1.96e-04\n",
      "  Batch 430/899 | Loss: 1.2266 | CLoss: 0.8594 | FLoss: 0.7344 | LR: 1.96e-04\n",
      "  Batch 440/899 | Loss: 0.7296 | CLoss: 0.5470 | FLoss: 0.3652 | LR: 1.96e-04\n",
      "  Batch 450/899 | Loss: 0.8025 | CLoss: 0.6469 | FLoss: 0.3113 | LR: 1.96e-04\n",
      "  Batch 460/899 | Loss: 0.7399 | CLoss: 0.5122 | FLoss: 0.4554 | LR: 1.96e-04\n",
      "  Batch 470/899 | Loss: 0.7103 | CLoss: 0.5393 | FLoss: 0.3420 | LR: 1.96e-04\n",
      "  Batch 480/899 | Loss: 1.0724 | CLoss: 0.8346 | FLoss: 0.4756 | LR: 1.96e-04\n",
      "  Batch 490/899 | Loss: 1.0230 | CLoss: 0.7378 | FLoss: 0.5705 | LR: 1.96e-04\n",
      "  Batch 500/899 | Loss: 0.7040 | CLoss: 0.5595 | FLoss: 0.2890 | LR: 1.96e-04\n",
      "  Batch 510/899 | Loss: 1.2707 | CLoss: 1.0316 | FLoss: 0.4781 | LR: 1.96e-04\n",
      "  Batch 520/899 | Loss: 1.1994 | CLoss: 0.9424 | FLoss: 0.5139 | LR: 1.96e-04\n",
      "  Batch 530/899 | Loss: 0.8482 | CLoss: 0.6310 | FLoss: 0.4345 | LR: 1.96e-04\n",
      "  Batch 540/899 | Loss: 0.5603 | CLoss: 0.4122 | FLoss: 0.2962 | LR: 1.96e-04\n",
      "  Batch 550/899 | Loss: 0.6993 | CLoss: 0.5189 | FLoss: 0.3609 | LR: 1.96e-04\n",
      "  Batch 560/899 | Loss: 1.2128 | CLoss: 0.9820 | FLoss: 0.4615 | LR: 1.96e-04\n",
      "  Batch 570/899 | Loss: 1.2263 | CLoss: 1.0085 | FLoss: 0.4356 | LR: 1.96e-04\n",
      "  Batch 580/899 | Loss: 0.9584 | CLoss: 0.6435 | FLoss: 0.6298 | LR: 1.96e-04\n",
      "  Batch 590/899 | Loss: 0.6741 | CLoss: 0.5366 | FLoss: 0.2751 | LR: 1.96e-04\n",
      "  Batch 600/899 | Loss: 1.5812 | CLoss: 1.2987 | FLoss: 0.5649 | LR: 1.96e-04\n",
      "  Batch 610/899 | Loss: 0.8005 | CLoss: 0.5783 | FLoss: 0.4445 | LR: 1.96e-04\n",
      "  Batch 620/899 | Loss: 1.3779 | CLoss: 1.1325 | FLoss: 0.4909 | LR: 1.96e-04\n",
      "  Batch 630/899 | Loss: 0.9269 | CLoss: 0.6514 | FLoss: 0.5509 | LR: 1.96e-04\n",
      "  Batch 640/899 | Loss: 1.4340 | CLoss: 1.2193 | FLoss: 0.4295 | LR: 1.96e-04\n",
      "  Batch 650/899 | Loss: 1.3274 | CLoss: 1.0690 | FLoss: 0.5168 | LR: 1.96e-04\n",
      "  Batch 660/899 | Loss: 0.5558 | CLoss: 0.3458 | FLoss: 0.4202 | LR: 1.96e-04\n",
      "  Batch 670/899 | Loss: 0.7558 | CLoss: 0.5586 | FLoss: 0.3945 | LR: 1.96e-04\n",
      "  Batch 680/899 | Loss: 0.7344 | CLoss: 0.5483 | FLoss: 0.3722 | LR: 1.96e-04\n",
      "  Batch 690/899 | Loss: 0.8359 | CLoss: 0.5750 | FLoss: 0.5217 | LR: 1.96e-04\n",
      "  Batch 700/899 | Loss: 0.7251 | CLoss: 0.4200 | FLoss: 0.6101 | LR: 1.96e-04\n",
      "  Batch 710/899 | Loss: 1.1615 | CLoss: 0.8546 | FLoss: 0.6137 | LR: 1.96e-04\n",
      "  Batch 720/899 | Loss: 0.8920 | CLoss: 0.6590 | FLoss: 0.4661 | LR: 1.96e-04\n",
      "  Batch 730/899 | Loss: 1.4500 | CLoss: 1.2066 | FLoss: 0.4867 | LR: 1.96e-04\n",
      "  Batch 740/899 | Loss: 0.7449 | CLoss: 0.4208 | FLoss: 0.6483 | LR: 1.96e-04\n",
      "  Batch 750/899 | Loss: 0.8383 | CLoss: 0.6289 | FLoss: 0.4187 | LR: 1.96e-04\n",
      "  Batch 760/899 | Loss: 0.9686 | CLoss: 0.6494 | FLoss: 0.6385 | LR: 1.96e-04\n",
      "  Batch 770/899 | Loss: 0.8713 | CLoss: 0.6639 | FLoss: 0.4147 | LR: 1.96e-04\n",
      "  Batch 780/899 | Loss: 0.9831 | CLoss: 0.7972 | FLoss: 0.3719 | LR: 1.96e-04\n",
      "  Batch 790/899 | Loss: 0.5211 | CLoss: 0.3964 | FLoss: 0.2495 | LR: 1.96e-04\n",
      "  Batch 800/899 | Loss: 1.2358 | CLoss: 0.9851 | FLoss: 0.5014 | LR: 1.96e-04\n",
      "  Batch 810/899 | Loss: 0.8951 | CLoss: 0.7215 | FLoss: 0.3472 | LR: 1.96e-04\n",
      "  Batch 820/899 | Loss: 1.0651 | CLoss: 0.8942 | FLoss: 0.3418 | LR: 1.96e-04\n",
      "  Batch 830/899 | Loss: 0.8222 | CLoss: 0.6208 | FLoss: 0.4028 | LR: 1.96e-04\n",
      "  Batch 840/899 | Loss: 0.7274 | CLoss: 0.5783 | FLoss: 0.2982 | LR: 1.96e-04\n",
      "  Batch 850/899 | Loss: 0.7617 | CLoss: 0.5412 | FLoss: 0.4409 | LR: 1.96e-04\n",
      "  Batch 860/899 | Loss: 0.7662 | CLoss: 0.6249 | FLoss: 0.2825 | LR: 1.96e-04\n",
      "  Batch 870/899 | Loss: 1.0058 | CLoss: 0.7688 | FLoss: 0.4740 | LR: 1.96e-04\n",
      "  Batch 880/899 | Loss: 0.7309 | CLoss: 0.5605 | FLoss: 0.3407 | LR: 1.96e-04\n",
      "  Batch 890/899 | Loss: 1.0190 | CLoss: 0.7916 | FLoss: 0.4548 | LR: 1.96e-04\n",
      "  Batch 899/899 | Loss: 1.0451 | CLoss: 0.5613 | FLoss: 0.9675 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.9386\n",
      "  Last Batch Loss: 1.0451\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.9899 | Batch Acc: 65.52%\n",
      "    Val Batch 010/101 | Loss: 0.6124 | Batch Acc: 72.41%\n",
      "    Val Batch 015/101 | Loss: 0.4217 | Batch Acc: 87.93%\n",
      "    Val Batch 020/101 | Loss: 0.4735 | Batch Acc: 60.34%\n",
      "    Val Batch 025/101 | Loss: 1.7552 | Batch Acc: 46.55%\n",
      "    Val Batch 030/101 | Loss: 1.7081 | Batch Acc: 41.38%\n",
      "    Val Batch 035/101 | Loss: 1.7288 | Batch Acc: 56.90%\n",
      "    Val Batch 040/101 | Loss: 0.8669 | Batch Acc: 77.59%\n",
      "    Val Batch 045/101 | Loss: 0.6122 | Batch Acc: 100.00%\n",
      "    Val Batch 050/101 | Loss: 0.9282 | Batch Acc: 67.24%\n",
      "    Val Batch 055/101 | Loss: 0.3497 | Batch Acc: 87.93%\n",
      "    Val Batch 060/101 | Loss: 1.0997 | Batch Acc: 58.62%\n",
      "    Val Batch 065/101 | Loss: 0.1895 | Batch Acc: 96.55%\n",
      "    Val Batch 070/101 | Loss: 0.1274 | Batch Acc: 94.83%\n",
      "    Val Batch 075/101 | Loss: 0.1864 | Batch Acc: 94.83%\n",
      "    Val Batch 080/101 | Loss: 1.0875 | Batch Acc: 67.24%\n",
      "    Val Batch 085/101 | Loss: 0.5656 | Batch Acc: 81.03%\n",
      "    Val Batch 090/101 | Loss: 0.2406 | Batch Acc: 91.38%\n",
      "    Val Batch 095/101 | Loss: 0.3573 | Batch Acc: 89.66%\n",
      "    Val Batch 100/101 | Loss: 0.1174 | Batch Acc: 94.83%\n",
      "    Val Batch 101/101 | Loss: 0.0644 | Batch Acc: 95.00%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.6658 | Accuracy: 76.91%\n",
      "  Current Best Acc: 78.28%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/899 | Loss: 0.8353 | CLoss: 0.6396 | FLoss: 0.3913 | LR: 1.04e-04\n",
      "  Batch 020/899 | Loss: 0.8618 | CLoss: 0.6679 | FLoss: 0.3879 | LR: 1.04e-04\n",
      "  Batch 030/899 | Loss: 0.8272 | CLoss: 0.5999 | FLoss: 0.4545 | LR: 1.04e-04\n",
      "  Batch 040/899 | Loss: 0.7290 | CLoss: 0.5570 | FLoss: 0.3441 | LR: 1.04e-04\n",
      "  Batch 050/899 | Loss: 0.9447 | CLoss: 0.7598 | FLoss: 0.3700 | LR: 1.04e-04\n",
      "  Batch 060/899 | Loss: 0.6688 | CLoss: 0.4860 | FLoss: 0.3657 | LR: 1.04e-04\n",
      "  Batch 070/899 | Loss: 1.3951 | CLoss: 1.1451 | FLoss: 0.5000 | LR: 1.04e-04\n",
      "  Batch 080/899 | Loss: 0.7607 | CLoss: 0.5999 | FLoss: 0.3215 | LR: 1.04e-04\n",
      "  Batch 090/899 | Loss: 0.6425 | CLoss: 0.4580 | FLoss: 0.3690 | LR: 1.04e-04\n",
      "  Batch 100/899 | Loss: 1.0854 | CLoss: 0.9093 | FLoss: 0.3521 | LR: 1.04e-04\n",
      "  Batch 110/899 | Loss: 0.6018 | CLoss: 0.4659 | FLoss: 0.2718 | LR: 1.04e-04\n",
      "  Batch 120/899 | Loss: 0.6634 | CLoss: 0.4833 | FLoss: 0.3602 | LR: 1.04e-04\n",
      "  Batch 130/899 | Loss: 1.5135 | CLoss: 1.1590 | FLoss: 0.7090 | LR: 1.04e-04\n",
      "  Batch 140/899 | Loss: 0.6871 | CLoss: 0.5289 | FLoss: 0.3163 | LR: 1.04e-04\n",
      "  Batch 150/899 | Loss: 0.5010 | CLoss: 0.3577 | FLoss: 0.2865 | LR: 1.04e-04\n",
      "  Batch 160/899 | Loss: 1.2321 | CLoss: 0.9797 | FLoss: 0.5047 | LR: 1.04e-04\n",
      "  Batch 170/899 | Loss: 1.0711 | CLoss: 0.8525 | FLoss: 0.4370 | LR: 1.04e-04\n",
      "  Batch 180/899 | Loss: 0.7610 | CLoss: 0.5036 | FLoss: 0.5147 | LR: 1.04e-04\n",
      "  Batch 190/899 | Loss: 0.8724 | CLoss: 0.6837 | FLoss: 0.3774 | LR: 1.04e-04\n",
      "  Batch 200/899 | Loss: 0.6965 | CLoss: 0.5291 | FLoss: 0.3349 | LR: 1.04e-04\n",
      "  Batch 210/899 | Loss: 0.8562 | CLoss: 0.7057 | FLoss: 0.3010 | LR: 1.04e-04\n",
      "  Batch 220/899 | Loss: 0.5653 | CLoss: 0.4257 | FLoss: 0.2792 | LR: 1.04e-04\n",
      "  Batch 230/899 | Loss: 0.8655 | CLoss: 0.6749 | FLoss: 0.3811 | LR: 1.04e-04\n",
      "  Batch 240/899 | Loss: 0.9909 | CLoss: 0.7431 | FLoss: 0.4956 | LR: 1.04e-04\n",
      "  Batch 250/899 | Loss: 0.9607 | CLoss: 0.6972 | FLoss: 0.5271 | LR: 1.04e-04\n",
      "  Batch 260/899 | Loss: 0.7737 | CLoss: 0.5696 | FLoss: 0.4083 | LR: 1.04e-04\n",
      "  Batch 270/899 | Loss: 0.9642 | CLoss: 0.7526 | FLoss: 0.4232 | LR: 1.04e-04\n",
      "  Batch 280/899 | Loss: 0.7199 | CLoss: 0.5314 | FLoss: 0.3769 | LR: 1.04e-04\n",
      "  Batch 290/899 | Loss: 0.7839 | CLoss: 0.6009 | FLoss: 0.3659 | LR: 1.04e-04\n",
      "  Batch 300/899 | Loss: 0.9073 | CLoss: 0.6788 | FLoss: 0.4569 | LR: 1.04e-04\n",
      "  Batch 310/899 | Loss: 0.9547 | CLoss: 0.7244 | FLoss: 0.4607 | LR: 1.04e-04\n",
      "  Batch 320/899 | Loss: 0.8674 | CLoss: 0.6628 | FLoss: 0.4092 | LR: 1.04e-04\n",
      "  Batch 330/899 | Loss: 0.7190 | CLoss: 0.5812 | FLoss: 0.2757 | LR: 1.04e-04\n",
      "  Batch 340/899 | Loss: 0.5598 | CLoss: 0.4243 | FLoss: 0.2709 | LR: 1.04e-04\n",
      "  Batch 350/899 | Loss: 0.9396 | CLoss: 0.7061 | FLoss: 0.4670 | LR: 1.04e-04\n",
      "  Batch 360/899 | Loss: 0.6414 | CLoss: 0.4442 | FLoss: 0.3944 | LR: 1.04e-04\n",
      "  Batch 370/899 | Loss: 0.8531 | CLoss: 0.6451 | FLoss: 0.4160 | LR: 1.04e-04\n",
      "  Batch 380/899 | Loss: 1.0152 | CLoss: 0.8067 | FLoss: 0.4169 | LR: 1.04e-04\n",
      "  Batch 390/899 | Loss: 0.5517 | CLoss: 0.3918 | FLoss: 0.3198 | LR: 1.04e-04\n",
      "  Batch 400/899 | Loss: 0.5815 | CLoss: 0.4245 | FLoss: 0.3140 | LR: 1.04e-04\n",
      "  Batch 410/899 | Loss: 0.5933 | CLoss: 0.4328 | FLoss: 0.3211 | LR: 1.04e-04\n",
      "  Batch 420/899 | Loss: 0.7637 | CLoss: 0.5795 | FLoss: 0.3683 | LR: 1.04e-04\n",
      "  Batch 430/899 | Loss: 0.8568 | CLoss: 0.6804 | FLoss: 0.3529 | LR: 1.04e-04\n",
      "  Batch 440/899 | Loss: 0.8758 | CLoss: 0.6074 | FLoss: 0.5369 | LR: 1.04e-04\n",
      "  Batch 450/899 | Loss: 0.4641 | CLoss: 0.3572 | FLoss: 0.2138 | LR: 1.04e-04\n",
      "  Batch 460/899 | Loss: 0.7361 | CLoss: 0.5561 | FLoss: 0.3600 | LR: 1.04e-04\n",
      "  Batch 470/899 | Loss: 0.7536 | CLoss: 0.5715 | FLoss: 0.3642 | LR: 1.04e-04\n",
      "  Batch 480/899 | Loss: 0.7652 | CLoss: 0.5660 | FLoss: 0.3984 | LR: 1.04e-04\n",
      "  Batch 490/899 | Loss: 1.1132 | CLoss: 0.9469 | FLoss: 0.3325 | LR: 1.04e-04\n",
      "  Batch 500/899 | Loss: 1.1241 | CLoss: 0.7805 | FLoss: 0.6871 | LR: 1.04e-04\n",
      "  Batch 510/899 | Loss: 0.5727 | CLoss: 0.4712 | FLoss: 0.2030 | LR: 1.04e-04\n",
      "  Batch 520/899 | Loss: 0.5787 | CLoss: 0.4572 | FLoss: 0.2431 | LR: 1.04e-04\n",
      "  Batch 530/899 | Loss: 1.1941 | CLoss: 0.9622 | FLoss: 0.4638 | LR: 1.04e-04\n",
      "  Batch 540/899 | Loss: 0.6727 | CLoss: 0.5780 | FLoss: 0.1894 | LR: 1.04e-04\n",
      "  Batch 550/899 | Loss: 1.2007 | CLoss: 0.9688 | FLoss: 0.4637 | LR: 1.04e-04\n",
      "  Batch 560/899 | Loss: 0.9696 | CLoss: 0.6793 | FLoss: 0.5806 | LR: 1.04e-04\n",
      "  Batch 570/899 | Loss: 0.5205 | CLoss: 0.3739 | FLoss: 0.2933 | LR: 1.04e-04\n",
      "  Batch 580/899 | Loss: 0.9630 | CLoss: 0.7583 | FLoss: 0.4093 | LR: 1.04e-04\n",
      "  Batch 590/899 | Loss: 0.8239 | CLoss: 0.6204 | FLoss: 0.4069 | LR: 1.04e-04\n",
      "  Batch 600/899 | Loss: 0.6637 | CLoss: 0.5436 | FLoss: 0.2402 | LR: 1.04e-04\n",
      "  Batch 610/899 | Loss: 0.6565 | CLoss: 0.4361 | FLoss: 0.4407 | LR: 1.04e-04\n",
      "  Batch 620/899 | Loss: 0.9143 | CLoss: 0.7312 | FLoss: 0.3663 | LR: 1.04e-04\n",
      "  Batch 630/899 | Loss: 0.5219 | CLoss: 0.4258 | FLoss: 0.1923 | LR: 1.04e-04\n",
      "  Batch 640/899 | Loss: 0.7105 | CLoss: 0.4860 | FLoss: 0.4490 | LR: 1.04e-04\n",
      "  Batch 650/899 | Loss: 0.6799 | CLoss: 0.4977 | FLoss: 0.3645 | LR: 1.04e-04\n",
      "  Batch 660/899 | Loss: 0.7787 | CLoss: 0.5702 | FLoss: 0.4170 | LR: 1.04e-04\n",
      "  Batch 670/899 | Loss: 1.1338 | CLoss: 0.8339 | FLoss: 0.5996 | LR: 1.04e-04\n",
      "  Batch 680/899 | Loss: 0.6132 | CLoss: 0.3916 | FLoss: 0.4433 | LR: 1.04e-04\n",
      "  Batch 690/899 | Loss: 0.4663 | CLoss: 0.3163 | FLoss: 0.3001 | LR: 1.04e-04\n",
      "  Batch 700/899 | Loss: 1.2068 | CLoss: 0.8661 | FLoss: 0.6815 | LR: 1.04e-04\n",
      "  Batch 710/899 | Loss: 0.9967 | CLoss: 0.7700 | FLoss: 0.4534 | LR: 1.04e-04\n",
      "  Batch 720/899 | Loss: 0.6906 | CLoss: 0.5275 | FLoss: 0.3261 | LR: 1.04e-04\n",
      "  Batch 730/899 | Loss: 0.4189 | CLoss: 0.3263 | FLoss: 0.1851 | LR: 1.04e-04\n",
      "  Batch 740/899 | Loss: 1.4914 | CLoss: 1.1709 | FLoss: 0.6412 | LR: 1.04e-04\n",
      "  Batch 750/899 | Loss: 0.7121 | CLoss: 0.5115 | FLoss: 0.4013 | LR: 1.04e-04\n",
      "  Batch 760/899 | Loss: 1.0476 | CLoss: 0.8416 | FLoss: 0.4120 | LR: 1.04e-04\n",
      "  Batch 770/899 | Loss: 0.8976 | CLoss: 0.7175 | FLoss: 0.3602 | LR: 1.04e-04\n",
      "  Batch 780/899 | Loss: 0.7531 | CLoss: 0.6231 | FLoss: 0.2600 | LR: 1.04e-04\n",
      "  Batch 790/899 | Loss: 0.9240 | CLoss: 0.7642 | FLoss: 0.3196 | LR: 1.04e-04\n",
      "  Batch 800/899 | Loss: 0.9621 | CLoss: 0.6890 | FLoss: 0.5462 | LR: 1.04e-04\n",
      "  Batch 810/899 | Loss: 1.1557 | CLoss: 0.8844 | FLoss: 0.5426 | LR: 1.04e-04\n",
      "  Batch 820/899 | Loss: 0.4301 | CLoss: 0.3029 | FLoss: 0.2545 | LR: 1.04e-04\n",
      "  Batch 830/899 | Loss: 1.0157 | CLoss: 0.7930 | FLoss: 0.4454 | LR: 1.04e-04\n",
      "  Batch 840/899 | Loss: 0.6399 | CLoss: 0.4686 | FLoss: 0.3425 | LR: 1.04e-04\n",
      "  Batch 850/899 | Loss: 1.0177 | CLoss: 0.7820 | FLoss: 0.4713 | LR: 1.04e-04\n",
      "  Batch 860/899 | Loss: 0.6304 | CLoss: 0.4789 | FLoss: 0.3032 | LR: 1.04e-04\n",
      "  Batch 870/899 | Loss: 0.8762 | CLoss: 0.7185 | FLoss: 0.3153 | LR: 1.04e-04\n",
      "  Batch 880/899 | Loss: 0.8856 | CLoss: 0.6949 | FLoss: 0.3814 | LR: 1.04e-04\n",
      "  Batch 890/899 | Loss: 0.7149 | CLoss: 0.6311 | FLoss: 0.1676 | LR: 1.04e-04\n",
      "  Batch 899/899 | Loss: 0.3532 | CLoss: 0.1372 | FLoss: 0.4319 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.8191\n",
      "  Last Batch Loss: 0.3532\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.4533 | Batch Acc: 81.03%\n",
      "    Val Batch 010/101 | Loss: 0.5309 | Batch Acc: 87.93%\n",
      "    Val Batch 015/101 | Loss: 0.3437 | Batch Acc: 87.93%\n",
      "    Val Batch 020/101 | Loss: 0.6497 | Batch Acc: 55.17%\n",
      "    Val Batch 025/101 | Loss: 1.4703 | Batch Acc: 51.72%\n",
      "    Val Batch 030/101 | Loss: 1.0410 | Batch Acc: 56.90%\n",
      "    Val Batch 035/101 | Loss: 1.4131 | Batch Acc: 63.79%\n",
      "    Val Batch 040/101 | Loss: 1.4966 | Batch Acc: 72.41%\n",
      "    Val Batch 045/101 | Loss: 0.5282 | Batch Acc: 98.28%\n",
      "    Val Batch 050/101 | Loss: 0.7034 | Batch Acc: 74.14%\n",
      "    Val Batch 055/101 | Loss: 0.6351 | Batch Acc: 74.14%\n",
      "    Val Batch 060/101 | Loss: 0.7143 | Batch Acc: 70.69%\n",
      "    Val Batch 065/101 | Loss: 0.3462 | Batch Acc: 87.93%\n",
      "    Val Batch 070/101 | Loss: 0.1583 | Batch Acc: 93.10%\n",
      "    Val Batch 075/101 | Loss: 0.3235 | Batch Acc: 87.93%\n",
      "    Val Batch 080/101 | Loss: 0.6741 | Batch Acc: 79.31%\n",
      "    Val Batch 085/101 | Loss: 0.4626 | Batch Acc: 86.21%\n",
      "    Val Batch 090/101 | Loss: 0.3797 | Batch Acc: 89.66%\n",
      "    Val Batch 095/101 | Loss: 0.3018 | Batch Acc: 93.10%\n",
      "    Val Batch 100/101 | Loss: 0.1828 | Batch Acc: 96.55%\n",
      "    Val Batch 101/101 | Loss: 0.0155 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.6271 | Accuracy: 78.75%\n",
      "  Current Best Acc: 78.75%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/899 | Loss: 0.7228 | CLoss: 0.5237 | FLoss: 0.3982 | LR: 2.86e-05\n",
      "  Batch 020/899 | Loss: 0.8119 | CLoss: 0.6683 | FLoss: 0.2872 | LR: 2.86e-05\n",
      "  Batch 030/899 | Loss: 0.7937 | CLoss: 0.5755 | FLoss: 0.4365 | LR: 2.86e-05\n",
      "  Batch 040/899 | Loss: 0.7696 | CLoss: 0.5091 | FLoss: 0.5208 | LR: 2.86e-05\n",
      "  Batch 050/899 | Loss: 0.6987 | CLoss: 0.5485 | FLoss: 0.3003 | LR: 2.86e-05\n",
      "  Batch 060/899 | Loss: 0.6948 | CLoss: 0.5316 | FLoss: 0.3264 | LR: 2.86e-05\n",
      "  Batch 070/899 | Loss: 0.4910 | CLoss: 0.3177 | FLoss: 0.3466 | LR: 2.86e-05\n",
      "  Batch 080/899 | Loss: 1.3486 | CLoss: 0.9908 | FLoss: 0.7157 | LR: 2.86e-05\n",
      "  Batch 090/899 | Loss: 1.2821 | CLoss: 1.0436 | FLoss: 0.4770 | LR: 2.86e-05\n",
      "  Batch 100/899 | Loss: 0.7689 | CLoss: 0.6026 | FLoss: 0.3327 | LR: 2.86e-05\n",
      "  Batch 110/899 | Loss: 0.7981 | CLoss: 0.5554 | FLoss: 0.4853 | LR: 2.86e-05\n",
      "  Batch 120/899 | Loss: 0.6420 | CLoss: 0.4832 | FLoss: 0.3177 | LR: 2.86e-05\n",
      "  Batch 130/899 | Loss: 0.9185 | CLoss: 0.7486 | FLoss: 0.3398 | LR: 2.86e-05\n",
      "  Batch 140/899 | Loss: 0.9154 | CLoss: 0.7478 | FLoss: 0.3350 | LR: 2.86e-05\n",
      "  Batch 150/899 | Loss: 0.5039 | CLoss: 0.3464 | FLoss: 0.3152 | LR: 2.86e-05\n",
      "  Batch 160/899 | Loss: 0.5097 | CLoss: 0.4228 | FLoss: 0.1738 | LR: 2.86e-05\n",
      "  Batch 170/899 | Loss: 0.7574 | CLoss: 0.5177 | FLoss: 0.4794 | LR: 2.86e-05\n",
      "  Batch 180/899 | Loss: 0.6749 | CLoss: 0.5190 | FLoss: 0.3117 | LR: 2.86e-05\n",
      "  Batch 190/899 | Loss: 1.0457 | CLoss: 0.8912 | FLoss: 0.3090 | LR: 2.86e-05\n",
      "  Batch 200/899 | Loss: 0.9198 | CLoss: 0.7042 | FLoss: 0.4314 | LR: 2.86e-05\n",
      "  Batch 210/899 | Loss: 0.5112 | CLoss: 0.3877 | FLoss: 0.2469 | LR: 2.86e-05\n",
      "  Batch 220/899 | Loss: 0.6753 | CLoss: 0.5725 | FLoss: 0.2057 | LR: 2.86e-05\n",
      "  Batch 230/899 | Loss: 0.7961 | CLoss: 0.6142 | FLoss: 0.3637 | LR: 2.86e-05\n",
      "  Batch 240/899 | Loss: 0.3334 | CLoss: 0.2387 | FLoss: 0.1893 | LR: 2.86e-05\n",
      "  Batch 250/899 | Loss: 0.4914 | CLoss: 0.3952 | FLoss: 0.1923 | LR: 2.86e-05\n",
      "  Batch 260/899 | Loss: 0.8316 | CLoss: 0.5394 | FLoss: 0.5845 | LR: 2.86e-05\n",
      "  Batch 270/899 | Loss: 0.7385 | CLoss: 0.5825 | FLoss: 0.3119 | LR: 2.86e-05\n",
      "  Batch 280/899 | Loss: 0.5860 | CLoss: 0.5039 | FLoss: 0.1643 | LR: 2.86e-05\n",
      "  Batch 290/899 | Loss: 1.1279 | CLoss: 0.8604 | FLoss: 0.5349 | LR: 2.86e-05\n",
      "  Batch 300/899 | Loss: 0.5873 | CLoss: 0.4257 | FLoss: 0.3233 | LR: 2.86e-05\n",
      "  Batch 310/899 | Loss: 0.7008 | CLoss: 0.5046 | FLoss: 0.3925 | LR: 2.86e-05\n",
      "  Batch 320/899 | Loss: 0.4801 | CLoss: 0.3647 | FLoss: 0.2309 | LR: 2.86e-05\n",
      "  Batch 330/899 | Loss: 0.8603 | CLoss: 0.6163 | FLoss: 0.4880 | LR: 2.86e-05\n",
      "  Batch 340/899 | Loss: 0.9322 | CLoss: 0.7301 | FLoss: 0.4043 | LR: 2.86e-05\n",
      "  Batch 350/899 | Loss: 0.9363 | CLoss: 0.7091 | FLoss: 0.4544 | LR: 2.86e-05\n",
      "  Batch 360/899 | Loss: 0.9295 | CLoss: 0.6983 | FLoss: 0.4623 | LR: 2.86e-05\n",
      "  Batch 370/899 | Loss: 0.9461 | CLoss: 0.6583 | FLoss: 0.5756 | LR: 2.86e-05\n",
      "  Batch 380/899 | Loss: 0.7326 | CLoss: 0.4995 | FLoss: 0.4663 | LR: 2.86e-05\n",
      "  Batch 390/899 | Loss: 0.4305 | CLoss: 0.3473 | FLoss: 0.1665 | LR: 2.86e-05\n",
      "  Batch 400/899 | Loss: 0.4958 | CLoss: 0.3104 | FLoss: 0.3709 | LR: 2.86e-05\n",
      "  Batch 410/899 | Loss: 0.5482 | CLoss: 0.4504 | FLoss: 0.1956 | LR: 2.86e-05\n",
      "  Batch 420/899 | Loss: 0.5751 | CLoss: 0.4028 | FLoss: 0.3445 | LR: 2.86e-05\n",
      "  Batch 430/899 | Loss: 0.7614 | CLoss: 0.5651 | FLoss: 0.3927 | LR: 2.86e-05\n",
      "  Batch 440/899 | Loss: 0.6600 | CLoss: 0.4634 | FLoss: 0.3933 | LR: 2.86e-05\n",
      "  Batch 450/899 | Loss: 0.8019 | CLoss: 0.5574 | FLoss: 0.4891 | LR: 2.86e-05\n",
      "  Batch 460/899 | Loss: 0.8173 | CLoss: 0.6422 | FLoss: 0.3502 | LR: 2.86e-05\n",
      "  Batch 470/899 | Loss: 0.5411 | CLoss: 0.3984 | FLoss: 0.2856 | LR: 2.86e-05\n",
      "  Batch 480/899 | Loss: 0.3523 | CLoss: 0.2651 | FLoss: 0.1743 | LR: 2.86e-05\n",
      "  Batch 490/899 | Loss: 0.6457 | CLoss: 0.3682 | FLoss: 0.5550 | LR: 2.86e-05\n",
      "  Batch 500/899 | Loss: 0.6216 | CLoss: 0.4629 | FLoss: 0.3175 | LR: 2.86e-05\n",
      "  Batch 510/899 | Loss: 0.5799 | CLoss: 0.3414 | FLoss: 0.4770 | LR: 2.86e-05\n",
      "  Batch 520/899 | Loss: 0.5991 | CLoss: 0.3891 | FLoss: 0.4199 | LR: 2.86e-05\n",
      "  Batch 530/899 | Loss: 0.8732 | CLoss: 0.6565 | FLoss: 0.4334 | LR: 2.86e-05\n",
      "  Batch 540/899 | Loss: 0.5386 | CLoss: 0.4068 | FLoss: 0.2636 | LR: 2.86e-05\n",
      "  Batch 550/899 | Loss: 0.6818 | CLoss: 0.4317 | FLoss: 0.5003 | LR: 2.86e-05\n",
      "  Batch 560/899 | Loss: 0.7815 | CLoss: 0.5714 | FLoss: 0.4201 | LR: 2.86e-05\n",
      "  Batch 570/899 | Loss: 0.6195 | CLoss: 0.4449 | FLoss: 0.3493 | LR: 2.86e-05\n",
      "  Batch 580/899 | Loss: 0.4954 | CLoss: 0.2661 | FLoss: 0.4585 | LR: 2.86e-05\n",
      "  Batch 590/899 | Loss: 0.9263 | CLoss: 0.7709 | FLoss: 0.3107 | LR: 2.86e-05\n",
      "  Batch 600/899 | Loss: 0.8745 | CLoss: 0.7181 | FLoss: 0.3129 | LR: 2.86e-05\n",
      "  Batch 610/899 | Loss: 0.3477 | CLoss: 0.1752 | FLoss: 0.3451 | LR: 2.86e-05\n",
      "  Batch 620/899 | Loss: 0.4604 | CLoss: 0.2912 | FLoss: 0.3383 | LR: 2.86e-05\n",
      "  Batch 630/899 | Loss: 0.7691 | CLoss: 0.5948 | FLoss: 0.3487 | LR: 2.86e-05\n",
      "  Batch 640/899 | Loss: 0.7309 | CLoss: 0.6224 | FLoss: 0.2172 | LR: 2.86e-05\n",
      "  Batch 650/899 | Loss: 0.8028 | CLoss: 0.6558 | FLoss: 0.2941 | LR: 2.86e-05\n",
      "  Batch 660/899 | Loss: 0.6360 | CLoss: 0.5341 | FLoss: 0.2038 | LR: 2.86e-05\n",
      "  Batch 670/899 | Loss: 0.4545 | CLoss: 0.3560 | FLoss: 0.1971 | LR: 2.86e-05\n",
      "  Batch 680/899 | Loss: 0.7117 | CLoss: 0.5448 | FLoss: 0.3338 | LR: 2.86e-05\n",
      "  Batch 690/899 | Loss: 0.5448 | CLoss: 0.3633 | FLoss: 0.3630 | LR: 2.86e-05\n",
      "  Batch 700/899 | Loss: 0.9075 | CLoss: 0.7123 | FLoss: 0.3905 | LR: 2.86e-05\n",
      "  Batch 710/899 | Loss: 0.8085 | CLoss: 0.6156 | FLoss: 0.3860 | LR: 2.86e-05\n",
      "  Batch 720/899 | Loss: 0.4991 | CLoss: 0.3712 | FLoss: 0.2558 | LR: 2.86e-05\n",
      "  Batch 730/899 | Loss: 0.6049 | CLoss: 0.4139 | FLoss: 0.3822 | LR: 2.86e-05\n",
      "  Batch 740/899 | Loss: 0.5225 | CLoss: 0.3502 | FLoss: 0.3447 | LR: 2.86e-05\n",
      "  Batch 750/899 | Loss: 0.3020 | CLoss: 0.2165 | FLoss: 0.1709 | LR: 2.86e-05\n",
      "  Batch 760/899 | Loss: 0.9274 | CLoss: 0.6511 | FLoss: 0.5526 | LR: 2.86e-05\n",
      "  Batch 770/899 | Loss: 0.4200 | CLoss: 0.2567 | FLoss: 0.3266 | LR: 2.86e-05\n",
      "  Batch 780/899 | Loss: 0.7218 | CLoss: 0.5640 | FLoss: 0.3156 | LR: 2.86e-05\n",
      "  Batch 790/899 | Loss: 0.6488 | CLoss: 0.4743 | FLoss: 0.3492 | LR: 2.86e-05\n",
      "  Batch 800/899 | Loss: 0.8895 | CLoss: 0.7368 | FLoss: 0.3055 | LR: 2.86e-05\n",
      "  Batch 810/899 | Loss: 0.7403 | CLoss: 0.5158 | FLoss: 0.4489 | LR: 2.86e-05\n",
      "  Batch 820/899 | Loss: 0.5330 | CLoss: 0.4303 | FLoss: 0.2054 | LR: 2.86e-05\n",
      "  Batch 830/899 | Loss: 1.0337 | CLoss: 0.8641 | FLoss: 0.3393 | LR: 2.86e-05\n",
      "  Batch 840/899 | Loss: 0.5306 | CLoss: 0.3464 | FLoss: 0.3684 | LR: 2.86e-05\n",
      "  Batch 850/899 | Loss: 0.6068 | CLoss: 0.4812 | FLoss: 0.2512 | LR: 2.86e-05\n",
      "  Batch 860/899 | Loss: 0.5290 | CLoss: 0.3900 | FLoss: 0.2781 | LR: 2.86e-05\n",
      "  Batch 870/899 | Loss: 0.7441 | CLoss: 0.5382 | FLoss: 0.4118 | LR: 2.86e-05\n",
      "  Batch 880/899 | Loss: 0.8314 | CLoss: 0.6426 | FLoss: 0.3777 | LR: 2.86e-05\n",
      "  Batch 890/899 | Loss: 0.5741 | CLoss: 0.4229 | FLoss: 0.3023 | LR: 2.86e-05\n",
      "  Batch 899/899 | Loss: 0.1504 | CLoss: 0.0828 | FLoss: 0.1351 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.7272\n",
      "  Last Batch Loss: 0.1504\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.6423 | Batch Acc: 74.14%\n",
      "    Val Batch 010/101 | Loss: 0.3893 | Batch Acc: 87.93%\n",
      "    Val Batch 015/101 | Loss: 0.4373 | Batch Acc: 84.48%\n",
      "    Val Batch 020/101 | Loss: 0.7284 | Batch Acc: 48.28%\n",
      "    Val Batch 025/101 | Loss: 1.8135 | Batch Acc: 48.28%\n",
      "    Val Batch 030/101 | Loss: 1.2836 | Batch Acc: 55.17%\n",
      "    Val Batch 035/101 | Loss: 0.8283 | Batch Acc: 74.14%\n",
      "    Val Batch 040/101 | Loss: 0.7502 | Batch Acc: 84.48%\n",
      "    Val Batch 045/101 | Loss: 0.5363 | Batch Acc: 96.55%\n",
      "    Val Batch 050/101 | Loss: 0.6563 | Batch Acc: 75.86%\n",
      "    Val Batch 055/101 | Loss: 0.7625 | Batch Acc: 70.69%\n",
      "    Val Batch 060/101 | Loss: 1.1911 | Batch Acc: 53.45%\n",
      "    Val Batch 065/101 | Loss: 0.1772 | Batch Acc: 96.55%\n",
      "    Val Batch 070/101 | Loss: 0.1600 | Batch Acc: 96.55%\n",
      "    Val Batch 075/101 | Loss: 0.1894 | Batch Acc: 93.10%\n",
      "    Val Batch 080/101 | Loss: 0.5905 | Batch Acc: 84.48%\n",
      "    Val Batch 085/101 | Loss: 0.4176 | Batch Acc: 89.66%\n",
      "    Val Batch 090/101 | Loss: 0.2269 | Batch Acc: 91.38%\n",
      "    Val Batch 095/101 | Loss: 0.0371 | Batch Acc: 100.00%\n",
      "    Val Batch 100/101 | Loss: 0.1468 | Batch Acc: 98.28%\n",
      "    Val Batch 101/101 | Loss: 0.2702 | Batch Acc: 95.00%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.5729 | Accuracy: 79.73%\n",
      "  Current Best Acc: 79.73%\n",
      "\n",
      "========================================\n",
      "=== Fold 3 Completed ===\n",
      "Best Validation Accuracy: 79.73%\n",
      "\n",
      "========================================\n",
      "=== Fold 4/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/898 | Loss: 1.9473 | CLoss: 1.0246 | FLoss: 1.8454 | LR: 3.00e-04\n",
      "  Batch 020/898 | Loss: 1.7497 | CLoss: 1.1101 | FLoss: 1.2792 | LR: 3.00e-04\n",
      "  Batch 030/898 | Loss: 1.4457 | CLoss: 0.9586 | FLoss: 0.9742 | LR: 3.00e-04\n",
      "  Batch 040/898 | Loss: 1.3190 | CLoss: 0.9041 | FLoss: 0.8299 | LR: 3.00e-04\n",
      "  Batch 050/898 | Loss: 0.9875 | CLoss: 0.6277 | FLoss: 0.7197 | LR: 3.00e-04\n",
      "  Batch 060/898 | Loss: 1.0426 | CLoss: 0.7201 | FLoss: 0.6448 | LR: 3.00e-04\n",
      "  Batch 070/898 | Loss: 1.2285 | CLoss: 0.9365 | FLoss: 0.5841 | LR: 3.00e-04\n",
      "  Batch 080/898 | Loss: 0.6978 | CLoss: 0.4847 | FLoss: 0.4262 | LR: 3.00e-04\n",
      "  Batch 090/898 | Loss: 1.4934 | CLoss: 1.1671 | FLoss: 0.6526 | LR: 3.00e-04\n",
      "  Batch 100/898 | Loss: 1.1592 | CLoss: 0.8285 | FLoss: 0.6613 | LR: 3.00e-04\n",
      "  Batch 110/898 | Loss: 0.8925 | CLoss: 0.6727 | FLoss: 0.4395 | LR: 3.00e-04\n",
      "  Batch 120/898 | Loss: 1.5480 | CLoss: 1.2760 | FLoss: 0.5438 | LR: 3.00e-04\n",
      "  Batch 130/898 | Loss: 0.8622 | CLoss: 0.5972 | FLoss: 0.5301 | LR: 3.00e-04\n",
      "  Batch 140/898 | Loss: 1.0235 | CLoss: 0.7806 | FLoss: 0.4858 | LR: 3.00e-04\n",
      "  Batch 150/898 | Loss: 1.3012 | CLoss: 1.0310 | FLoss: 0.5406 | LR: 3.00e-04\n",
      "  Batch 160/898 | Loss: 1.0956 | CLoss: 0.8323 | FLoss: 0.5266 | LR: 3.00e-04\n",
      "  Batch 170/898 | Loss: 0.9718 | CLoss: 0.7493 | FLoss: 0.4449 | LR: 3.00e-04\n",
      "  Batch 180/898 | Loss: 1.0233 | CLoss: 0.7489 | FLoss: 0.5489 | LR: 3.00e-04\n",
      "  Batch 190/898 | Loss: 0.9107 | CLoss: 0.6829 | FLoss: 0.4557 | LR: 3.00e-04\n",
      "  Batch 200/898 | Loss: 0.8728 | CLoss: 0.6566 | FLoss: 0.4323 | LR: 3.00e-04\n",
      "  Batch 210/898 | Loss: 1.4928 | CLoss: 1.1246 | FLoss: 0.7365 | LR: 3.00e-04\n",
      "  Batch 220/898 | Loss: 1.4557 | CLoss: 1.0920 | FLoss: 0.7273 | LR: 3.00e-04\n",
      "  Batch 230/898 | Loss: 1.3946 | CLoss: 1.2007 | FLoss: 0.3876 | LR: 3.00e-04\n",
      "  Batch 240/898 | Loss: 1.2754 | CLoss: 1.0092 | FLoss: 0.5323 | LR: 3.00e-04\n",
      "  Batch 250/898 | Loss: 0.7292 | CLoss: 0.4986 | FLoss: 0.4610 | LR: 3.00e-04\n",
      "  Batch 260/898 | Loss: 1.3119 | CLoss: 1.0728 | FLoss: 0.4781 | LR: 3.00e-04\n",
      "  Batch 270/898 | Loss: 1.1923 | CLoss: 0.9276 | FLoss: 0.5293 | LR: 3.00e-04\n",
      "  Batch 280/898 | Loss: 1.0917 | CLoss: 0.8444 | FLoss: 0.4946 | LR: 3.00e-04\n",
      "  Batch 290/898 | Loss: 1.1007 | CLoss: 0.7386 | FLoss: 0.7241 | LR: 3.00e-04\n",
      "  Batch 300/898 | Loss: 1.6285 | CLoss: 1.3301 | FLoss: 0.5967 | LR: 3.00e-04\n",
      "  Batch 310/898 | Loss: 1.2794 | CLoss: 0.9524 | FLoss: 0.6540 | LR: 3.00e-04\n",
      "  Batch 320/898 | Loss: 0.7433 | CLoss: 0.4927 | FLoss: 0.5013 | LR: 3.00e-04\n",
      "  Batch 330/898 | Loss: 1.2934 | CLoss: 0.9836 | FLoss: 0.6197 | LR: 3.00e-04\n",
      "  Batch 340/898 | Loss: 1.0184 | CLoss: 0.7692 | FLoss: 0.4985 | LR: 3.00e-04\n",
      "  Batch 350/898 | Loss: 1.0091 | CLoss: 0.7781 | FLoss: 0.4620 | LR: 3.00e-04\n",
      "  Batch 360/898 | Loss: 0.9862 | CLoss: 0.6334 | FLoss: 0.7056 | LR: 3.00e-04\n",
      "  Batch 370/898 | Loss: 1.0029 | CLoss: 0.7217 | FLoss: 0.5624 | LR: 3.00e-04\n",
      "  Batch 380/898 | Loss: 1.0973 | CLoss: 0.7823 | FLoss: 0.6302 | LR: 3.00e-04\n",
      "  Batch 390/898 | Loss: 0.9123 | CLoss: 0.6984 | FLoss: 0.4278 | LR: 3.00e-04\n",
      "  Batch 400/898 | Loss: 1.0017 | CLoss: 0.7479 | FLoss: 0.5076 | LR: 3.00e-04\n",
      "  Batch 410/898 | Loss: 1.0395 | CLoss: 0.6852 | FLoss: 0.7088 | LR: 3.00e-04\n",
      "  Batch 420/898 | Loss: 1.1214 | CLoss: 0.9139 | FLoss: 0.4150 | LR: 3.00e-04\n",
      "  Batch 430/898 | Loss: 1.5273 | CLoss: 1.2914 | FLoss: 0.4717 | LR: 3.00e-04\n",
      "  Batch 440/898 | Loss: 1.2887 | CLoss: 1.0488 | FLoss: 0.4798 | LR: 3.00e-04\n",
      "  Batch 450/898 | Loss: 1.2809 | CLoss: 1.0088 | FLoss: 0.5443 | LR: 3.00e-04\n",
      "  Batch 460/898 | Loss: 0.6254 | CLoss: 0.5099 | FLoss: 0.2311 | LR: 3.00e-04\n",
      "  Batch 470/898 | Loss: 1.3725 | CLoss: 1.1220 | FLoss: 0.5009 | LR: 3.00e-04\n",
      "  Batch 480/898 | Loss: 1.2698 | CLoss: 0.9870 | FLoss: 0.5656 | LR: 3.00e-04\n",
      "  Batch 490/898 | Loss: 1.2738 | CLoss: 1.0310 | FLoss: 0.4855 | LR: 3.00e-04\n",
      "  Batch 500/898 | Loss: 1.0627 | CLoss: 0.7570 | FLoss: 0.6114 | LR: 3.00e-04\n",
      "  Batch 510/898 | Loss: 0.9419 | CLoss: 0.6760 | FLoss: 0.5318 | LR: 3.00e-04\n",
      "  Batch 520/898 | Loss: 1.2292 | CLoss: 0.9176 | FLoss: 0.6231 | LR: 3.00e-04\n",
      "  Batch 530/898 | Loss: 1.3219 | CLoss: 1.0354 | FLoss: 0.5730 | LR: 3.00e-04\n",
      "  Batch 540/898 | Loss: 0.9612 | CLoss: 0.7037 | FLoss: 0.5150 | LR: 3.00e-04\n",
      "  Batch 550/898 | Loss: 0.8719 | CLoss: 0.6302 | FLoss: 0.4835 | LR: 3.00e-04\n",
      "  Batch 560/898 | Loss: 0.7733 | CLoss: 0.6103 | FLoss: 0.3259 | LR: 3.00e-04\n",
      "  Batch 570/898 | Loss: 0.8196 | CLoss: 0.6048 | FLoss: 0.4298 | LR: 3.00e-04\n",
      "  Batch 580/898 | Loss: 0.8521 | CLoss: 0.5775 | FLoss: 0.5491 | LR: 3.00e-04\n",
      "  Batch 590/898 | Loss: 1.3765 | CLoss: 1.1373 | FLoss: 0.4784 | LR: 3.00e-04\n",
      "  Batch 600/898 | Loss: 1.5795 | CLoss: 1.2521 | FLoss: 0.6548 | LR: 3.00e-04\n",
      "  Batch 610/898 | Loss: 1.0554 | CLoss: 0.7455 | FLoss: 0.6197 | LR: 3.00e-04\n",
      "  Batch 620/898 | Loss: 0.7439 | CLoss: 0.5885 | FLoss: 0.3107 | LR: 3.00e-04\n",
      "  Batch 630/898 | Loss: 1.2465 | CLoss: 1.0095 | FLoss: 0.4741 | LR: 3.00e-04\n",
      "  Batch 640/898 | Loss: 0.7992 | CLoss: 0.5621 | FLoss: 0.4743 | LR: 3.00e-04\n",
      "  Batch 650/898 | Loss: 0.8246 | CLoss: 0.6183 | FLoss: 0.4128 | LR: 3.00e-04\n",
      "  Batch 660/898 | Loss: 0.7380 | CLoss: 0.5755 | FLoss: 0.3250 | LR: 3.00e-04\n",
      "  Batch 670/898 | Loss: 1.0738 | CLoss: 0.7541 | FLoss: 0.6394 | LR: 3.00e-04\n",
      "  Batch 680/898 | Loss: 1.2192 | CLoss: 0.9328 | FLoss: 0.5728 | LR: 3.00e-04\n",
      "  Batch 690/898 | Loss: 0.9459 | CLoss: 0.7299 | FLoss: 0.4320 | LR: 3.00e-04\n",
      "  Batch 700/898 | Loss: 1.4619 | CLoss: 1.0924 | FLoss: 0.7388 | LR: 3.00e-04\n",
      "  Batch 710/898 | Loss: 0.7828 | CLoss: 0.5393 | FLoss: 0.4869 | LR: 3.00e-04\n",
      "  Batch 720/898 | Loss: 0.7753 | CLoss: 0.5550 | FLoss: 0.4405 | LR: 3.00e-04\n",
      "  Batch 730/898 | Loss: 1.0640 | CLoss: 0.8162 | FLoss: 0.4956 | LR: 3.00e-04\n",
      "  Batch 740/898 | Loss: 0.8444 | CLoss: 0.6572 | FLoss: 0.3743 | LR: 3.00e-04\n",
      "  Batch 750/898 | Loss: 1.0716 | CLoss: 0.7409 | FLoss: 0.6615 | LR: 3.00e-04\n",
      "  Batch 760/898 | Loss: 0.8173 | CLoss: 0.6099 | FLoss: 0.4147 | LR: 3.00e-04\n",
      "  Batch 770/898 | Loss: 0.8120 | CLoss: 0.5496 | FLoss: 0.5248 | LR: 3.00e-04\n",
      "  Batch 780/898 | Loss: 1.1233 | CLoss: 0.9203 | FLoss: 0.4061 | LR: 3.00e-04\n",
      "  Batch 790/898 | Loss: 0.8749 | CLoss: 0.4854 | FLoss: 0.7789 | LR: 3.00e-04\n",
      "  Batch 800/898 | Loss: 0.8913 | CLoss: 0.6938 | FLoss: 0.3949 | LR: 3.00e-04\n",
      "  Batch 810/898 | Loss: 1.2873 | CLoss: 1.0368 | FLoss: 0.5010 | LR: 3.00e-04\n",
      "  Batch 820/898 | Loss: 0.6950 | CLoss: 0.5427 | FLoss: 0.3046 | LR: 3.00e-04\n",
      "  Batch 830/898 | Loss: 1.3548 | CLoss: 1.0783 | FLoss: 0.5529 | LR: 3.00e-04\n",
      "  Batch 840/898 | Loss: 1.1198 | CLoss: 0.9420 | FLoss: 0.3556 | LR: 3.00e-04\n",
      "  Batch 850/898 | Loss: 0.8803 | CLoss: 0.7104 | FLoss: 0.3397 | LR: 3.00e-04\n",
      "  Batch 860/898 | Loss: 0.7965 | CLoss: 0.5345 | FLoss: 0.5239 | LR: 3.00e-04\n",
      "  Batch 870/898 | Loss: 1.5843 | CLoss: 1.4122 | FLoss: 0.3444 | LR: 3.00e-04\n",
      "  Batch 880/898 | Loss: 1.2677 | CLoss: 0.8807 | FLoss: 0.7741 | LR: 3.00e-04\n",
      "  Batch 890/898 | Loss: 1.0998 | CLoss: 0.8648 | FLoss: 0.4700 | LR: 3.00e-04\n",
      "  Batch 898/898 | Loss: 1.0226 | CLoss: 0.8051 | FLoss: 0.4350 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 1.1383\n",
      "  Last Batch Loss: 1.0226\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.6410 | Batch Acc: 72.41%\n",
      "    Val Batch 010/101 | Loss: 0.4408 | Batch Acc: 89.66%\n",
      "    Val Batch 015/101 | Loss: 0.6595 | Batch Acc: 74.14%\n",
      "    Val Batch 020/101 | Loss: 0.4988 | Batch Acc: 91.38%\n",
      "    Val Batch 025/101 | Loss: 0.3023 | Batch Acc: 89.66%\n",
      "    Val Batch 030/101 | Loss: 0.6568 | Batch Acc: 81.03%\n",
      "    Val Batch 035/101 | Loss: 0.6203 | Batch Acc: 81.03%\n",
      "    Val Batch 040/101 | Loss: 0.7592 | Batch Acc: 46.55%\n",
      "    Val Batch 045/101 | Loss: 1.3382 | Batch Acc: 17.24%\n",
      "    Val Batch 050/101 | Loss: 0.3562 | Batch Acc: 87.93%\n",
      "    Val Batch 055/101 | Loss: 1.3179 | Batch Acc: 60.34%\n",
      "    Val Batch 060/101 | Loss: 1.0371 | Batch Acc: 68.97%\n",
      "    Val Batch 065/101 | Loss: 0.3202 | Batch Acc: 89.66%\n",
      "    Val Batch 070/101 | Loss: 0.3482 | Batch Acc: 82.76%\n",
      "    Val Batch 075/101 | Loss: 0.2083 | Batch Acc: 94.83%\n",
      "    Val Batch 080/101 | Loss: 0.2690 | Batch Acc: 93.10%\n",
      "    Val Batch 085/101 | Loss: 0.5049 | Batch Acc: 86.21%\n",
      "    Val Batch 090/101 | Loss: 0.3343 | Batch Acc: 87.93%\n",
      "    Val Batch 095/101 | Loss: 0.2190 | Batch Acc: 94.83%\n",
      "    Val Batch 100/101 | Loss: 0.3204 | Batch Acc: 91.38%\n",
      "    Val Batch 101/101 | Loss: 0.4139 | Batch Acc: 90.91%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.5148 | Accuracy: 81.74%\n",
      "  Current Best Acc: 81.74%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/898 | Loss: 0.8522 | CLoss: 0.6490 | FLoss: 0.4066 | LR: 2.71e-04\n",
      "  Batch 020/898 | Loss: 1.1475 | CLoss: 0.9261 | FLoss: 0.4428 | LR: 2.71e-04\n",
      "  Batch 030/898 | Loss: 0.8939 | CLoss: 0.7070 | FLoss: 0.3738 | LR: 2.71e-04\n",
      "  Batch 040/898 | Loss: 1.0557 | CLoss: 0.7964 | FLoss: 0.5187 | LR: 2.71e-04\n",
      "  Batch 050/898 | Loss: 1.1310 | CLoss: 0.8428 | FLoss: 0.5762 | LR: 2.71e-04\n",
      "  Batch 060/898 | Loss: 0.7287 | CLoss: 0.5329 | FLoss: 0.3915 | LR: 2.71e-04\n",
      "  Batch 070/898 | Loss: 0.8391 | CLoss: 0.6416 | FLoss: 0.3950 | LR: 2.71e-04\n",
      "  Batch 080/898 | Loss: 0.4798 | CLoss: 0.2861 | FLoss: 0.3874 | LR: 2.71e-04\n",
      "  Batch 090/898 | Loss: 1.4256 | CLoss: 1.1397 | FLoss: 0.5719 | LR: 2.71e-04\n",
      "  Batch 100/898 | Loss: 0.9901 | CLoss: 0.7415 | FLoss: 0.4971 | LR: 2.71e-04\n",
      "  Batch 110/898 | Loss: 0.8194 | CLoss: 0.5308 | FLoss: 0.5771 | LR: 2.71e-04\n",
      "  Batch 120/898 | Loss: 0.8273 | CLoss: 0.6627 | FLoss: 0.3292 | LR: 2.71e-04\n",
      "  Batch 130/898 | Loss: 0.5272 | CLoss: 0.3595 | FLoss: 0.3354 | LR: 2.71e-04\n",
      "  Batch 140/898 | Loss: 1.3080 | CLoss: 1.0365 | FLoss: 0.5429 | LR: 2.71e-04\n",
      "  Batch 150/898 | Loss: 0.5665 | CLoss: 0.4037 | FLoss: 0.3255 | LR: 2.71e-04\n",
      "  Batch 160/898 | Loss: 1.1241 | CLoss: 0.9087 | FLoss: 0.4308 | LR: 2.71e-04\n",
      "  Batch 170/898 | Loss: 0.9983 | CLoss: 0.8132 | FLoss: 0.3702 | LR: 2.71e-04\n",
      "  Batch 180/898 | Loss: 0.8196 | CLoss: 0.6269 | FLoss: 0.3854 | LR: 2.71e-04\n",
      "  Batch 190/898 | Loss: 1.2209 | CLoss: 0.9735 | FLoss: 0.4948 | LR: 2.71e-04\n",
      "  Batch 200/898 | Loss: 1.0187 | CLoss: 0.7313 | FLoss: 0.5748 | LR: 2.71e-04\n",
      "  Batch 210/898 | Loss: 0.9686 | CLoss: 0.7258 | FLoss: 0.4856 | LR: 2.71e-04\n",
      "  Batch 220/898 | Loss: 1.2663 | CLoss: 0.9438 | FLoss: 0.6451 | LR: 2.71e-04\n",
      "  Batch 230/898 | Loss: 0.7364 | CLoss: 0.5176 | FLoss: 0.4377 | LR: 2.71e-04\n",
      "  Batch 240/898 | Loss: 1.5657 | CLoss: 1.2511 | FLoss: 0.6292 | LR: 2.71e-04\n",
      "  Batch 250/898 | Loss: 1.0897 | CLoss: 0.7508 | FLoss: 0.6778 | LR: 2.71e-04\n",
      "  Batch 260/898 | Loss: 0.6829 | CLoss: 0.5297 | FLoss: 0.3064 | LR: 2.71e-04\n",
      "  Batch 270/898 | Loss: 0.5553 | CLoss: 0.3869 | FLoss: 0.3368 | LR: 2.71e-04\n",
      "  Batch 280/898 | Loss: 1.0161 | CLoss: 0.7422 | FLoss: 0.5478 | LR: 2.71e-04\n",
      "  Batch 290/898 | Loss: 0.6023 | CLoss: 0.4443 | FLoss: 0.3161 | LR: 2.71e-04\n",
      "  Batch 300/898 | Loss: 1.2658 | CLoss: 0.9547 | FLoss: 0.6222 | LR: 2.71e-04\n",
      "  Batch 310/898 | Loss: 0.8938 | CLoss: 0.6156 | FLoss: 0.5564 | LR: 2.71e-04\n",
      "  Batch 320/898 | Loss: 1.2295 | CLoss: 1.0529 | FLoss: 0.3531 | LR: 2.71e-04\n",
      "  Batch 330/898 | Loss: 1.1024 | CLoss: 0.8782 | FLoss: 0.4485 | LR: 2.71e-04\n",
      "  Batch 340/898 | Loss: 1.7091 | CLoss: 1.4216 | FLoss: 0.5749 | LR: 2.71e-04\n",
      "  Batch 350/898 | Loss: 0.9360 | CLoss: 0.6996 | FLoss: 0.4728 | LR: 2.71e-04\n",
      "  Batch 360/898 | Loss: 0.7396 | CLoss: 0.5032 | FLoss: 0.4727 | LR: 2.71e-04\n",
      "  Batch 370/898 | Loss: 0.9707 | CLoss: 0.7397 | FLoss: 0.4620 | LR: 2.71e-04\n",
      "  Batch 380/898 | Loss: 1.0000 | CLoss: 0.7947 | FLoss: 0.4106 | LR: 2.71e-04\n",
      "  Batch 390/898 | Loss: 0.7797 | CLoss: 0.5360 | FLoss: 0.4875 | LR: 2.71e-04\n",
      "  Batch 400/898 | Loss: 1.0519 | CLoss: 0.7678 | FLoss: 0.5682 | LR: 2.71e-04\n",
      "  Batch 410/898 | Loss: 1.1929 | CLoss: 0.9836 | FLoss: 0.4186 | LR: 2.71e-04\n",
      "  Batch 420/898 | Loss: 0.9827 | CLoss: 0.8368 | FLoss: 0.2918 | LR: 2.71e-04\n",
      "  Batch 430/898 | Loss: 0.9975 | CLoss: 0.7729 | FLoss: 0.4492 | LR: 2.71e-04\n",
      "  Batch 440/898 | Loss: 0.9730 | CLoss: 0.7736 | FLoss: 0.3989 | LR: 2.71e-04\n",
      "  Batch 450/898 | Loss: 0.5604 | CLoss: 0.3905 | FLoss: 0.3399 | LR: 2.71e-04\n",
      "  Batch 460/898 | Loss: 1.2548 | CLoss: 0.9627 | FLoss: 0.5842 | LR: 2.71e-04\n",
      "  Batch 470/898 | Loss: 0.8769 | CLoss: 0.5828 | FLoss: 0.5881 | LR: 2.71e-04\n",
      "  Batch 480/898 | Loss: 1.0066 | CLoss: 0.8007 | FLoss: 0.4119 | LR: 2.71e-04\n",
      "  Batch 490/898 | Loss: 0.9058 | CLoss: 0.6206 | FLoss: 0.5703 | LR: 2.71e-04\n",
      "  Batch 500/898 | Loss: 1.2531 | CLoss: 0.9418 | FLoss: 0.6227 | LR: 2.71e-04\n",
      "  Batch 510/898 | Loss: 1.0583 | CLoss: 0.7932 | FLoss: 0.5302 | LR: 2.71e-04\n",
      "  Batch 520/898 | Loss: 0.7706 | CLoss: 0.5068 | FLoss: 0.5276 | LR: 2.71e-04\n",
      "  Batch 530/898 | Loss: 0.8181 | CLoss: 0.6229 | FLoss: 0.3904 | LR: 2.71e-04\n",
      "  Batch 540/898 | Loss: 0.9982 | CLoss: 0.7544 | FLoss: 0.4878 | LR: 2.71e-04\n",
      "  Batch 550/898 | Loss: 1.0135 | CLoss: 0.7726 | FLoss: 0.4818 | LR: 2.71e-04\n",
      "  Batch 560/898 | Loss: 0.8577 | CLoss: 0.6016 | FLoss: 0.5122 | LR: 2.71e-04\n",
      "  Batch 570/898 | Loss: 0.9951 | CLoss: 0.7843 | FLoss: 0.4217 | LR: 2.71e-04\n",
      "  Batch 580/898 | Loss: 1.1381 | CLoss: 0.8938 | FLoss: 0.4886 | LR: 2.71e-04\n",
      "  Batch 590/898 | Loss: 0.8393 | CLoss: 0.6298 | FLoss: 0.4190 | LR: 2.71e-04\n",
      "  Batch 600/898 | Loss: 0.5022 | CLoss: 0.3396 | FLoss: 0.3251 | LR: 2.71e-04\n",
      "  Batch 610/898 | Loss: 0.7019 | CLoss: 0.4958 | FLoss: 0.4124 | LR: 2.71e-04\n",
      "  Batch 620/898 | Loss: 1.0114 | CLoss: 0.7561 | FLoss: 0.5106 | LR: 2.71e-04\n",
      "  Batch 630/898 | Loss: 1.0353 | CLoss: 0.7810 | FLoss: 0.5087 | LR: 2.71e-04\n",
      "  Batch 640/898 | Loss: 0.9653 | CLoss: 0.7628 | FLoss: 0.4050 | LR: 2.71e-04\n",
      "  Batch 650/898 | Loss: 1.2609 | CLoss: 0.9446 | FLoss: 0.6327 | LR: 2.71e-04\n",
      "  Batch 660/898 | Loss: 0.9324 | CLoss: 0.6486 | FLoss: 0.5675 | LR: 2.71e-04\n",
      "  Batch 670/898 | Loss: 0.9829 | CLoss: 0.7354 | FLoss: 0.4950 | LR: 2.71e-04\n",
      "  Batch 680/898 | Loss: 0.7266 | CLoss: 0.5334 | FLoss: 0.3863 | LR: 2.71e-04\n",
      "  Batch 690/898 | Loss: 0.8186 | CLoss: 0.6007 | FLoss: 0.4357 | LR: 2.71e-04\n",
      "  Batch 700/898 | Loss: 1.1857 | CLoss: 0.8227 | FLoss: 0.7258 | LR: 2.71e-04\n",
      "  Batch 710/898 | Loss: 0.9651 | CLoss: 0.7284 | FLoss: 0.4734 | LR: 2.71e-04\n",
      "  Batch 720/898 | Loss: 1.0656 | CLoss: 0.7919 | FLoss: 0.5474 | LR: 2.71e-04\n",
      "  Batch 730/898 | Loss: 0.9598 | CLoss: 0.7615 | FLoss: 0.3965 | LR: 2.71e-04\n",
      "  Batch 740/898 | Loss: 1.3662 | CLoss: 1.0217 | FLoss: 0.6890 | LR: 2.71e-04\n",
      "  Batch 750/898 | Loss: 0.8571 | CLoss: 0.7055 | FLoss: 0.3031 | LR: 2.71e-04\n",
      "  Batch 760/898 | Loss: 1.5150 | CLoss: 1.1146 | FLoss: 0.8009 | LR: 2.71e-04\n",
      "  Batch 770/898 | Loss: 1.1645 | CLoss: 0.9205 | FLoss: 0.4878 | LR: 2.71e-04\n",
      "  Batch 780/898 | Loss: 1.2072 | CLoss: 0.9489 | FLoss: 0.5166 | LR: 2.71e-04\n",
      "  Batch 790/898 | Loss: 0.7740 | CLoss: 0.5066 | FLoss: 0.5346 | LR: 2.71e-04\n",
      "  Batch 800/898 | Loss: 1.1804 | CLoss: 0.9365 | FLoss: 0.4879 | LR: 2.71e-04\n",
      "  Batch 810/898 | Loss: 0.7529 | CLoss: 0.4966 | FLoss: 0.5125 | LR: 2.71e-04\n",
      "  Batch 820/898 | Loss: 0.6318 | CLoss: 0.4742 | FLoss: 0.3154 | LR: 2.71e-04\n",
      "  Batch 830/898 | Loss: 1.4715 | CLoss: 1.2826 | FLoss: 0.3779 | LR: 2.71e-04\n",
      "  Batch 840/898 | Loss: 1.0544 | CLoss: 0.8240 | FLoss: 0.4608 | LR: 2.71e-04\n",
      "  Batch 850/898 | Loss: 1.3470 | CLoss: 1.1433 | FLoss: 0.4074 | LR: 2.71e-04\n",
      "  Batch 860/898 | Loss: 0.8272 | CLoss: 0.6747 | FLoss: 0.3050 | LR: 2.71e-04\n",
      "  Batch 870/898 | Loss: 1.1394 | CLoss: 0.9250 | FLoss: 0.4288 | LR: 2.71e-04\n",
      "  Batch 880/898 | Loss: 1.1155 | CLoss: 0.8958 | FLoss: 0.4395 | LR: 2.71e-04\n",
      "  Batch 890/898 | Loss: 1.1114 | CLoss: 0.8259 | FLoss: 0.5710 | LR: 2.71e-04\n",
      "  Batch 898/898 | Loss: 1.1450 | CLoss: 0.8859 | FLoss: 0.5182 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.9981\n",
      "  Last Batch Loss: 1.1450\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.8943 | Batch Acc: 63.79%\n",
      "    Val Batch 010/101 | Loss: 0.3634 | Batch Acc: 93.10%\n",
      "    Val Batch 015/101 | Loss: 0.5041 | Batch Acc: 89.66%\n",
      "    Val Batch 020/101 | Loss: 0.7106 | Batch Acc: 65.52%\n",
      "    Val Batch 025/101 | Loss: 0.4996 | Batch Acc: 84.48%\n",
      "    Val Batch 030/101 | Loss: 0.7853 | Batch Acc: 72.41%\n",
      "    Val Batch 035/101 | Loss: 1.0525 | Batch Acc: 62.07%\n",
      "    Val Batch 040/101 | Loss: 0.5940 | Batch Acc: 98.28%\n",
      "    Val Batch 045/101 | Loss: 0.9664 | Batch Acc: 84.48%\n",
      "    Val Batch 050/101 | Loss: 0.5224 | Batch Acc: 86.21%\n",
      "    Val Batch 055/101 | Loss: 1.5774 | Batch Acc: 43.10%\n",
      "    Val Batch 060/101 | Loss: 0.9088 | Batch Acc: 72.41%\n",
      "    Val Batch 065/101 | Loss: 0.5203 | Batch Acc: 82.76%\n",
      "    Val Batch 070/101 | Loss: 0.4163 | Batch Acc: 84.48%\n",
      "    Val Batch 075/101 | Loss: 0.4251 | Batch Acc: 86.21%\n",
      "    Val Batch 080/101 | Loss: 0.2987 | Batch Acc: 93.10%\n",
      "    Val Batch 085/101 | Loss: 0.2469 | Batch Acc: 93.10%\n",
      "    Val Batch 090/101 | Loss: 0.1920 | Batch Acc: 93.10%\n",
      "    Val Batch 095/101 | Loss: 0.4761 | Batch Acc: 87.93%\n",
      "    Val Batch 100/101 | Loss: 0.3518 | Batch Acc: 89.66%\n",
      "    Val Batch 101/101 | Loss: 0.3958 | Batch Acc: 88.64%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.5784 | Accuracy: 81.72%\n",
      "  Current Best Acc: 81.74%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/898 | Loss: 0.9681 | CLoss: 0.6561 | FLoss: 0.6238 | LR: 1.96e-04\n",
      "  Batch 020/898 | Loss: 1.2725 | CLoss: 1.1129 | FLoss: 0.3191 | LR: 1.96e-04\n",
      "  Batch 030/898 | Loss: 1.4132 | CLoss: 1.0794 | FLoss: 0.6675 | LR: 1.96e-04\n",
      "  Batch 040/898 | Loss: 0.8746 | CLoss: 0.6164 | FLoss: 0.5163 | LR: 1.96e-04\n",
      "  Batch 050/898 | Loss: 0.5044 | CLoss: 0.3633 | FLoss: 0.2823 | LR: 1.96e-04\n",
      "  Batch 060/898 | Loss: 1.1475 | CLoss: 0.9274 | FLoss: 0.4400 | LR: 1.96e-04\n",
      "  Batch 070/898 | Loss: 0.7815 | CLoss: 0.6384 | FLoss: 0.2862 | LR: 1.96e-04\n",
      "  Batch 080/898 | Loss: 0.7179 | CLoss: 0.6051 | FLoss: 0.2255 | LR: 1.96e-04\n",
      "  Batch 090/898 | Loss: 0.9057 | CLoss: 0.6840 | FLoss: 0.4435 | LR: 1.96e-04\n",
      "  Batch 100/898 | Loss: 0.9980 | CLoss: 0.8541 | FLoss: 0.2878 | LR: 1.96e-04\n",
      "  Batch 110/898 | Loss: 1.1628 | CLoss: 0.8805 | FLoss: 0.5646 | LR: 1.96e-04\n",
      "  Batch 120/898 | Loss: 1.1624 | CLoss: 0.9152 | FLoss: 0.4945 | LR: 1.96e-04\n",
      "  Batch 130/898 | Loss: 1.2341 | CLoss: 0.8783 | FLoss: 0.7114 | LR: 1.96e-04\n",
      "  Batch 140/898 | Loss: 0.9118 | CLoss: 0.6778 | FLoss: 0.4681 | LR: 1.96e-04\n",
      "  Batch 150/898 | Loss: 0.9156 | CLoss: 0.6904 | FLoss: 0.4505 | LR: 1.96e-04\n",
      "  Batch 160/898 | Loss: 0.5222 | CLoss: 0.3985 | FLoss: 0.2474 | LR: 1.96e-04\n",
      "  Batch 170/898 | Loss: 0.9609 | CLoss: 0.7107 | FLoss: 0.5004 | LR: 1.96e-04\n",
      "  Batch 180/898 | Loss: 1.0367 | CLoss: 0.7456 | FLoss: 0.5822 | LR: 1.96e-04\n",
      "  Batch 190/898 | Loss: 0.6183 | CLoss: 0.4786 | FLoss: 0.2794 | LR: 1.96e-04\n",
      "  Batch 200/898 | Loss: 1.1786 | CLoss: 0.8198 | FLoss: 0.7177 | LR: 1.96e-04\n",
      "  Batch 210/898 | Loss: 0.5052 | CLoss: 0.3435 | FLoss: 0.3235 | LR: 1.96e-04\n",
      "  Batch 220/898 | Loss: 0.9304 | CLoss: 0.7923 | FLoss: 0.2763 | LR: 1.96e-04\n",
      "  Batch 230/898 | Loss: 0.9380 | CLoss: 0.6631 | FLoss: 0.5498 | LR: 1.96e-04\n",
      "  Batch 240/898 | Loss: 0.5985 | CLoss: 0.3957 | FLoss: 0.4057 | LR: 1.96e-04\n",
      "  Batch 250/898 | Loss: 0.9006 | CLoss: 0.6588 | FLoss: 0.4835 | LR: 1.96e-04\n",
      "  Batch 260/898 | Loss: 0.7411 | CLoss: 0.5445 | FLoss: 0.3932 | LR: 1.96e-04\n",
      "  Batch 270/898 | Loss: 0.6829 | CLoss: 0.5386 | FLoss: 0.2887 | LR: 1.96e-04\n",
      "  Batch 280/898 | Loss: 1.2382 | CLoss: 0.9745 | FLoss: 0.5274 | LR: 1.96e-04\n",
      "  Batch 290/898 | Loss: 0.7321 | CLoss: 0.5238 | FLoss: 0.4166 | LR: 1.96e-04\n",
      "  Batch 300/898 | Loss: 0.7184 | CLoss: 0.5873 | FLoss: 0.2623 | LR: 1.96e-04\n",
      "  Batch 310/898 | Loss: 0.7882 | CLoss: 0.5841 | FLoss: 0.4082 | LR: 1.96e-04\n",
      "  Batch 320/898 | Loss: 0.8742 | CLoss: 0.7535 | FLoss: 0.2414 | LR: 1.96e-04\n",
      "  Batch 330/898 | Loss: 0.8470 | CLoss: 0.6823 | FLoss: 0.3294 | LR: 1.96e-04\n",
      "  Batch 340/898 | Loss: 0.4338 | CLoss: 0.3228 | FLoss: 0.2220 | LR: 1.96e-04\n",
      "  Batch 350/898 | Loss: 0.7453 | CLoss: 0.5449 | FLoss: 0.4008 | LR: 1.96e-04\n",
      "  Batch 360/898 | Loss: 0.8014 | CLoss: 0.5116 | FLoss: 0.5797 | LR: 1.96e-04\n",
      "  Batch 370/898 | Loss: 0.8645 | CLoss: 0.6559 | FLoss: 0.4173 | LR: 1.96e-04\n",
      "  Batch 380/898 | Loss: 0.7091 | CLoss: 0.5267 | FLoss: 0.3647 | LR: 1.96e-04\n",
      "  Batch 390/898 | Loss: 0.9199 | CLoss: 0.7420 | FLoss: 0.3559 | LR: 1.96e-04\n",
      "  Batch 400/898 | Loss: 0.8323 | CLoss: 0.6750 | FLoss: 0.3148 | LR: 1.96e-04\n",
      "  Batch 410/898 | Loss: 0.9288 | CLoss: 0.7115 | FLoss: 0.4347 | LR: 1.96e-04\n",
      "  Batch 420/898 | Loss: 0.7879 | CLoss: 0.5664 | FLoss: 0.4430 | LR: 1.96e-04\n",
      "  Batch 430/898 | Loss: 0.6909 | CLoss: 0.5645 | FLoss: 0.2529 | LR: 1.96e-04\n",
      "  Batch 440/898 | Loss: 1.4231 | CLoss: 1.0505 | FLoss: 0.7451 | LR: 1.96e-04\n",
      "  Batch 450/898 | Loss: 0.6350 | CLoss: 0.4966 | FLoss: 0.2769 | LR: 1.96e-04\n",
      "  Batch 460/898 | Loss: 0.9168 | CLoss: 0.6744 | FLoss: 0.4848 | LR: 1.96e-04\n",
      "  Batch 470/898 | Loss: 0.7102 | CLoss: 0.5102 | FLoss: 0.4001 | LR: 1.96e-04\n",
      "  Batch 480/898 | Loss: 1.1902 | CLoss: 0.9277 | FLoss: 0.5249 | LR: 1.96e-04\n",
      "  Batch 490/898 | Loss: 0.4556 | CLoss: 0.3293 | FLoss: 0.2527 | LR: 1.96e-04\n",
      "  Batch 500/898 | Loss: 0.6905 | CLoss: 0.5476 | FLoss: 0.2859 | LR: 1.96e-04\n",
      "  Batch 510/898 | Loss: 0.9165 | CLoss: 0.6457 | FLoss: 0.5416 | LR: 1.96e-04\n",
      "  Batch 520/898 | Loss: 0.8837 | CLoss: 0.6629 | FLoss: 0.4416 | LR: 1.96e-04\n",
      "  Batch 530/898 | Loss: 0.7504 | CLoss: 0.5565 | FLoss: 0.3877 | LR: 1.96e-04\n",
      "  Batch 540/898 | Loss: 0.7822 | CLoss: 0.5885 | FLoss: 0.3873 | LR: 1.96e-04\n",
      "  Batch 550/898 | Loss: 1.0762 | CLoss: 0.8569 | FLoss: 0.4386 | LR: 1.96e-04\n",
      "  Batch 560/898 | Loss: 1.0852 | CLoss: 0.8018 | FLoss: 0.5667 | LR: 1.96e-04\n",
      "  Batch 570/898 | Loss: 0.9771 | CLoss: 0.7193 | FLoss: 0.5156 | LR: 1.96e-04\n",
      "  Batch 580/898 | Loss: 0.7813 | CLoss: 0.5979 | FLoss: 0.3668 | LR: 1.96e-04\n",
      "  Batch 590/898 | Loss: 0.6623 | CLoss: 0.4391 | FLoss: 0.4463 | LR: 1.96e-04\n",
      "  Batch 600/898 | Loss: 1.1604 | CLoss: 0.9515 | FLoss: 0.4178 | LR: 1.96e-04\n",
      "  Batch 610/898 | Loss: 1.0394 | CLoss: 0.7800 | FLoss: 0.5189 | LR: 1.96e-04\n",
      "  Batch 620/898 | Loss: 0.5729 | CLoss: 0.4299 | FLoss: 0.2859 | LR: 1.96e-04\n",
      "  Batch 630/898 | Loss: 1.0975 | CLoss: 0.8196 | FLoss: 0.5558 | LR: 1.96e-04\n",
      "  Batch 640/898 | Loss: 0.9886 | CLoss: 0.7495 | FLoss: 0.4783 | LR: 1.96e-04\n",
      "  Batch 650/898 | Loss: 0.7762 | CLoss: 0.5649 | FLoss: 0.4227 | LR: 1.96e-04\n",
      "  Batch 660/898 | Loss: 0.9863 | CLoss: 0.8113 | FLoss: 0.3501 | LR: 1.96e-04\n",
      "  Batch 670/898 | Loss: 0.5273 | CLoss: 0.4182 | FLoss: 0.2184 | LR: 1.96e-04\n",
      "  Batch 680/898 | Loss: 0.6079 | CLoss: 0.4941 | FLoss: 0.2276 | LR: 1.96e-04\n",
      "  Batch 690/898 | Loss: 0.5583 | CLoss: 0.4355 | FLoss: 0.2455 | LR: 1.96e-04\n",
      "  Batch 700/898 | Loss: 1.0622 | CLoss: 0.8605 | FLoss: 0.4034 | LR: 1.96e-04\n",
      "  Batch 710/898 | Loss: 1.0840 | CLoss: 0.8306 | FLoss: 0.5067 | LR: 1.96e-04\n",
      "  Batch 720/898 | Loss: 1.0686 | CLoss: 0.8143 | FLoss: 0.5085 | LR: 1.96e-04\n",
      "  Batch 730/898 | Loss: 0.6268 | CLoss: 0.4499 | FLoss: 0.3538 | LR: 1.96e-04\n",
      "  Batch 740/898 | Loss: 1.0775 | CLoss: 0.8168 | FLoss: 0.5214 | LR: 1.96e-04\n",
      "  Batch 750/898 | Loss: 0.9663 | CLoss: 0.7634 | FLoss: 0.4059 | LR: 1.96e-04\n",
      "  Batch 760/898 | Loss: 1.3517 | CLoss: 1.0477 | FLoss: 0.6080 | LR: 1.96e-04\n",
      "  Batch 770/898 | Loss: 1.1092 | CLoss: 0.8829 | FLoss: 0.4526 | LR: 1.96e-04\n",
      "  Batch 780/898 | Loss: 0.5833 | CLoss: 0.4290 | FLoss: 0.3084 | LR: 1.96e-04\n",
      "  Batch 790/898 | Loss: 0.9021 | CLoss: 0.6877 | FLoss: 0.4287 | LR: 1.96e-04\n",
      "  Batch 800/898 | Loss: 0.4625 | CLoss: 0.3631 | FLoss: 0.1989 | LR: 1.96e-04\n",
      "  Batch 810/898 | Loss: 0.8969 | CLoss: 0.6991 | FLoss: 0.3955 | LR: 1.96e-04\n",
      "  Batch 820/898 | Loss: 0.7167 | CLoss: 0.5454 | FLoss: 0.3424 | LR: 1.96e-04\n",
      "  Batch 830/898 | Loss: 1.2093 | CLoss: 0.9699 | FLoss: 0.4789 | LR: 1.96e-04\n",
      "  Batch 840/898 | Loss: 1.0799 | CLoss: 0.8367 | FLoss: 0.4866 | LR: 1.96e-04\n",
      "  Batch 850/898 | Loss: 1.0364 | CLoss: 0.7561 | FLoss: 0.5607 | LR: 1.96e-04\n",
      "  Batch 860/898 | Loss: 0.6937 | CLoss: 0.5648 | FLoss: 0.2578 | LR: 1.96e-04\n",
      "  Batch 870/898 | Loss: 0.8256 | CLoss: 0.5639 | FLoss: 0.5233 | LR: 1.96e-04\n",
      "  Batch 880/898 | Loss: 0.5896 | CLoss: 0.4333 | FLoss: 0.3125 | LR: 1.96e-04\n",
      "  Batch 890/898 | Loss: 0.7809 | CLoss: 0.6342 | FLoss: 0.2934 | LR: 1.96e-04\n",
      "  Batch 898/898 | Loss: 0.8151 | CLoss: 0.5878 | FLoss: 0.4546 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.8756\n",
      "  Last Batch Loss: 0.8151\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.3821 | Batch Acc: 82.76%\n",
      "    Val Batch 010/101 | Loss: 0.3040 | Batch Acc: 84.48%\n",
      "    Val Batch 015/101 | Loss: 0.8006 | Batch Acc: 72.41%\n",
      "    Val Batch 020/101 | Loss: 0.8073 | Batch Acc: 77.59%\n",
      "    Val Batch 025/101 | Loss: 0.4596 | Batch Acc: 87.93%\n",
      "    Val Batch 030/101 | Loss: 0.5036 | Batch Acc: 86.21%\n",
      "    Val Batch 035/101 | Loss: 0.7134 | Batch Acc: 72.41%\n",
      "    Val Batch 040/101 | Loss: 0.8200 | Batch Acc: 46.55%\n",
      "    Val Batch 045/101 | Loss: 1.2522 | Batch Acc: 20.69%\n",
      "    Val Batch 050/101 | Loss: 0.2103 | Batch Acc: 91.38%\n",
      "    Val Batch 055/101 | Loss: 1.6268 | Batch Acc: 51.72%\n",
      "    Val Batch 060/101 | Loss: 1.3848 | Batch Acc: 55.17%\n",
      "    Val Batch 065/101 | Loss: 0.3886 | Batch Acc: 86.21%\n",
      "    Val Batch 070/101 | Loss: 0.4807 | Batch Acc: 86.21%\n",
      "    Val Batch 075/101 | Loss: 0.1736 | Batch Acc: 93.10%\n",
      "    Val Batch 080/101 | Loss: 0.4266 | Batch Acc: 84.48%\n",
      "    Val Batch 085/101 | Loss: 0.1726 | Batch Acc: 93.10%\n",
      "    Val Batch 090/101 | Loss: 0.2494 | Batch Acc: 93.10%\n",
      "    Val Batch 095/101 | Loss: 0.1739 | Batch Acc: 94.83%\n",
      "    Val Batch 100/101 | Loss: 0.1898 | Batch Acc: 94.83%\n",
      "    Val Batch 101/101 | Loss: 0.2943 | Batch Acc: 88.64%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.5310 | Accuracy: 80.66%\n",
      "  Current Best Acc: 81.74%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/898 | Loss: 0.6762 | CLoss: 0.5450 | FLoss: 0.2624 | LR: 1.04e-04\n",
      "  Batch 020/898 | Loss: 0.7345 | CLoss: 0.5000 | FLoss: 0.4690 | LR: 1.04e-04\n",
      "  Batch 030/898 | Loss: 0.8297 | CLoss: 0.5918 | FLoss: 0.4759 | LR: 1.04e-04\n",
      "  Batch 040/898 | Loss: 1.0008 | CLoss: 0.7464 | FLoss: 0.5086 | LR: 1.04e-04\n",
      "  Batch 050/898 | Loss: 0.7636 | CLoss: 0.6208 | FLoss: 0.2856 | LR: 1.04e-04\n",
      "  Batch 060/898 | Loss: 0.7450 | CLoss: 0.6190 | FLoss: 0.2519 | LR: 1.04e-04\n",
      "  Batch 070/898 | Loss: 0.8304 | CLoss: 0.6299 | FLoss: 0.4010 | LR: 1.04e-04\n",
      "  Batch 080/898 | Loss: 0.6451 | CLoss: 0.4795 | FLoss: 0.3312 | LR: 1.04e-04\n",
      "  Batch 090/898 | Loss: 0.9446 | CLoss: 0.7591 | FLoss: 0.3709 | LR: 1.04e-04\n",
      "  Batch 100/898 | Loss: 0.9518 | CLoss: 0.6599 | FLoss: 0.5838 | LR: 1.04e-04\n",
      "  Batch 110/898 | Loss: 1.1429 | CLoss: 0.8806 | FLoss: 0.5246 | LR: 1.04e-04\n",
      "  Batch 120/898 | Loss: 0.9687 | CLoss: 0.7933 | FLoss: 0.3509 | LR: 1.04e-04\n",
      "  Batch 130/898 | Loss: 0.5979 | CLoss: 0.4512 | FLoss: 0.2934 | LR: 1.04e-04\n",
      "  Batch 140/898 | Loss: 0.8143 | CLoss: 0.6161 | FLoss: 0.3964 | LR: 1.04e-04\n",
      "  Batch 150/898 | Loss: 0.8417 | CLoss: 0.6204 | FLoss: 0.4427 | LR: 1.04e-04\n",
      "  Batch 160/898 | Loss: 0.6430 | CLoss: 0.4917 | FLoss: 0.3026 | LR: 1.04e-04\n",
      "  Batch 170/898 | Loss: 0.7624 | CLoss: 0.5527 | FLoss: 0.4193 | LR: 1.04e-04\n",
      "  Batch 180/898 | Loss: 0.5850 | CLoss: 0.4176 | FLoss: 0.3348 | LR: 1.04e-04\n",
      "  Batch 190/898 | Loss: 0.9044 | CLoss: 0.7171 | FLoss: 0.3746 | LR: 1.04e-04\n",
      "  Batch 200/898 | Loss: 0.7627 | CLoss: 0.6031 | FLoss: 0.3192 | LR: 1.04e-04\n",
      "  Batch 210/898 | Loss: 0.8067 | CLoss: 0.6872 | FLoss: 0.2391 | LR: 1.04e-04\n",
      "  Batch 220/898 | Loss: 1.2010 | CLoss: 0.8988 | FLoss: 0.6044 | LR: 1.04e-04\n",
      "  Batch 230/898 | Loss: 0.3438 | CLoss: 0.2372 | FLoss: 0.2132 | LR: 1.04e-04\n",
      "  Batch 240/898 | Loss: 0.6506 | CLoss: 0.5227 | FLoss: 0.2559 | LR: 1.04e-04\n",
      "  Batch 250/898 | Loss: 0.8919 | CLoss: 0.7302 | FLoss: 0.3234 | LR: 1.04e-04\n",
      "  Batch 260/898 | Loss: 0.6717 | CLoss: 0.5133 | FLoss: 0.3170 | LR: 1.04e-04\n",
      "  Batch 270/898 | Loss: 1.3888 | CLoss: 1.0417 | FLoss: 0.6943 | LR: 1.04e-04\n",
      "  Batch 280/898 | Loss: 0.8271 | CLoss: 0.6815 | FLoss: 0.2911 | LR: 1.04e-04\n",
      "  Batch 290/898 | Loss: 1.0212 | CLoss: 0.8105 | FLoss: 0.4214 | LR: 1.04e-04\n",
      "  Batch 300/898 | Loss: 0.7462 | CLoss: 0.5504 | FLoss: 0.3915 | LR: 1.04e-04\n",
      "  Batch 310/898 | Loss: 0.6649 | CLoss: 0.5284 | FLoss: 0.2729 | LR: 1.04e-04\n",
      "  Batch 320/898 | Loss: 0.6833 | CLoss: 0.5505 | FLoss: 0.2655 | LR: 1.04e-04\n",
      "  Batch 330/898 | Loss: 0.6863 | CLoss: 0.4729 | FLoss: 0.4269 | LR: 1.04e-04\n",
      "  Batch 340/898 | Loss: 0.8664 | CLoss: 0.7049 | FLoss: 0.3230 | LR: 1.04e-04\n",
      "  Batch 350/898 | Loss: 0.8873 | CLoss: 0.7288 | FLoss: 0.3170 | LR: 1.04e-04\n",
      "  Batch 360/898 | Loss: 0.4651 | CLoss: 0.3095 | FLoss: 0.3114 | LR: 1.04e-04\n",
      "  Batch 370/898 | Loss: 0.7280 | CLoss: 0.5957 | FLoss: 0.2647 | LR: 1.04e-04\n",
      "  Batch 380/898 | Loss: 0.8901 | CLoss: 0.6404 | FLoss: 0.4993 | LR: 1.04e-04\n",
      "  Batch 390/898 | Loss: 0.4898 | CLoss: 0.2990 | FLoss: 0.3816 | LR: 1.04e-04\n",
      "  Batch 400/898 | Loss: 0.8049 | CLoss: 0.6458 | FLoss: 0.3182 | LR: 1.04e-04\n",
      "  Batch 410/898 | Loss: 0.8464 | CLoss: 0.6538 | FLoss: 0.3851 | LR: 1.04e-04\n",
      "  Batch 420/898 | Loss: 0.7308 | CLoss: 0.5657 | FLoss: 0.3302 | LR: 1.04e-04\n",
      "  Batch 430/898 | Loss: 0.6991 | CLoss: 0.5710 | FLoss: 0.2562 | LR: 1.04e-04\n",
      "  Batch 440/898 | Loss: 1.1546 | CLoss: 0.9402 | FLoss: 0.4288 | LR: 1.04e-04\n",
      "  Batch 450/898 | Loss: 0.6220 | CLoss: 0.4915 | FLoss: 0.2608 | LR: 1.04e-04\n",
      "  Batch 460/898 | Loss: 0.5762 | CLoss: 0.3468 | FLoss: 0.4588 | LR: 1.04e-04\n",
      "  Batch 470/898 | Loss: 0.6764 | CLoss: 0.4857 | FLoss: 0.3813 | LR: 1.04e-04\n",
      "  Batch 480/898 | Loss: 0.6841 | CLoss: 0.5000 | FLoss: 0.3682 | LR: 1.04e-04\n",
      "  Batch 490/898 | Loss: 0.9401 | CLoss: 0.7784 | FLoss: 0.3233 | LR: 1.04e-04\n",
      "  Batch 500/898 | Loss: 0.6218 | CLoss: 0.4849 | FLoss: 0.2738 | LR: 1.04e-04\n",
      "  Batch 510/898 | Loss: 0.4152 | CLoss: 0.3169 | FLoss: 0.1966 | LR: 1.04e-04\n",
      "  Batch 520/898 | Loss: 0.8713 | CLoss: 0.6550 | FLoss: 0.4325 | LR: 1.04e-04\n",
      "  Batch 530/898 | Loss: 0.8049 | CLoss: 0.6276 | FLoss: 0.3544 | LR: 1.04e-04\n",
      "  Batch 540/898 | Loss: 0.5408 | CLoss: 0.3438 | FLoss: 0.3939 | LR: 1.04e-04\n",
      "  Batch 550/898 | Loss: 0.7284 | CLoss: 0.5146 | FLoss: 0.4276 | LR: 1.04e-04\n",
      "  Batch 560/898 | Loss: 0.9380 | CLoss: 0.7047 | FLoss: 0.4666 | LR: 1.04e-04\n",
      "  Batch 570/898 | Loss: 0.9588 | CLoss: 0.8058 | FLoss: 0.3061 | LR: 1.04e-04\n",
      "  Batch 580/898 | Loss: 0.6091 | CLoss: 0.4948 | FLoss: 0.2287 | LR: 1.04e-04\n",
      "  Batch 590/898 | Loss: 0.5189 | CLoss: 0.3684 | FLoss: 0.3009 | LR: 1.04e-04\n",
      "  Batch 600/898 | Loss: 0.9548 | CLoss: 0.6347 | FLoss: 0.6402 | LR: 1.04e-04\n",
      "  Batch 610/898 | Loss: 0.5107 | CLoss: 0.3830 | FLoss: 0.2554 | LR: 1.04e-04\n",
      "  Batch 620/898 | Loss: 0.9265 | CLoss: 0.7246 | FLoss: 0.4038 | LR: 1.04e-04\n",
      "  Batch 630/898 | Loss: 1.0109 | CLoss: 0.7404 | FLoss: 0.5409 | LR: 1.04e-04\n",
      "  Batch 640/898 | Loss: 0.4228 | CLoss: 0.3722 | FLoss: 0.1012 | LR: 1.04e-04\n",
      "  Batch 650/898 | Loss: 0.4591 | CLoss: 0.3402 | FLoss: 0.2379 | LR: 1.04e-04\n",
      "  Batch 660/898 | Loss: 0.8310 | CLoss: 0.6721 | FLoss: 0.3178 | LR: 1.04e-04\n",
      "  Batch 670/898 | Loss: 0.2607 | CLoss: 0.1421 | FLoss: 0.2372 | LR: 1.04e-04\n",
      "  Batch 680/898 | Loss: 0.8928 | CLoss: 0.6427 | FLoss: 0.5003 | LR: 1.04e-04\n",
      "  Batch 690/898 | Loss: 0.5521 | CLoss: 0.4446 | FLoss: 0.2149 | LR: 1.04e-04\n",
      "  Batch 700/898 | Loss: 0.7142 | CLoss: 0.4219 | FLoss: 0.5846 | LR: 1.04e-04\n",
      "  Batch 710/898 | Loss: 0.6291 | CLoss: 0.4891 | FLoss: 0.2800 | LR: 1.04e-04\n",
      "  Batch 720/898 | Loss: 0.5895 | CLoss: 0.4487 | FLoss: 0.2817 | LR: 1.04e-04\n",
      "  Batch 730/898 | Loss: 0.5149 | CLoss: 0.4000 | FLoss: 0.2297 | LR: 1.04e-04\n",
      "  Batch 740/898 | Loss: 0.4794 | CLoss: 0.3594 | FLoss: 0.2399 | LR: 1.04e-04\n",
      "  Batch 750/898 | Loss: 0.7043 | CLoss: 0.5825 | FLoss: 0.2438 | LR: 1.04e-04\n",
      "  Batch 760/898 | Loss: 0.5827 | CLoss: 0.4347 | FLoss: 0.2959 | LR: 1.04e-04\n",
      "  Batch 770/898 | Loss: 0.8827 | CLoss: 0.7148 | FLoss: 0.3359 | LR: 1.04e-04\n",
      "  Batch 780/898 | Loss: 1.3011 | CLoss: 1.0566 | FLoss: 0.4891 | LR: 1.04e-04\n",
      "  Batch 790/898 | Loss: 0.4565 | CLoss: 0.4055 | FLoss: 0.1019 | LR: 1.04e-04\n",
      "  Batch 800/898 | Loss: 1.0296 | CLoss: 0.7799 | FLoss: 0.4995 | LR: 1.04e-04\n",
      "  Batch 810/898 | Loss: 0.7623 | CLoss: 0.6478 | FLoss: 0.2290 | LR: 1.04e-04\n",
      "  Batch 820/898 | Loss: 0.8134 | CLoss: 0.5937 | FLoss: 0.4394 | LR: 1.04e-04\n",
      "  Batch 830/898 | Loss: 0.5910 | CLoss: 0.4533 | FLoss: 0.2754 | LR: 1.04e-04\n",
      "  Batch 840/898 | Loss: 0.4568 | CLoss: 0.3511 | FLoss: 0.2114 | LR: 1.04e-04\n",
      "  Batch 850/898 | Loss: 0.6072 | CLoss: 0.4396 | FLoss: 0.3352 | LR: 1.04e-04\n",
      "  Batch 860/898 | Loss: 0.6286 | CLoss: 0.5202 | FLoss: 0.2169 | LR: 1.04e-04\n",
      "  Batch 870/898 | Loss: 0.9816 | CLoss: 0.7887 | FLoss: 0.3858 | LR: 1.04e-04\n",
      "  Batch 880/898 | Loss: 0.5347 | CLoss: 0.3882 | FLoss: 0.2932 | LR: 1.04e-04\n",
      "  Batch 890/898 | Loss: 0.7853 | CLoss: 0.6379 | FLoss: 0.2949 | LR: 1.04e-04\n",
      "  Batch 898/898 | Loss: 0.4458 | CLoss: 0.2946 | FLoss: 0.3024 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.7608\n",
      "  Last Batch Loss: 0.4458\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.7774 | Batch Acc: 74.14%\n",
      "    Val Batch 010/101 | Loss: 0.3083 | Batch Acc: 89.66%\n",
      "    Val Batch 015/101 | Loss: 0.8182 | Batch Acc: 84.48%\n",
      "    Val Batch 020/101 | Loss: 0.9761 | Batch Acc: 55.17%\n",
      "    Val Batch 025/101 | Loss: 0.3355 | Batch Acc: 86.21%\n",
      "    Val Batch 030/101 | Loss: 0.5289 | Batch Acc: 84.48%\n",
      "    Val Batch 035/101 | Loss: 0.3072 | Batch Acc: 87.93%\n",
      "    Val Batch 040/101 | Loss: 0.5143 | Batch Acc: 98.28%\n",
      "    Val Batch 045/101 | Loss: 1.0213 | Batch Acc: 87.93%\n",
      "    Val Batch 050/101 | Loss: 0.3423 | Batch Acc: 86.21%\n",
      "    Val Batch 055/101 | Loss: 0.9629 | Batch Acc: 67.24%\n",
      "    Val Batch 060/101 | Loss: 0.9971 | Batch Acc: 60.34%\n",
      "    Val Batch 065/101 | Loss: 0.4781 | Batch Acc: 81.03%\n",
      "    Val Batch 070/101 | Loss: 0.4026 | Batch Acc: 86.21%\n",
      "    Val Batch 075/101 | Loss: 0.1757 | Batch Acc: 91.38%\n",
      "    Val Batch 080/101 | Loss: 0.3291 | Batch Acc: 89.66%\n",
      "    Val Batch 085/101 | Loss: 0.1400 | Batch Acc: 94.83%\n",
      "    Val Batch 090/101 | Loss: 0.2126 | Batch Acc: 91.38%\n",
      "    Val Batch 095/101 | Loss: 0.3228 | Batch Acc: 84.48%\n",
      "    Val Batch 100/101 | Loss: 0.2903 | Batch Acc: 93.10%\n",
      "    Val Batch 101/101 | Loss: 0.1198 | Batch Acc: 97.73%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.4845 | Accuracy: 84.26%\n",
      "  Current Best Acc: 84.26%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/898 | Loss: 1.1041 | CLoss: 0.8722 | FLoss: 0.4638 | LR: 2.86e-05\n",
      "  Batch 020/898 | Loss: 0.7571 | CLoss: 0.5867 | FLoss: 0.3407 | LR: 2.86e-05\n",
      "  Batch 030/898 | Loss: 0.6595 | CLoss: 0.4833 | FLoss: 0.3525 | LR: 2.86e-05\n",
      "  Batch 040/898 | Loss: 1.0190 | CLoss: 0.8492 | FLoss: 0.3397 | LR: 2.86e-05\n",
      "  Batch 050/898 | Loss: 0.9238 | CLoss: 0.7188 | FLoss: 0.4101 | LR: 2.86e-05\n",
      "  Batch 060/898 | Loss: 0.7742 | CLoss: 0.5949 | FLoss: 0.3585 | LR: 2.86e-05\n",
      "  Batch 070/898 | Loss: 0.7111 | CLoss: 0.4913 | FLoss: 0.4396 | LR: 2.86e-05\n",
      "  Batch 080/898 | Loss: 0.5014 | CLoss: 0.3205 | FLoss: 0.3618 | LR: 2.86e-05\n",
      "  Batch 090/898 | Loss: 0.5259 | CLoss: 0.4020 | FLoss: 0.2479 | LR: 2.86e-05\n",
      "  Batch 100/898 | Loss: 0.5319 | CLoss: 0.3604 | FLoss: 0.3431 | LR: 2.86e-05\n",
      "  Batch 110/898 | Loss: 0.7545 | CLoss: 0.5715 | FLoss: 0.3661 | LR: 2.86e-05\n",
      "  Batch 120/898 | Loss: 0.5146 | CLoss: 0.3810 | FLoss: 0.2673 | LR: 2.86e-05\n",
      "  Batch 130/898 | Loss: 1.1428 | CLoss: 0.9170 | FLoss: 0.4515 | LR: 2.86e-05\n",
      "  Batch 140/898 | Loss: 0.8679 | CLoss: 0.7082 | FLoss: 0.3194 | LR: 2.86e-05\n",
      "  Batch 150/898 | Loss: 0.6305 | CLoss: 0.4117 | FLoss: 0.4376 | LR: 2.86e-05\n",
      "  Batch 160/898 | Loss: 1.0407 | CLoss: 0.8257 | FLoss: 0.4300 | LR: 2.86e-05\n",
      "  Batch 170/898 | Loss: 0.7443 | CLoss: 0.5747 | FLoss: 0.3393 | LR: 2.86e-05\n",
      "  Batch 180/898 | Loss: 0.9306 | CLoss: 0.7521 | FLoss: 0.3570 | LR: 2.86e-05\n",
      "  Batch 190/898 | Loss: 0.5696 | CLoss: 0.4168 | FLoss: 0.3057 | LR: 2.86e-05\n",
      "  Batch 200/898 | Loss: 0.7699 | CLoss: 0.6277 | FLoss: 0.2842 | LR: 2.86e-05\n",
      "  Batch 210/898 | Loss: 0.5367 | CLoss: 0.4307 | FLoss: 0.2118 | LR: 2.86e-05\n",
      "  Batch 220/898 | Loss: 1.0797 | CLoss: 0.8291 | FLoss: 0.5012 | LR: 2.86e-05\n",
      "  Batch 230/898 | Loss: 0.5779 | CLoss: 0.4522 | FLoss: 0.2515 | LR: 2.86e-05\n",
      "  Batch 240/898 | Loss: 0.3394 | CLoss: 0.2319 | FLoss: 0.2152 | LR: 2.86e-05\n",
      "  Batch 250/898 | Loss: 0.4160 | CLoss: 0.2971 | FLoss: 0.2379 | LR: 2.86e-05\n",
      "  Batch 260/898 | Loss: 0.5479 | CLoss: 0.3697 | FLoss: 0.3565 | LR: 2.86e-05\n",
      "  Batch 270/898 | Loss: 0.7716 | CLoss: 0.5820 | FLoss: 0.3792 | LR: 2.86e-05\n",
      "  Batch 280/898 | Loss: 0.6600 | CLoss: 0.4609 | FLoss: 0.3981 | LR: 2.86e-05\n",
      "  Batch 290/898 | Loss: 0.6457 | CLoss: 0.3960 | FLoss: 0.4996 | LR: 2.86e-05\n",
      "  Batch 300/898 | Loss: 0.8803 | CLoss: 0.6879 | FLoss: 0.3848 | LR: 2.86e-05\n",
      "  Batch 310/898 | Loss: 0.3051 | CLoss: 0.2021 | FLoss: 0.2061 | LR: 2.86e-05\n",
      "  Batch 320/898 | Loss: 0.6977 | CLoss: 0.5958 | FLoss: 0.2039 | LR: 2.86e-05\n",
      "  Batch 330/898 | Loss: 0.5422 | CLoss: 0.4523 | FLoss: 0.1799 | LR: 2.86e-05\n",
      "  Batch 340/898 | Loss: 0.6473 | CLoss: 0.4954 | FLoss: 0.3038 | LR: 2.86e-05\n",
      "  Batch 350/898 | Loss: 0.4700 | CLoss: 0.3327 | FLoss: 0.2745 | LR: 2.86e-05\n",
      "  Batch 360/898 | Loss: 0.6530 | CLoss: 0.4333 | FLoss: 0.4395 | LR: 2.86e-05\n",
      "  Batch 370/898 | Loss: 0.5681 | CLoss: 0.3891 | FLoss: 0.3581 | LR: 2.86e-05\n",
      "  Batch 380/898 | Loss: 0.6011 | CLoss: 0.4290 | FLoss: 0.3442 | LR: 2.86e-05\n",
      "  Batch 390/898 | Loss: 0.5694 | CLoss: 0.4602 | FLoss: 0.2185 | LR: 2.86e-05\n",
      "  Batch 400/898 | Loss: 1.1181 | CLoss: 0.8750 | FLoss: 0.4861 | LR: 2.86e-05\n",
      "  Batch 410/898 | Loss: 0.5498 | CLoss: 0.3761 | FLoss: 0.3475 | LR: 2.86e-05\n",
      "  Batch 420/898 | Loss: 0.4828 | CLoss: 0.3461 | FLoss: 0.2733 | LR: 2.86e-05\n",
      "  Batch 430/898 | Loss: 0.4675 | CLoss: 0.3561 | FLoss: 0.2229 | LR: 2.86e-05\n",
      "  Batch 440/898 | Loss: 0.8060 | CLoss: 0.5818 | FLoss: 0.4484 | LR: 2.86e-05\n",
      "  Batch 450/898 | Loss: 0.7004 | CLoss: 0.4627 | FLoss: 0.4753 | LR: 2.86e-05\n",
      "  Batch 460/898 | Loss: 0.7315 | CLoss: 0.5471 | FLoss: 0.3687 | LR: 2.86e-05\n",
      "  Batch 470/898 | Loss: 0.7838 | CLoss: 0.5805 | FLoss: 0.4067 | LR: 2.86e-05\n",
      "  Batch 480/898 | Loss: 0.8040 | CLoss: 0.6206 | FLoss: 0.3669 | LR: 2.86e-05\n",
      "  Batch 490/898 | Loss: 0.6120 | CLoss: 0.4323 | FLoss: 0.3594 | LR: 2.86e-05\n",
      "  Batch 500/898 | Loss: 0.7531 | CLoss: 0.5011 | FLoss: 0.5039 | LR: 2.86e-05\n",
      "  Batch 510/898 | Loss: 1.0716 | CLoss: 0.8908 | FLoss: 0.3616 | LR: 2.86e-05\n",
      "  Batch 520/898 | Loss: 0.9800 | CLoss: 0.8173 | FLoss: 0.3256 | LR: 2.86e-05\n",
      "  Batch 530/898 | Loss: 0.5961 | CLoss: 0.5127 | FLoss: 0.1667 | LR: 2.86e-05\n",
      "  Batch 540/898 | Loss: 0.8481 | CLoss: 0.6472 | FLoss: 0.4018 | LR: 2.86e-05\n",
      "  Batch 550/898 | Loss: 0.5451 | CLoss: 0.3990 | FLoss: 0.2920 | LR: 2.86e-05\n",
      "  Batch 560/898 | Loss: 0.4931 | CLoss: 0.3609 | FLoss: 0.2644 | LR: 2.86e-05\n",
      "  Batch 570/898 | Loss: 0.7917 | CLoss: 0.6725 | FLoss: 0.2382 | LR: 2.86e-05\n",
      "  Batch 580/898 | Loss: 0.7495 | CLoss: 0.6478 | FLoss: 0.2034 | LR: 2.86e-05\n",
      "  Batch 590/898 | Loss: 0.8424 | CLoss: 0.7285 | FLoss: 0.2278 | LR: 2.86e-05\n",
      "  Batch 600/898 | Loss: 0.5219 | CLoss: 0.3876 | FLoss: 0.2685 | LR: 2.86e-05\n",
      "  Batch 610/898 | Loss: 1.1063 | CLoss: 0.8697 | FLoss: 0.4733 | LR: 2.86e-05\n",
      "  Batch 620/898 | Loss: 0.4712 | CLoss: 0.3721 | FLoss: 0.1981 | LR: 2.86e-05\n",
      "  Batch 630/898 | Loss: 0.8410 | CLoss: 0.7218 | FLoss: 0.2384 | LR: 2.86e-05\n",
      "  Batch 640/898 | Loss: 1.0337 | CLoss: 0.8761 | FLoss: 0.3151 | LR: 2.86e-05\n",
      "  Batch 650/898 | Loss: 0.5932 | CLoss: 0.4163 | FLoss: 0.3539 | LR: 2.86e-05\n",
      "  Batch 660/898 | Loss: 0.5025 | CLoss: 0.3645 | FLoss: 0.2759 | LR: 2.86e-05\n",
      "  Batch 670/898 | Loss: 0.7503 | CLoss: 0.6362 | FLoss: 0.2284 | LR: 2.86e-05\n",
      "  Batch 680/898 | Loss: 0.5540 | CLoss: 0.4606 | FLoss: 0.1868 | LR: 2.86e-05\n",
      "  Batch 690/898 | Loss: 0.4524 | CLoss: 0.4039 | FLoss: 0.0969 | LR: 2.86e-05\n",
      "  Batch 700/898 | Loss: 0.4393 | CLoss: 0.3441 | FLoss: 0.1903 | LR: 2.86e-05\n",
      "  Batch 710/898 | Loss: 0.3143 | CLoss: 0.1688 | FLoss: 0.2908 | LR: 2.86e-05\n",
      "  Batch 720/898 | Loss: 0.6896 | CLoss: 0.5506 | FLoss: 0.2779 | LR: 2.86e-05\n",
      "  Batch 730/898 | Loss: 1.1035 | CLoss: 0.8273 | FLoss: 0.5523 | LR: 2.86e-05\n",
      "  Batch 740/898 | Loss: 1.1212 | CLoss: 0.8701 | FLoss: 0.5022 | LR: 2.86e-05\n",
      "  Batch 750/898 | Loss: 0.9153 | CLoss: 0.7183 | FLoss: 0.3940 | LR: 2.86e-05\n",
      "  Batch 760/898 | Loss: 0.7177 | CLoss: 0.5314 | FLoss: 0.3726 | LR: 2.86e-05\n",
      "  Batch 770/898 | Loss: 0.6572 | CLoss: 0.4689 | FLoss: 0.3766 | LR: 2.86e-05\n",
      "  Batch 780/898 | Loss: 0.5651 | CLoss: 0.4067 | FLoss: 0.3168 | LR: 2.86e-05\n",
      "  Batch 790/898 | Loss: 0.5554 | CLoss: 0.4626 | FLoss: 0.1856 | LR: 2.86e-05\n",
      "  Batch 800/898 | Loss: 0.5661 | CLoss: 0.4190 | FLoss: 0.2941 | LR: 2.86e-05\n",
      "  Batch 810/898 | Loss: 0.7900 | CLoss: 0.6017 | FLoss: 0.3764 | LR: 2.86e-05\n",
      "  Batch 820/898 | Loss: 0.4381 | CLoss: 0.3412 | FLoss: 0.1937 | LR: 2.86e-05\n",
      "  Batch 830/898 | Loss: 0.6023 | CLoss: 0.4447 | FLoss: 0.3152 | LR: 2.86e-05\n",
      "  Batch 840/898 | Loss: 0.6077 | CLoss: 0.4953 | FLoss: 0.2247 | LR: 2.86e-05\n",
      "  Batch 850/898 | Loss: 0.3416 | CLoss: 0.2547 | FLoss: 0.1738 | LR: 2.86e-05\n",
      "  Batch 860/898 | Loss: 0.6213 | CLoss: 0.4903 | FLoss: 0.2620 | LR: 2.86e-05\n",
      "  Batch 870/898 | Loss: 0.8422 | CLoss: 0.6518 | FLoss: 0.3808 | LR: 2.86e-05\n",
      "  Batch 880/898 | Loss: 0.6164 | CLoss: 0.4417 | FLoss: 0.3493 | LR: 2.86e-05\n",
      "  Batch 890/898 | Loss: 0.7513 | CLoss: 0.5709 | FLoss: 0.3608 | LR: 2.86e-05\n",
      "  Batch 898/898 | Loss: 0.9768 | CLoss: 0.7742 | FLoss: 0.4052 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.6825\n",
      "  Last Batch Loss: 0.9768\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.6307 | Batch Acc: 70.69%\n",
      "    Val Batch 010/101 | Loss: 0.2191 | Batch Acc: 93.10%\n",
      "    Val Batch 015/101 | Loss: 0.4572 | Batch Acc: 79.31%\n",
      "    Val Batch 020/101 | Loss: 0.7669 | Batch Acc: 68.97%\n",
      "    Val Batch 025/101 | Loss: 0.2191 | Batch Acc: 94.83%\n",
      "    Val Batch 030/101 | Loss: 0.2358 | Batch Acc: 93.10%\n",
      "    Val Batch 035/101 | Loss: 0.6159 | Batch Acc: 81.03%\n",
      "    Val Batch 040/101 | Loss: 0.6550 | Batch Acc: 62.07%\n",
      "    Val Batch 045/101 | Loss: 1.3028 | Batch Acc: 20.69%\n",
      "    Val Batch 050/101 | Loss: 0.3319 | Batch Acc: 87.93%\n",
      "    Val Batch 055/101 | Loss: 1.2175 | Batch Acc: 65.52%\n",
      "    Val Batch 060/101 | Loss: 1.0559 | Batch Acc: 68.97%\n",
      "    Val Batch 065/101 | Loss: 0.4775 | Batch Acc: 82.76%\n",
      "    Val Batch 070/101 | Loss: 0.2731 | Batch Acc: 89.66%\n",
      "    Val Batch 075/101 | Loss: 0.0926 | Batch Acc: 94.83%\n",
      "    Val Batch 080/101 | Loss: 0.2829 | Batch Acc: 93.10%\n",
      "    Val Batch 085/101 | Loss: 0.1338 | Batch Acc: 96.55%\n",
      "    Val Batch 090/101 | Loss: 0.1572 | Batch Acc: 93.10%\n",
      "    Val Batch 095/101 | Loss: 0.0587 | Batch Acc: 98.28%\n",
      "    Val Batch 100/101 | Loss: 0.1850 | Batch Acc: 91.38%\n",
      "    Val Batch 101/101 | Loss: 0.3825 | Batch Acc: 93.18%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.4167 | Accuracy: 84.02%\n",
      "  Current Best Acc: 84.26%\n",
      "\n",
      "========================================\n",
      "=== Fold 4 Completed ===\n",
      "Best Validation Accuracy: 84.26%\n",
      "\n",
      "========================================\n",
      "=== Fold 5/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/898 | Loss: 2.6234 | CLoss: 1.6586 | FLoss: 1.9296 | LR: 3.00e-04\n",
      "  Batch 020/898 | Loss: 1.8121 | CLoss: 1.0725 | FLoss: 1.4792 | LR: 3.00e-04\n",
      "  Batch 030/898 | Loss: 1.3483 | CLoss: 0.8734 | FLoss: 0.9497 | LR: 3.00e-04\n",
      "  Batch 040/898 | Loss: 0.9245 | CLoss: 0.5477 | FLoss: 0.7537 | LR: 3.00e-04\n",
      "  Batch 050/898 | Loss: 1.1562 | CLoss: 0.8168 | FLoss: 0.6788 | LR: 3.00e-04\n",
      "  Batch 060/898 | Loss: 1.3334 | CLoss: 0.9544 | FLoss: 0.7580 | LR: 3.00e-04\n",
      "  Batch 070/898 | Loss: 1.3845 | CLoss: 1.0370 | FLoss: 0.6949 | LR: 3.00e-04\n",
      "  Batch 080/898 | Loss: 0.7905 | CLoss: 0.5408 | FLoss: 0.4994 | LR: 3.00e-04\n",
      "  Batch 090/898 | Loss: 1.0245 | CLoss: 0.7143 | FLoss: 0.6203 | LR: 3.00e-04\n",
      "  Batch 100/898 | Loss: 1.1775 | CLoss: 0.9220 | FLoss: 0.5110 | LR: 3.00e-04\n",
      "  Batch 110/898 | Loss: 1.2484 | CLoss: 0.9498 | FLoss: 0.5972 | LR: 3.00e-04\n",
      "  Batch 120/898 | Loss: 0.8939 | CLoss: 0.6609 | FLoss: 0.4661 | LR: 3.00e-04\n",
      "  Batch 130/898 | Loss: 1.1346 | CLoss: 0.8518 | FLoss: 0.5656 | LR: 3.00e-04\n",
      "  Batch 140/898 | Loss: 0.8417 | CLoss: 0.6255 | FLoss: 0.4324 | LR: 3.00e-04\n",
      "  Batch 150/898 | Loss: 1.1592 | CLoss: 0.9921 | FLoss: 0.3343 | LR: 3.00e-04\n",
      "  Batch 160/898 | Loss: 1.1858 | CLoss: 0.8943 | FLoss: 0.5831 | LR: 3.00e-04\n",
      "  Batch 170/898 | Loss: 1.1763 | CLoss: 0.9296 | FLoss: 0.4933 | LR: 3.00e-04\n",
      "  Batch 180/898 | Loss: 1.2614 | CLoss: 1.0182 | FLoss: 0.4863 | LR: 3.00e-04\n",
      "  Batch 190/898 | Loss: 1.0187 | CLoss: 0.8077 | FLoss: 0.4221 | LR: 3.00e-04\n",
      "  Batch 200/898 | Loss: 1.0049 | CLoss: 0.7877 | FLoss: 0.4342 | LR: 3.00e-04\n",
      "  Batch 210/898 | Loss: 1.3698 | CLoss: 1.1016 | FLoss: 0.5364 | LR: 3.00e-04\n",
      "  Batch 220/898 | Loss: 0.6557 | CLoss: 0.5120 | FLoss: 0.2874 | LR: 3.00e-04\n",
      "  Batch 230/898 | Loss: 1.3593 | CLoss: 1.0934 | FLoss: 0.5319 | LR: 3.00e-04\n",
      "  Batch 240/898 | Loss: 0.8833 | CLoss: 0.6212 | FLoss: 0.5242 | LR: 3.00e-04\n",
      "  Batch 250/898 | Loss: 1.2549 | CLoss: 1.0091 | FLoss: 0.4916 | LR: 3.00e-04\n",
      "  Batch 260/898 | Loss: 1.2078 | CLoss: 0.9377 | FLoss: 0.5402 | LR: 3.00e-04\n",
      "  Batch 270/898 | Loss: 0.8610 | CLoss: 0.5816 | FLoss: 0.5587 | LR: 3.00e-04\n",
      "  Batch 280/898 | Loss: 0.7649 | CLoss: 0.5969 | FLoss: 0.3360 | LR: 3.00e-04\n",
      "  Batch 290/898 | Loss: 1.0631 | CLoss: 0.8280 | FLoss: 0.4702 | LR: 3.00e-04\n",
      "  Batch 300/898 | Loss: 0.9669 | CLoss: 0.7032 | FLoss: 0.5273 | LR: 3.00e-04\n",
      "  Batch 310/898 | Loss: 0.9211 | CLoss: 0.7046 | FLoss: 0.4330 | LR: 3.00e-04\n",
      "  Batch 320/898 | Loss: 0.9407 | CLoss: 0.7010 | FLoss: 0.4793 | LR: 3.00e-04\n",
      "  Batch 330/898 | Loss: 1.1159 | CLoss: 0.8938 | FLoss: 0.4441 | LR: 3.00e-04\n",
      "  Batch 340/898 | Loss: 0.8543 | CLoss: 0.6245 | FLoss: 0.4596 | LR: 3.00e-04\n",
      "  Batch 350/898 | Loss: 0.9419 | CLoss: 0.6765 | FLoss: 0.5308 | LR: 3.00e-04\n",
      "  Batch 360/898 | Loss: 1.0795 | CLoss: 0.8046 | FLoss: 0.5497 | LR: 3.00e-04\n",
      "  Batch 370/898 | Loss: 1.1825 | CLoss: 0.9385 | FLoss: 0.4880 | LR: 3.00e-04\n",
      "  Batch 380/898 | Loss: 1.2850 | CLoss: 0.9625 | FLoss: 0.6449 | LR: 3.00e-04\n",
      "  Batch 390/898 | Loss: 1.0603 | CLoss: 0.7806 | FLoss: 0.5594 | LR: 3.00e-04\n",
      "  Batch 400/898 | Loss: 0.8680 | CLoss: 0.6497 | FLoss: 0.4366 | LR: 3.00e-04\n",
      "  Batch 410/898 | Loss: 1.4399 | CLoss: 1.1725 | FLoss: 0.5348 | LR: 3.00e-04\n",
      "  Batch 420/898 | Loss: 0.9835 | CLoss: 0.7640 | FLoss: 0.4390 | LR: 3.00e-04\n",
      "  Batch 430/898 | Loss: 0.8450 | CLoss: 0.6338 | FLoss: 0.4222 | LR: 3.00e-04\n",
      "  Batch 440/898 | Loss: 0.8762 | CLoss: 0.6694 | FLoss: 0.4136 | LR: 3.00e-04\n",
      "  Batch 450/898 | Loss: 1.1920 | CLoss: 0.9496 | FLoss: 0.4848 | LR: 3.00e-04\n",
      "  Batch 460/898 | Loss: 1.0164 | CLoss: 0.7991 | FLoss: 0.4346 | LR: 3.00e-04\n",
      "  Batch 470/898 | Loss: 1.2742 | CLoss: 1.0374 | FLoss: 0.4737 | LR: 3.00e-04\n",
      "  Batch 480/898 | Loss: 1.0176 | CLoss: 0.8067 | FLoss: 0.4218 | LR: 3.00e-04\n",
      "  Batch 490/898 | Loss: 1.1837 | CLoss: 0.9381 | FLoss: 0.4911 | LR: 3.00e-04\n",
      "  Batch 500/898 | Loss: 1.1112 | CLoss: 0.8306 | FLoss: 0.5613 | LR: 3.00e-04\n",
      "  Batch 510/898 | Loss: 0.6090 | CLoss: 0.4071 | FLoss: 0.4037 | LR: 3.00e-04\n",
      "  Batch 520/898 | Loss: 1.4554 | CLoss: 1.1855 | FLoss: 0.5398 | LR: 3.00e-04\n",
      "  Batch 530/898 | Loss: 0.6214 | CLoss: 0.4591 | FLoss: 0.3247 | LR: 3.00e-04\n",
      "  Batch 540/898 | Loss: 0.7587 | CLoss: 0.5246 | FLoss: 0.4682 | LR: 3.00e-04\n",
      "  Batch 550/898 | Loss: 1.0118 | CLoss: 0.6918 | FLoss: 0.6400 | LR: 3.00e-04\n",
      "  Batch 560/898 | Loss: 0.9759 | CLoss: 0.7204 | FLoss: 0.5110 | LR: 3.00e-04\n",
      "  Batch 570/898 | Loss: 1.0436 | CLoss: 0.8044 | FLoss: 0.4784 | LR: 3.00e-04\n",
      "  Batch 580/898 | Loss: 0.6504 | CLoss: 0.4452 | FLoss: 0.4104 | LR: 3.00e-04\n",
      "  Batch 590/898 | Loss: 1.0994 | CLoss: 0.8796 | FLoss: 0.4396 | LR: 3.00e-04\n",
      "  Batch 600/898 | Loss: 0.9265 | CLoss: 0.7283 | FLoss: 0.3964 | LR: 3.00e-04\n",
      "  Batch 610/898 | Loss: 0.9602 | CLoss: 0.6854 | FLoss: 0.5494 | LR: 3.00e-04\n",
      "  Batch 620/898 | Loss: 1.1907 | CLoss: 0.9945 | FLoss: 0.3925 | LR: 3.00e-04\n",
      "  Batch 630/898 | Loss: 0.7557 | CLoss: 0.5380 | FLoss: 0.4354 | LR: 3.00e-04\n",
      "  Batch 640/898 | Loss: 1.0375 | CLoss: 0.8446 | FLoss: 0.3859 | LR: 3.00e-04\n",
      "  Batch 650/898 | Loss: 1.4101 | CLoss: 1.2100 | FLoss: 0.4002 | LR: 3.00e-04\n",
      "  Batch 660/898 | Loss: 0.8978 | CLoss: 0.6368 | FLoss: 0.5219 | LR: 3.00e-04\n",
      "  Batch 670/898 | Loss: 0.9478 | CLoss: 0.7050 | FLoss: 0.4855 | LR: 3.00e-04\n",
      "  Batch 680/898 | Loss: 1.2261 | CLoss: 0.8535 | FLoss: 0.7453 | LR: 3.00e-04\n",
      "  Batch 690/898 | Loss: 1.3024 | CLoss: 0.9052 | FLoss: 0.7943 | LR: 3.00e-04\n",
      "  Batch 700/898 | Loss: 1.3754 | CLoss: 1.0336 | FLoss: 0.6836 | LR: 3.00e-04\n",
      "  Batch 710/898 | Loss: 0.8497 | CLoss: 0.6035 | FLoss: 0.4923 | LR: 3.00e-04\n",
      "  Batch 720/898 | Loss: 1.0837 | CLoss: 0.9036 | FLoss: 0.3601 | LR: 3.00e-04\n",
      "  Batch 730/898 | Loss: 0.7726 | CLoss: 0.6479 | FLoss: 0.2493 | LR: 3.00e-04\n",
      "  Batch 740/898 | Loss: 0.8093 | CLoss: 0.5957 | FLoss: 0.4273 | LR: 3.00e-04\n",
      "  Batch 750/898 | Loss: 1.0332 | CLoss: 0.8097 | FLoss: 0.4470 | LR: 3.00e-04\n",
      "  Batch 760/898 | Loss: 0.7074 | CLoss: 0.5265 | FLoss: 0.3617 | LR: 3.00e-04\n",
      "  Batch 770/898 | Loss: 1.0430 | CLoss: 0.7423 | FLoss: 0.6014 | LR: 3.00e-04\n",
      "  Batch 780/898 | Loss: 1.0597 | CLoss: 0.7618 | FLoss: 0.5957 | LR: 3.00e-04\n",
      "  Batch 790/898 | Loss: 1.1797 | CLoss: 0.9470 | FLoss: 0.4655 | LR: 3.00e-04\n",
      "  Batch 800/898 | Loss: 0.9606 | CLoss: 0.7706 | FLoss: 0.3800 | LR: 3.00e-04\n",
      "  Batch 810/898 | Loss: 0.7898 | CLoss: 0.6250 | FLoss: 0.3296 | LR: 3.00e-04\n",
      "  Batch 820/898 | Loss: 1.2142 | CLoss: 0.8857 | FLoss: 0.6570 | LR: 3.00e-04\n",
      "  Batch 830/898 | Loss: 0.8172 | CLoss: 0.6012 | FLoss: 0.4319 | LR: 3.00e-04\n",
      "  Batch 840/898 | Loss: 1.0772 | CLoss: 0.8690 | FLoss: 0.4163 | LR: 3.00e-04\n",
      "  Batch 850/898 | Loss: 1.1538 | CLoss: 0.9296 | FLoss: 0.4483 | LR: 3.00e-04\n",
      "  Batch 860/898 | Loss: 0.7432 | CLoss: 0.5813 | FLoss: 0.3238 | LR: 3.00e-04\n",
      "  Batch 870/898 | Loss: 0.9910 | CLoss: 0.7209 | FLoss: 0.5402 | LR: 3.00e-04\n",
      "  Batch 880/898 | Loss: 1.4974 | CLoss: 1.1910 | FLoss: 0.6128 | LR: 3.00e-04\n",
      "  Batch 890/898 | Loss: 1.2674 | CLoss: 0.9767 | FLoss: 0.5814 | LR: 3.00e-04\n",
      "  Batch 898/898 | Loss: 0.9615 | CLoss: 0.5928 | FLoss: 0.7374 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 1.0493\n",
      "  Last Batch Loss: 0.9615\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 1.5300 | Batch Acc: 51.72%\n",
      "    Val Batch 010/102 | Loss: 0.0812 | Batch Acc: 98.28%\n",
      "    Val Batch 015/102 | Loss: 0.2924 | Batch Acc: 93.10%\n",
      "    Val Batch 020/102 | Loss: 1.9140 | Batch Acc: 34.48%\n",
      "    Val Batch 025/102 | Loss: 0.2695 | Batch Acc: 87.93%\n",
      "    Val Batch 030/102 | Loss: 0.6389 | Batch Acc: 72.41%\n",
      "    Val Batch 035/102 | Loss: 0.6461 | Batch Acc: 72.41%\n",
      "    Val Batch 040/102 | Loss: 0.6899 | Batch Acc: 98.28%\n",
      "    Val Batch 045/102 | Loss: 0.6340 | Batch Acc: 100.00%\n",
      "    Val Batch 050/102 | Loss: 1.1646 | Batch Acc: 60.34%\n",
      "    Val Batch 055/102 | Loss: 0.9250 | Batch Acc: 72.41%\n",
      "    Val Batch 060/102 | Loss: 0.9162 | Batch Acc: 72.41%\n",
      "    Val Batch 065/102 | Loss: 0.7856 | Batch Acc: 68.97%\n",
      "    Val Batch 070/102 | Loss: 0.7255 | Batch Acc: 67.24%\n",
      "    Val Batch 075/102 | Loss: 0.1440 | Batch Acc: 96.55%\n",
      "    Val Batch 080/102 | Loss: 0.3817 | Batch Acc: 91.38%\n",
      "    Val Batch 085/102 | Loss: 0.1896 | Batch Acc: 93.10%\n",
      "    Val Batch 090/102 | Loss: 0.4019 | Batch Acc: 89.66%\n",
      "    Val Batch 095/102 | Loss: 0.1485 | Batch Acc: 96.55%\n",
      "    Val Batch 100/102 | Loss: 0.1371 | Batch Acc: 96.55%\n",
      "    Val Batch 102/102 | Loss: 0.1736 | Batch Acc: 90.00%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.7172 | Accuracy: 76.81%\n",
      "  Current Best Acc: 76.81%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/898 | Loss: 0.8958 | CLoss: 0.6764 | FLoss: 0.4388 | LR: 2.71e-04\n",
      "  Batch 020/898 | Loss: 0.7274 | CLoss: 0.6104 | FLoss: 0.2340 | LR: 2.71e-04\n",
      "  Batch 030/898 | Loss: 0.8981 | CLoss: 0.7665 | FLoss: 0.2632 | LR: 2.71e-04\n",
      "  Batch 040/898 | Loss: 0.8946 | CLoss: 0.6947 | FLoss: 0.3997 | LR: 2.71e-04\n",
      "  Batch 050/898 | Loss: 0.9895 | CLoss: 0.7091 | FLoss: 0.5609 | LR: 2.71e-04\n",
      "  Batch 060/898 | Loss: 0.8102 | CLoss: 0.6557 | FLoss: 0.3092 | LR: 2.71e-04\n",
      "  Batch 070/898 | Loss: 0.7775 | CLoss: 0.6373 | FLoss: 0.2804 | LR: 2.71e-04\n",
      "  Batch 080/898 | Loss: 1.1535 | CLoss: 0.9271 | FLoss: 0.4528 | LR: 2.71e-04\n",
      "  Batch 090/898 | Loss: 1.3396 | CLoss: 1.1337 | FLoss: 0.4119 | LR: 2.71e-04\n",
      "  Batch 100/898 | Loss: 0.8253 | CLoss: 0.7025 | FLoss: 0.2456 | LR: 2.71e-04\n",
      "  Batch 110/898 | Loss: 0.6439 | CLoss: 0.4775 | FLoss: 0.3327 | LR: 2.71e-04\n",
      "  Batch 120/898 | Loss: 0.4718 | CLoss: 0.2369 | FLoss: 0.4698 | LR: 2.71e-04\n",
      "  Batch 130/898 | Loss: 0.7520 | CLoss: 0.5070 | FLoss: 0.4901 | LR: 2.71e-04\n",
      "  Batch 140/898 | Loss: 1.1010 | CLoss: 0.8546 | FLoss: 0.4929 | LR: 2.71e-04\n",
      "  Batch 150/898 | Loss: 0.9431 | CLoss: 0.7069 | FLoss: 0.4724 | LR: 2.71e-04\n",
      "  Batch 160/898 | Loss: 0.9717 | CLoss: 0.7831 | FLoss: 0.3772 | LR: 2.71e-04\n",
      "  Batch 170/898 | Loss: 0.8227 | CLoss: 0.5808 | FLoss: 0.4838 | LR: 2.71e-04\n",
      "  Batch 180/898 | Loss: 0.4690 | CLoss: 0.3417 | FLoss: 0.2545 | LR: 2.71e-04\n",
      "  Batch 190/898 | Loss: 1.0134 | CLoss: 0.7897 | FLoss: 0.4475 | LR: 2.71e-04\n",
      "  Batch 200/898 | Loss: 0.8213 | CLoss: 0.6319 | FLoss: 0.3788 | LR: 2.71e-04\n",
      "  Batch 210/898 | Loss: 0.8892 | CLoss: 0.7211 | FLoss: 0.3362 | LR: 2.71e-04\n",
      "  Batch 220/898 | Loss: 0.6413 | CLoss: 0.4664 | FLoss: 0.3498 | LR: 2.71e-04\n",
      "  Batch 230/898 | Loss: 1.0763 | CLoss: 0.8508 | FLoss: 0.4511 | LR: 2.71e-04\n",
      "  Batch 240/898 | Loss: 1.0163 | CLoss: 0.8353 | FLoss: 0.3619 | LR: 2.71e-04\n",
      "  Batch 250/898 | Loss: 1.3859 | CLoss: 1.2014 | FLoss: 0.3689 | LR: 2.71e-04\n",
      "  Batch 260/898 | Loss: 0.8962 | CLoss: 0.6820 | FLoss: 0.4284 | LR: 2.71e-04\n",
      "  Batch 270/898 | Loss: 0.7546 | CLoss: 0.4724 | FLoss: 0.5644 | LR: 2.71e-04\n",
      "  Batch 280/898 | Loss: 0.9184 | CLoss: 0.7255 | FLoss: 0.3858 | LR: 2.71e-04\n",
      "  Batch 290/898 | Loss: 0.7599 | CLoss: 0.5975 | FLoss: 0.3248 | LR: 2.71e-04\n",
      "  Batch 300/898 | Loss: 1.3250 | CLoss: 1.0011 | FLoss: 0.6480 | LR: 2.71e-04\n",
      "  Batch 310/898 | Loss: 0.6975 | CLoss: 0.5095 | FLoss: 0.3759 | LR: 2.71e-04\n",
      "  Batch 320/898 | Loss: 0.9668 | CLoss: 0.8283 | FLoss: 0.2770 | LR: 2.71e-04\n",
      "  Batch 330/898 | Loss: 0.5495 | CLoss: 0.3050 | FLoss: 0.4888 | LR: 2.71e-04\n",
      "  Batch 340/898 | Loss: 1.0048 | CLoss: 0.7222 | FLoss: 0.5653 | LR: 2.71e-04\n",
      "  Batch 350/898 | Loss: 0.7244 | CLoss: 0.5183 | FLoss: 0.4122 | LR: 2.71e-04\n",
      "  Batch 360/898 | Loss: 1.0837 | CLoss: 0.8558 | FLoss: 0.4558 | LR: 2.71e-04\n",
      "  Batch 370/898 | Loss: 1.3706 | CLoss: 1.0307 | FLoss: 0.6798 | LR: 2.71e-04\n",
      "  Batch 380/898 | Loss: 0.6600 | CLoss: 0.4159 | FLoss: 0.4881 | LR: 2.71e-04\n",
      "  Batch 390/898 | Loss: 0.8226 | CLoss: 0.6462 | FLoss: 0.3528 | LR: 2.71e-04\n",
      "  Batch 400/898 | Loss: 1.1221 | CLoss: 0.8917 | FLoss: 0.4609 | LR: 2.71e-04\n",
      "  Batch 410/898 | Loss: 0.8776 | CLoss: 0.6638 | FLoss: 0.4276 | LR: 2.71e-04\n",
      "  Batch 420/898 | Loss: 0.6396 | CLoss: 0.4893 | FLoss: 0.3006 | LR: 2.71e-04\n",
      "  Batch 430/898 | Loss: 0.8870 | CLoss: 0.7049 | FLoss: 0.3643 | LR: 2.71e-04\n",
      "  Batch 440/898 | Loss: 1.2686 | CLoss: 1.0233 | FLoss: 0.4907 | LR: 2.71e-04\n",
      "  Batch 450/898 | Loss: 0.6901 | CLoss: 0.5231 | FLoss: 0.3340 | LR: 2.71e-04\n",
      "  Batch 460/898 | Loss: 0.6732 | CLoss: 0.4594 | FLoss: 0.4276 | LR: 2.71e-04\n",
      "  Batch 470/898 | Loss: 0.8361 | CLoss: 0.6095 | FLoss: 0.4530 | LR: 2.71e-04\n",
      "  Batch 480/898 | Loss: 1.5356 | CLoss: 1.2157 | FLoss: 0.6399 | LR: 2.71e-04\n",
      "  Batch 490/898 | Loss: 0.5684 | CLoss: 0.4560 | FLoss: 0.2248 | LR: 2.71e-04\n",
      "  Batch 500/898 | Loss: 1.2953 | CLoss: 1.0949 | FLoss: 0.4008 | LR: 2.71e-04\n",
      "  Batch 510/898 | Loss: 1.5564 | CLoss: 1.3114 | FLoss: 0.4901 | LR: 2.71e-04\n",
      "  Batch 520/898 | Loss: 1.1655 | CLoss: 0.9199 | FLoss: 0.4913 | LR: 2.71e-04\n",
      "  Batch 530/898 | Loss: 1.0222 | CLoss: 0.7491 | FLoss: 0.5462 | LR: 2.71e-04\n",
      "  Batch 540/898 | Loss: 0.8341 | CLoss: 0.6087 | FLoss: 0.4508 | LR: 2.71e-04\n",
      "  Batch 550/898 | Loss: 1.2139 | CLoss: 1.0169 | FLoss: 0.3940 | LR: 2.71e-04\n",
      "  Batch 560/898 | Loss: 0.7102 | CLoss: 0.4770 | FLoss: 0.4664 | LR: 2.71e-04\n",
      "  Batch 570/898 | Loss: 1.1710 | CLoss: 0.8874 | FLoss: 0.5672 | LR: 2.71e-04\n",
      "  Batch 580/898 | Loss: 0.8265 | CLoss: 0.6582 | FLoss: 0.3365 | LR: 2.71e-04\n",
      "  Batch 590/898 | Loss: 0.9584 | CLoss: 0.7393 | FLoss: 0.4383 | LR: 2.71e-04\n",
      "  Batch 600/898 | Loss: 1.2884 | CLoss: 1.0274 | FLoss: 0.5219 | LR: 2.71e-04\n",
      "  Batch 610/898 | Loss: 0.5654 | CLoss: 0.3698 | FLoss: 0.3912 | LR: 2.71e-04\n",
      "  Batch 620/898 | Loss: 1.1332 | CLoss: 0.8732 | FLoss: 0.5199 | LR: 2.71e-04\n",
      "  Batch 630/898 | Loss: 1.2233 | CLoss: 1.0035 | FLoss: 0.4396 | LR: 2.71e-04\n",
      "  Batch 640/898 | Loss: 0.4648 | CLoss: 0.3197 | FLoss: 0.2901 | LR: 2.71e-04\n",
      "  Batch 650/898 | Loss: 0.6390 | CLoss: 0.4731 | FLoss: 0.3317 | LR: 2.71e-04\n",
      "  Batch 660/898 | Loss: 0.9305 | CLoss: 0.7144 | FLoss: 0.4322 | LR: 2.71e-04\n",
      "  Batch 670/898 | Loss: 0.4745 | CLoss: 0.3446 | FLoss: 0.2597 | LR: 2.71e-04\n",
      "  Batch 680/898 | Loss: 0.9982 | CLoss: 0.7805 | FLoss: 0.4354 | LR: 2.71e-04\n",
      "  Batch 690/898 | Loss: 1.1744 | CLoss: 0.9468 | FLoss: 0.4552 | LR: 2.71e-04\n",
      "  Batch 700/898 | Loss: 0.7093 | CLoss: 0.5937 | FLoss: 0.2312 | LR: 2.71e-04\n",
      "  Batch 710/898 | Loss: 1.0215 | CLoss: 0.6960 | FLoss: 0.6509 | LR: 2.71e-04\n",
      "  Batch 720/898 | Loss: 1.2184 | CLoss: 0.9928 | FLoss: 0.4513 | LR: 2.71e-04\n",
      "  Batch 730/898 | Loss: 0.9897 | CLoss: 0.8090 | FLoss: 0.3614 | LR: 2.71e-04\n",
      "  Batch 740/898 | Loss: 0.6997 | CLoss: 0.5213 | FLoss: 0.3566 | LR: 2.71e-04\n",
      "  Batch 750/898 | Loss: 0.6807 | CLoss: 0.5406 | FLoss: 0.2802 | LR: 2.71e-04\n",
      "  Batch 760/898 | Loss: 0.5520 | CLoss: 0.3815 | FLoss: 0.3410 | LR: 2.71e-04\n",
      "  Batch 770/898 | Loss: 0.5370 | CLoss: 0.3939 | FLoss: 0.2862 | LR: 2.71e-04\n",
      "  Batch 780/898 | Loss: 1.0581 | CLoss: 0.8411 | FLoss: 0.4338 | LR: 2.71e-04\n",
      "  Batch 790/898 | Loss: 1.0603 | CLoss: 0.8281 | FLoss: 0.4644 | LR: 2.71e-04\n",
      "  Batch 800/898 | Loss: 1.1021 | CLoss: 0.8577 | FLoss: 0.4888 | LR: 2.71e-04\n",
      "  Batch 810/898 | Loss: 0.7567 | CLoss: 0.5617 | FLoss: 0.3899 | LR: 2.71e-04\n",
      "  Batch 820/898 | Loss: 0.7828 | CLoss: 0.6507 | FLoss: 0.2642 | LR: 2.71e-04\n",
      "  Batch 830/898 | Loss: 0.8213 | CLoss: 0.6606 | FLoss: 0.3213 | LR: 2.71e-04\n",
      "  Batch 840/898 | Loss: 0.4654 | CLoss: 0.3461 | FLoss: 0.2385 | LR: 2.71e-04\n",
      "  Batch 850/898 | Loss: 1.2979 | CLoss: 0.9595 | FLoss: 0.6769 | LR: 2.71e-04\n",
      "  Batch 860/898 | Loss: 0.8853 | CLoss: 0.6211 | FLoss: 0.5284 | LR: 2.71e-04\n",
      "  Batch 870/898 | Loss: 0.5035 | CLoss: 0.3850 | FLoss: 0.2370 | LR: 2.71e-04\n",
      "  Batch 880/898 | Loss: 1.2544 | CLoss: 0.9612 | FLoss: 0.5865 | LR: 2.71e-04\n",
      "  Batch 890/898 | Loss: 0.6093 | CLoss: 0.4441 | FLoss: 0.3304 | LR: 2.71e-04\n",
      "  Batch 898/898 | Loss: 0.9406 | CLoss: 0.5508 | FLoss: 0.7797 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.9130\n",
      "  Last Batch Loss: 0.9406\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 1.3244 | Batch Acc: 43.10%\n",
      "    Val Batch 010/102 | Loss: 0.3098 | Batch Acc: 93.10%\n",
      "    Val Batch 015/102 | Loss: 0.1937 | Batch Acc: 96.55%\n",
      "    Val Batch 020/102 | Loss: 0.8301 | Batch Acc: 75.86%\n",
      "    Val Batch 025/102 | Loss: 0.2610 | Batch Acc: 91.38%\n",
      "    Val Batch 030/102 | Loss: 0.3520 | Batch Acc: 91.38%\n",
      "    Val Batch 035/102 | Loss: 0.4827 | Batch Acc: 82.76%\n",
      "    Val Batch 040/102 | Loss: 0.8829 | Batch Acc: 43.10%\n",
      "    Val Batch 045/102 | Loss: 1.0150 | Batch Acc: 32.76%\n",
      "    Val Batch 050/102 | Loss: 1.1976 | Batch Acc: 60.34%\n",
      "    Val Batch 055/102 | Loss: 0.7151 | Batch Acc: 74.14%\n",
      "    Val Batch 060/102 | Loss: 1.1978 | Batch Acc: 63.79%\n",
      "    Val Batch 065/102 | Loss: 0.9367 | Batch Acc: 68.97%\n",
      "    Val Batch 070/102 | Loss: 1.1474 | Batch Acc: 56.90%\n",
      "    Val Batch 075/102 | Loss: 0.3155 | Batch Acc: 91.38%\n",
      "    Val Batch 080/102 | Loss: 0.5206 | Batch Acc: 84.48%\n",
      "    Val Batch 085/102 | Loss: 0.2096 | Batch Acc: 94.83%\n",
      "    Val Batch 090/102 | Loss: 0.2942 | Batch Acc: 89.66%\n",
      "    Val Batch 095/102 | Loss: 0.1724 | Batch Acc: 94.83%\n",
      "    Val Batch 100/102 | Loss: 0.2025 | Batch Acc: 94.83%\n",
      "    Val Batch 102/102 | Loss: 0.3202 | Batch Acc: 90.00%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.6255 | Accuracy: 77.37%\n",
      "  Current Best Acc: 77.37%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/898 | Loss: 0.7791 | CLoss: 0.6716 | FLoss: 0.2151 | LR: 1.96e-04\n",
      "  Batch 020/898 | Loss: 0.7560 | CLoss: 0.6243 | FLoss: 0.2634 | LR: 1.96e-04\n",
      "  Batch 030/898 | Loss: 0.7152 | CLoss: 0.5243 | FLoss: 0.3818 | LR: 1.96e-04\n",
      "  Batch 040/898 | Loss: 1.1829 | CLoss: 0.9732 | FLoss: 0.4192 | LR: 1.96e-04\n",
      "  Batch 050/898 | Loss: 0.9491 | CLoss: 0.7923 | FLoss: 0.3136 | LR: 1.96e-04\n",
      "  Batch 060/898 | Loss: 0.9248 | CLoss: 0.6768 | FLoss: 0.4959 | LR: 1.96e-04\n",
      "  Batch 070/898 | Loss: 0.6532 | CLoss: 0.4849 | FLoss: 0.3368 | LR: 1.96e-04\n",
      "  Batch 080/898 | Loss: 0.7376 | CLoss: 0.5976 | FLoss: 0.2801 | LR: 1.96e-04\n",
      "  Batch 090/898 | Loss: 1.2461 | CLoss: 1.0193 | FLoss: 0.4536 | LR: 1.96e-04\n",
      "  Batch 100/898 | Loss: 0.5697 | CLoss: 0.4324 | FLoss: 0.2745 | LR: 1.96e-04\n",
      "  Batch 110/898 | Loss: 0.4199 | CLoss: 0.3161 | FLoss: 0.2076 | LR: 1.96e-04\n",
      "  Batch 120/898 | Loss: 0.6118 | CLoss: 0.4786 | FLoss: 0.2663 | LR: 1.96e-04\n",
      "  Batch 130/898 | Loss: 1.1706 | CLoss: 0.8713 | FLoss: 0.5986 | LR: 1.96e-04\n",
      "  Batch 140/898 | Loss: 0.7330 | CLoss: 0.5385 | FLoss: 0.3889 | LR: 1.96e-04\n",
      "  Batch 150/898 | Loss: 0.4394 | CLoss: 0.3106 | FLoss: 0.2576 | LR: 1.96e-04\n",
      "  Batch 160/898 | Loss: 0.5207 | CLoss: 0.4003 | FLoss: 0.2409 | LR: 1.96e-04\n",
      "  Batch 170/898 | Loss: 0.5672 | CLoss: 0.4416 | FLoss: 0.2512 | LR: 1.96e-04\n",
      "  Batch 180/898 | Loss: 0.6767 | CLoss: 0.5244 | FLoss: 0.3046 | LR: 1.96e-04\n",
      "  Batch 190/898 | Loss: 0.7586 | CLoss: 0.5748 | FLoss: 0.3675 | LR: 1.96e-04\n",
      "  Batch 200/898 | Loss: 0.7895 | CLoss: 0.5536 | FLoss: 0.4719 | LR: 1.96e-04\n",
      "  Batch 210/898 | Loss: 0.6434 | CLoss: 0.4961 | FLoss: 0.2946 | LR: 1.96e-04\n",
      "  Batch 220/898 | Loss: 0.9641 | CLoss: 0.7936 | FLoss: 0.3410 | LR: 1.96e-04\n",
      "  Batch 230/898 | Loss: 0.8561 | CLoss: 0.7138 | FLoss: 0.2847 | LR: 1.96e-04\n",
      "  Batch 240/898 | Loss: 0.6959 | CLoss: 0.5469 | FLoss: 0.2981 | LR: 1.96e-04\n",
      "  Batch 250/898 | Loss: 0.4806 | CLoss: 0.2873 | FLoss: 0.3866 | LR: 1.96e-04\n",
      "  Batch 260/898 | Loss: 1.0502 | CLoss: 0.8363 | FLoss: 0.4277 | LR: 1.96e-04\n",
      "  Batch 270/898 | Loss: 0.4060 | CLoss: 0.2891 | FLoss: 0.2337 | LR: 1.96e-04\n",
      "  Batch 280/898 | Loss: 0.5305 | CLoss: 0.4082 | FLoss: 0.2446 | LR: 1.96e-04\n",
      "  Batch 290/898 | Loss: 0.8994 | CLoss: 0.7071 | FLoss: 0.3845 | LR: 1.96e-04\n",
      "  Batch 300/898 | Loss: 1.4577 | CLoss: 1.1406 | FLoss: 0.6341 | LR: 1.96e-04\n",
      "  Batch 310/898 | Loss: 0.6280 | CLoss: 0.4961 | FLoss: 0.2637 | LR: 1.96e-04\n",
      "  Batch 320/898 | Loss: 1.0518 | CLoss: 0.8297 | FLoss: 0.4440 | LR: 1.96e-04\n",
      "  Batch 330/898 | Loss: 0.5121 | CLoss: 0.3199 | FLoss: 0.3844 | LR: 1.96e-04\n",
      "  Batch 340/898 | Loss: 0.5357 | CLoss: 0.4137 | FLoss: 0.2440 | LR: 1.96e-04\n",
      "  Batch 350/898 | Loss: 0.7918 | CLoss: 0.5705 | FLoss: 0.4426 | LR: 1.96e-04\n",
      "  Batch 360/898 | Loss: 0.8945 | CLoss: 0.6199 | FLoss: 0.5492 | LR: 1.96e-04\n",
      "  Batch 370/898 | Loss: 0.7018 | CLoss: 0.5544 | FLoss: 0.2949 | LR: 1.96e-04\n",
      "  Batch 380/898 | Loss: 1.0057 | CLoss: 0.7223 | FLoss: 0.5669 | LR: 1.96e-04\n",
      "  Batch 390/898 | Loss: 0.5949 | CLoss: 0.3857 | FLoss: 0.4184 | LR: 1.96e-04\n",
      "  Batch 400/898 | Loss: 0.7774 | CLoss: 0.6349 | FLoss: 0.2848 | LR: 1.96e-04\n",
      "  Batch 410/898 | Loss: 1.0682 | CLoss: 0.7785 | FLoss: 0.5793 | LR: 1.96e-04\n",
      "  Batch 420/898 | Loss: 0.6698 | CLoss: 0.4821 | FLoss: 0.3754 | LR: 1.96e-04\n",
      "  Batch 430/898 | Loss: 1.1017 | CLoss: 0.9262 | FLoss: 0.3510 | LR: 1.96e-04\n",
      "  Batch 440/898 | Loss: 0.4462 | CLoss: 0.3487 | FLoss: 0.1950 | LR: 1.96e-04\n",
      "  Batch 450/898 | Loss: 0.6085 | CLoss: 0.4411 | FLoss: 0.3347 | LR: 1.96e-04\n",
      "  Batch 460/898 | Loss: 0.9077 | CLoss: 0.7230 | FLoss: 0.3693 | LR: 1.96e-04\n",
      "  Batch 470/898 | Loss: 0.7732 | CLoss: 0.5401 | FLoss: 0.4663 | LR: 1.96e-04\n",
      "  Batch 480/898 | Loss: 0.8935 | CLoss: 0.6429 | FLoss: 0.5011 | LR: 1.96e-04\n",
      "  Batch 490/898 | Loss: 0.9231 | CLoss: 0.6798 | FLoss: 0.4866 | LR: 1.96e-04\n",
      "  Batch 500/898 | Loss: 0.5204 | CLoss: 0.3207 | FLoss: 0.3996 | LR: 1.96e-04\n",
      "  Batch 510/898 | Loss: 0.6211 | CLoss: 0.4367 | FLoss: 0.3688 | LR: 1.96e-04\n",
      "  Batch 520/898 | Loss: 0.8768 | CLoss: 0.6356 | FLoss: 0.4824 | LR: 1.96e-04\n",
      "  Batch 530/898 | Loss: 0.4972 | CLoss: 0.4482 | FLoss: 0.0981 | LR: 1.96e-04\n",
      "  Batch 540/898 | Loss: 0.4517 | CLoss: 0.3181 | FLoss: 0.2670 | LR: 1.96e-04\n",
      "  Batch 550/898 | Loss: 1.0386 | CLoss: 0.8310 | FLoss: 0.4151 | LR: 1.96e-04\n",
      "  Batch 560/898 | Loss: 0.7160 | CLoss: 0.5705 | FLoss: 0.2910 | LR: 1.96e-04\n",
      "  Batch 570/898 | Loss: 0.7580 | CLoss: 0.5362 | FLoss: 0.4436 | LR: 1.96e-04\n",
      "  Batch 580/898 | Loss: 0.4821 | CLoss: 0.3531 | FLoss: 0.2581 | LR: 1.96e-04\n",
      "  Batch 590/898 | Loss: 0.4407 | CLoss: 0.3607 | FLoss: 0.1599 | LR: 1.96e-04\n",
      "  Batch 600/898 | Loss: 0.6197 | CLoss: 0.4611 | FLoss: 0.3173 | LR: 1.96e-04\n",
      "  Batch 610/898 | Loss: 1.1605 | CLoss: 0.8734 | FLoss: 0.5743 | LR: 1.96e-04\n",
      "  Batch 620/898 | Loss: 0.4635 | CLoss: 0.3081 | FLoss: 0.3107 | LR: 1.96e-04\n",
      "  Batch 630/898 | Loss: 0.6060 | CLoss: 0.4840 | FLoss: 0.2440 | LR: 1.96e-04\n",
      "  Batch 640/898 | Loss: 1.0270 | CLoss: 0.8645 | FLoss: 0.3250 | LR: 1.96e-04\n",
      "  Batch 650/898 | Loss: 0.7173 | CLoss: 0.4950 | FLoss: 0.4446 | LR: 1.96e-04\n",
      "  Batch 660/898 | Loss: 1.0470 | CLoss: 0.8089 | FLoss: 0.4761 | LR: 1.96e-04\n",
      "  Batch 670/898 | Loss: 0.7597 | CLoss: 0.6513 | FLoss: 0.2168 | LR: 1.96e-04\n",
      "  Batch 680/898 | Loss: 0.6676 | CLoss: 0.5533 | FLoss: 0.2286 | LR: 1.96e-04\n",
      "  Batch 690/898 | Loss: 0.6336 | CLoss: 0.4365 | FLoss: 0.3942 | LR: 1.96e-04\n",
      "  Batch 700/898 | Loss: 0.5811 | CLoss: 0.3689 | FLoss: 0.4244 | LR: 1.96e-04\n",
      "  Batch 710/898 | Loss: 1.3556 | CLoss: 1.0660 | FLoss: 0.5792 | LR: 1.96e-04\n",
      "  Batch 720/898 | Loss: 0.5384 | CLoss: 0.3684 | FLoss: 0.3400 | LR: 1.96e-04\n",
      "  Batch 730/898 | Loss: 0.7279 | CLoss: 0.6120 | FLoss: 0.2319 | LR: 1.96e-04\n",
      "  Batch 740/898 | Loss: 0.7929 | CLoss: 0.7113 | FLoss: 0.1631 | LR: 1.96e-04\n",
      "  Batch 750/898 | Loss: 0.4451 | CLoss: 0.3259 | FLoss: 0.2385 | LR: 1.96e-04\n",
      "  Batch 760/898 | Loss: 1.1116 | CLoss: 0.8862 | FLoss: 0.4507 | LR: 1.96e-04\n",
      "  Batch 770/898 | Loss: 0.9528 | CLoss: 0.7438 | FLoss: 0.4179 | LR: 1.96e-04\n",
      "  Batch 780/898 | Loss: 0.6371 | CLoss: 0.5478 | FLoss: 0.1786 | LR: 1.96e-04\n",
      "  Batch 790/898 | Loss: 1.2883 | CLoss: 1.0148 | FLoss: 0.5469 | LR: 1.96e-04\n",
      "  Batch 800/898 | Loss: 0.7572 | CLoss: 0.5474 | FLoss: 0.4196 | LR: 1.96e-04\n",
      "  Batch 810/898 | Loss: 0.6956 | CLoss: 0.5420 | FLoss: 0.3070 | LR: 1.96e-04\n",
      "  Batch 820/898 | Loss: 1.0906 | CLoss: 0.8831 | FLoss: 0.4149 | LR: 1.96e-04\n",
      "  Batch 830/898 | Loss: 0.9838 | CLoss: 0.7353 | FLoss: 0.4969 | LR: 1.96e-04\n",
      "  Batch 840/898 | Loss: 0.7824 | CLoss: 0.5907 | FLoss: 0.3835 | LR: 1.96e-04\n",
      "  Batch 850/898 | Loss: 1.2248 | CLoss: 0.8917 | FLoss: 0.6662 | LR: 1.96e-04\n",
      "  Batch 860/898 | Loss: 1.0032 | CLoss: 0.8260 | FLoss: 0.3544 | LR: 1.96e-04\n",
      "  Batch 870/898 | Loss: 0.7288 | CLoss: 0.4479 | FLoss: 0.5619 | LR: 1.96e-04\n",
      "  Batch 880/898 | Loss: 0.6560 | CLoss: 0.5130 | FLoss: 0.2861 | LR: 1.96e-04\n",
      "  Batch 890/898 | Loss: 0.8026 | CLoss: 0.5531 | FLoss: 0.4990 | LR: 1.96e-04\n",
      "  Batch 898/898 | Loss: 0.5729 | CLoss: 0.4418 | FLoss: 0.2621 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.8062\n",
      "  Last Batch Loss: 0.5729\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 1.3536 | Batch Acc: 36.21%\n",
      "    Val Batch 010/102 | Loss: 0.1214 | Batch Acc: 96.55%\n",
      "    Val Batch 015/102 | Loss: 0.3081 | Batch Acc: 89.66%\n",
      "    Val Batch 020/102 | Loss: 0.5213 | Batch Acc: 79.31%\n",
      "    Val Batch 025/102 | Loss: 0.6910 | Batch Acc: 82.76%\n",
      "    Val Batch 030/102 | Loss: 0.7989 | Batch Acc: 74.14%\n",
      "    Val Batch 035/102 | Loss: 0.1512 | Batch Acc: 94.83%\n",
      "    Val Batch 040/102 | Loss: 0.5737 | Batch Acc: 96.55%\n",
      "    Val Batch 045/102 | Loss: 0.7340 | Batch Acc: 98.28%\n",
      "    Val Batch 050/102 | Loss: 0.7176 | Batch Acc: 75.86%\n",
      "    Val Batch 055/102 | Loss: 0.6245 | Batch Acc: 84.48%\n",
      "    Val Batch 060/102 | Loss: 1.1774 | Batch Acc: 67.24%\n",
      "    Val Batch 065/102 | Loss: 1.1029 | Batch Acc: 63.79%\n",
      "    Val Batch 070/102 | Loss: 1.0852 | Batch Acc: 63.79%\n",
      "    Val Batch 075/102 | Loss: 0.1699 | Batch Acc: 93.10%\n",
      "    Val Batch 080/102 | Loss: 0.3408 | Batch Acc: 93.10%\n",
      "    Val Batch 085/102 | Loss: 0.1724 | Batch Acc: 89.66%\n",
      "    Val Batch 090/102 | Loss: 0.3125 | Batch Acc: 93.10%\n",
      "    Val Batch 095/102 | Loss: 0.1274 | Batch Acc: 96.55%\n",
      "    Val Batch 100/102 | Loss: 0.4703 | Batch Acc: 82.76%\n",
      "    Val Batch 102/102 | Loss: 0.3834 | Batch Acc: 90.00%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.5621 | Accuracy: 81.99%\n",
      "  Current Best Acc: 81.99%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/898 | Loss: 0.9543 | CLoss: 0.7319 | FLoss: 0.4450 | LR: 1.04e-04\n",
      "  Batch 020/898 | Loss: 0.8156 | CLoss: 0.6770 | FLoss: 0.2771 | LR: 1.04e-04\n",
      "  Batch 030/898 | Loss: 0.6680 | CLoss: 0.4968 | FLoss: 0.3423 | LR: 1.04e-04\n",
      "  Batch 040/898 | Loss: 1.0811 | CLoss: 0.8416 | FLoss: 0.4790 | LR: 1.04e-04\n",
      "  Batch 050/898 | Loss: 0.7267 | CLoss: 0.5186 | FLoss: 0.4162 | LR: 1.04e-04\n",
      "  Batch 060/898 | Loss: 0.7619 | CLoss: 0.6293 | FLoss: 0.2651 | LR: 1.04e-04\n",
      "  Batch 070/898 | Loss: 0.3775 | CLoss: 0.2666 | FLoss: 0.2218 | LR: 1.04e-04\n",
      "  Batch 080/898 | Loss: 0.4127 | CLoss: 0.2336 | FLoss: 0.3583 | LR: 1.04e-04\n",
      "  Batch 090/898 | Loss: 0.9135 | CLoss: 0.7550 | FLoss: 0.3170 | LR: 1.04e-04\n",
      "  Batch 100/898 | Loss: 0.7814 | CLoss: 0.6145 | FLoss: 0.3338 | LR: 1.04e-04\n",
      "  Batch 110/898 | Loss: 0.5807 | CLoss: 0.4510 | FLoss: 0.2593 | LR: 1.04e-04\n",
      "  Batch 120/898 | Loss: 0.7257 | CLoss: 0.4833 | FLoss: 0.4847 | LR: 1.04e-04\n",
      "  Batch 130/898 | Loss: 0.5375 | CLoss: 0.3977 | FLoss: 0.2797 | LR: 1.04e-04\n",
      "  Batch 140/898 | Loss: 0.7604 | CLoss: 0.5212 | FLoss: 0.4785 | LR: 1.04e-04\n",
      "  Batch 150/898 | Loss: 0.4472 | CLoss: 0.3887 | FLoss: 0.1170 | LR: 1.04e-04\n",
      "  Batch 160/898 | Loss: 0.6822 | CLoss: 0.4957 | FLoss: 0.3730 | LR: 1.04e-04\n",
      "  Batch 170/898 | Loss: 0.6155 | CLoss: 0.4599 | FLoss: 0.3113 | LR: 1.04e-04\n",
      "  Batch 180/898 | Loss: 0.6534 | CLoss: 0.5287 | FLoss: 0.2493 | LR: 1.04e-04\n",
      "  Batch 190/898 | Loss: 0.8434 | CLoss: 0.5939 | FLoss: 0.4990 | LR: 1.04e-04\n",
      "  Batch 200/898 | Loss: 0.5708 | CLoss: 0.4430 | FLoss: 0.2557 | LR: 1.04e-04\n",
      "  Batch 210/898 | Loss: 0.9573 | CLoss: 0.8038 | FLoss: 0.3070 | LR: 1.04e-04\n",
      "  Batch 220/898 | Loss: 0.9469 | CLoss: 0.7493 | FLoss: 0.3953 | LR: 1.04e-04\n",
      "  Batch 230/898 | Loss: 0.8635 | CLoss: 0.6333 | FLoss: 0.4604 | LR: 1.04e-04\n",
      "  Batch 240/898 | Loss: 0.8368 | CLoss: 0.5986 | FLoss: 0.4763 | LR: 1.04e-04\n",
      "  Batch 250/898 | Loss: 0.5427 | CLoss: 0.3615 | FLoss: 0.3625 | LR: 1.04e-04\n",
      "  Batch 260/898 | Loss: 0.7327 | CLoss: 0.5787 | FLoss: 0.3080 | LR: 1.04e-04\n",
      "  Batch 270/898 | Loss: 0.8444 | CLoss: 0.6194 | FLoss: 0.4499 | LR: 1.04e-04\n",
      "  Batch 280/898 | Loss: 0.9847 | CLoss: 0.7650 | FLoss: 0.4395 | LR: 1.04e-04\n",
      "  Batch 290/898 | Loss: 0.7989 | CLoss: 0.6634 | FLoss: 0.2710 | LR: 1.04e-04\n",
      "  Batch 300/898 | Loss: 0.9445 | CLoss: 0.6107 | FLoss: 0.6677 | LR: 1.04e-04\n",
      "  Batch 310/898 | Loss: 0.6101 | CLoss: 0.4817 | FLoss: 0.2568 | LR: 1.04e-04\n",
      "  Batch 320/898 | Loss: 0.4900 | CLoss: 0.3437 | FLoss: 0.2926 | LR: 1.04e-04\n",
      "  Batch 330/898 | Loss: 0.8812 | CLoss: 0.7045 | FLoss: 0.3535 | LR: 1.04e-04\n",
      "  Batch 340/898 | Loss: 0.5907 | CLoss: 0.4664 | FLoss: 0.2487 | LR: 1.04e-04\n",
      "  Batch 350/898 | Loss: 0.6459 | CLoss: 0.4739 | FLoss: 0.3440 | LR: 1.04e-04\n",
      "  Batch 360/898 | Loss: 0.7917 | CLoss: 0.5965 | FLoss: 0.3904 | LR: 1.04e-04\n",
      "  Batch 370/898 | Loss: 0.6369 | CLoss: 0.4650 | FLoss: 0.3436 | LR: 1.04e-04\n",
      "  Batch 380/898 | Loss: 0.6939 | CLoss: 0.5442 | FLoss: 0.2994 | LR: 1.04e-04\n",
      "  Batch 390/898 | Loss: 0.7913 | CLoss: 0.5714 | FLoss: 0.4398 | LR: 1.04e-04\n",
      "  Batch 400/898 | Loss: 0.3855 | CLoss: 0.2446 | FLoss: 0.2817 | LR: 1.04e-04\n",
      "  Batch 410/898 | Loss: 0.8534 | CLoss: 0.7500 | FLoss: 0.2068 | LR: 1.04e-04\n",
      "  Batch 420/898 | Loss: 0.6831 | CLoss: 0.5347 | FLoss: 0.2969 | LR: 1.04e-04\n",
      "  Batch 430/898 | Loss: 0.4238 | CLoss: 0.3123 | FLoss: 0.2231 | LR: 1.04e-04\n",
      "  Batch 440/898 | Loss: 1.3120 | CLoss: 1.0096 | FLoss: 0.6048 | LR: 1.04e-04\n",
      "  Batch 450/898 | Loss: 0.8395 | CLoss: 0.6436 | FLoss: 0.3916 | LR: 1.04e-04\n",
      "  Batch 460/898 | Loss: 0.6418 | CLoss: 0.5295 | FLoss: 0.2246 | LR: 1.04e-04\n",
      "  Batch 470/898 | Loss: 0.3973 | CLoss: 0.2686 | FLoss: 0.2575 | LR: 1.04e-04\n",
      "  Batch 480/898 | Loss: 0.8619 | CLoss: 0.6866 | FLoss: 0.3506 | LR: 1.04e-04\n",
      "  Batch 490/898 | Loss: 0.9521 | CLoss: 0.7572 | FLoss: 0.3897 | LR: 1.04e-04\n",
      "  Batch 500/898 | Loss: 0.9014 | CLoss: 0.7302 | FLoss: 0.3424 | LR: 1.04e-04\n",
      "  Batch 510/898 | Loss: 0.6170 | CLoss: 0.4132 | FLoss: 0.4076 | LR: 1.04e-04\n",
      "  Batch 520/898 | Loss: 0.3517 | CLoss: 0.2458 | FLoss: 0.2118 | LR: 1.04e-04\n",
      "  Batch 530/898 | Loss: 0.8409 | CLoss: 0.6305 | FLoss: 0.4210 | LR: 1.04e-04\n",
      "  Batch 540/898 | Loss: 0.9575 | CLoss: 0.7173 | FLoss: 0.4805 | LR: 1.04e-04\n",
      "  Batch 550/898 | Loss: 0.7634 | CLoss: 0.5908 | FLoss: 0.3452 | LR: 1.04e-04\n",
      "  Batch 560/898 | Loss: 0.8326 | CLoss: 0.5936 | FLoss: 0.4780 | LR: 1.04e-04\n",
      "  Batch 570/898 | Loss: 0.4485 | CLoss: 0.3177 | FLoss: 0.2615 | LR: 1.04e-04\n",
      "  Batch 580/898 | Loss: 0.8372 | CLoss: 0.6573 | FLoss: 0.3598 | LR: 1.04e-04\n",
      "  Batch 590/898 | Loss: 0.8484 | CLoss: 0.6460 | FLoss: 0.4049 | LR: 1.04e-04\n",
      "  Batch 600/898 | Loss: 0.8665 | CLoss: 0.6528 | FLoss: 0.4273 | LR: 1.04e-04\n",
      "  Batch 610/898 | Loss: 0.4085 | CLoss: 0.3060 | FLoss: 0.2051 | LR: 1.04e-04\n",
      "  Batch 620/898 | Loss: 1.0378 | CLoss: 0.8400 | FLoss: 0.3955 | LR: 1.04e-04\n",
      "  Batch 630/898 | Loss: 0.3705 | CLoss: 0.3027 | FLoss: 0.1357 | LR: 1.04e-04\n",
      "  Batch 640/898 | Loss: 0.9679 | CLoss: 0.7844 | FLoss: 0.3669 | LR: 1.04e-04\n",
      "  Batch 650/898 | Loss: 0.7234 | CLoss: 0.5455 | FLoss: 0.3557 | LR: 1.04e-04\n",
      "  Batch 660/898 | Loss: 0.2844 | CLoss: 0.2075 | FLoss: 0.1539 | LR: 1.04e-04\n",
      "  Batch 670/898 | Loss: 0.4694 | CLoss: 0.3381 | FLoss: 0.2626 | LR: 1.04e-04\n",
      "  Batch 680/898 | Loss: 0.6817 | CLoss: 0.4962 | FLoss: 0.3709 | LR: 1.04e-04\n",
      "  Batch 690/898 | Loss: 0.6835 | CLoss: 0.4910 | FLoss: 0.3850 | LR: 1.04e-04\n",
      "  Batch 700/898 | Loss: 0.3911 | CLoss: 0.3266 | FLoss: 0.1291 | LR: 1.04e-04\n",
      "  Batch 710/898 | Loss: 0.7066 | CLoss: 0.5362 | FLoss: 0.3407 | LR: 1.04e-04\n",
      "  Batch 720/898 | Loss: 0.5330 | CLoss: 0.3701 | FLoss: 0.3258 | LR: 1.04e-04\n",
      "  Batch 730/898 | Loss: 0.6796 | CLoss: 0.5522 | FLoss: 0.2550 | LR: 1.04e-04\n",
      "  Batch 740/898 | Loss: 1.0875 | CLoss: 0.8882 | FLoss: 0.3986 | LR: 1.04e-04\n",
      "  Batch 750/898 | Loss: 0.4609 | CLoss: 0.3112 | FLoss: 0.2993 | LR: 1.04e-04\n",
      "  Batch 760/898 | Loss: 0.3413 | CLoss: 0.2821 | FLoss: 0.1183 | LR: 1.04e-04\n",
      "  Batch 770/898 | Loss: 0.6270 | CLoss: 0.4374 | FLoss: 0.3791 | LR: 1.04e-04\n",
      "  Batch 780/898 | Loss: 0.8849 | CLoss: 0.7008 | FLoss: 0.3682 | LR: 1.04e-04\n",
      "  Batch 790/898 | Loss: 0.8316 | CLoss: 0.6923 | FLoss: 0.2786 | LR: 1.04e-04\n",
      "  Batch 800/898 | Loss: 0.5157 | CLoss: 0.3062 | FLoss: 0.4191 | LR: 1.04e-04\n",
      "  Batch 810/898 | Loss: 0.9650 | CLoss: 0.8457 | FLoss: 0.2385 | LR: 1.04e-04\n",
      "  Batch 820/898 | Loss: 0.8926 | CLoss: 0.6880 | FLoss: 0.4093 | LR: 1.04e-04\n",
      "  Batch 830/898 | Loss: 0.4670 | CLoss: 0.3497 | FLoss: 0.2346 | LR: 1.04e-04\n",
      "  Batch 840/898 | Loss: 0.5924 | CLoss: 0.3855 | FLoss: 0.4137 | LR: 1.04e-04\n",
      "  Batch 850/898 | Loss: 0.8079 | CLoss: 0.6585 | FLoss: 0.2989 | LR: 1.04e-04\n",
      "  Batch 860/898 | Loss: 1.0174 | CLoss: 0.7817 | FLoss: 0.4715 | LR: 1.04e-04\n",
      "  Batch 870/898 | Loss: 0.8196 | CLoss: 0.6776 | FLoss: 0.2840 | LR: 1.04e-04\n",
      "  Batch 880/898 | Loss: 1.4974 | CLoss: 1.2477 | FLoss: 0.4994 | LR: 1.04e-04\n",
      "  Batch 890/898 | Loss: 0.5475 | CLoss: 0.4223 | FLoss: 0.2504 | LR: 1.04e-04\n",
      "  Batch 898/898 | Loss: 0.0984 | CLoss: 0.0534 | FLoss: 0.0900 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.7115\n",
      "  Last Batch Loss: 0.0984\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 1.3894 | Batch Acc: 41.38%\n",
      "    Val Batch 010/102 | Loss: 0.1986 | Batch Acc: 94.83%\n",
      "    Val Batch 015/102 | Loss: 0.2327 | Batch Acc: 89.66%\n",
      "    Val Batch 020/102 | Loss: 0.5245 | Batch Acc: 89.66%\n",
      "    Val Batch 025/102 | Loss: 0.4310 | Batch Acc: 86.21%\n",
      "    Val Batch 030/102 | Loss: 0.4749 | Batch Acc: 84.48%\n",
      "    Val Batch 035/102 | Loss: 0.2185 | Batch Acc: 91.38%\n",
      "    Val Batch 040/102 | Loss: 0.7821 | Batch Acc: 36.21%\n",
      "    Val Batch 045/102 | Loss: 0.8862 | Batch Acc: 29.31%\n",
      "    Val Batch 050/102 | Loss: 0.4232 | Batch Acc: 82.76%\n",
      "    Val Batch 055/102 | Loss: 0.5926 | Batch Acc: 79.31%\n",
      "    Val Batch 060/102 | Loss: 1.0996 | Batch Acc: 65.52%\n",
      "    Val Batch 065/102 | Loss: 1.0341 | Batch Acc: 67.24%\n",
      "    Val Batch 070/102 | Loss: 1.2607 | Batch Acc: 63.79%\n",
      "    Val Batch 075/102 | Loss: 0.3167 | Batch Acc: 87.93%\n",
      "    Val Batch 080/102 | Loss: 0.4766 | Batch Acc: 89.66%\n",
      "    Val Batch 085/102 | Loss: 0.3018 | Batch Acc: 89.66%\n",
      "    Val Batch 090/102 | Loss: 0.1005 | Batch Acc: 98.28%\n",
      "    Val Batch 095/102 | Loss: 0.0800 | Batch Acc: 96.55%\n",
      "    Val Batch 100/102 | Loss: 0.1811 | Batch Acc: 94.83%\n",
      "    Val Batch 102/102 | Loss: 0.0249 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.5089 | Accuracy: 81.12%\n",
      "  Current Best Acc: 81.99%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/898 | Loss: 0.4185 | CLoss: 0.3258 | FLoss: 0.1854 | LR: 2.86e-05\n",
      "  Batch 020/898 | Loss: 1.1373 | CLoss: 0.9068 | FLoss: 0.4609 | LR: 2.86e-05\n",
      "  Batch 030/898 | Loss: 0.4723 | CLoss: 0.3707 | FLoss: 0.2032 | LR: 2.86e-05\n",
      "  Batch 040/898 | Loss: 0.6366 | CLoss: 0.4652 | FLoss: 0.3427 | LR: 2.86e-05\n",
      "  Batch 050/898 | Loss: 0.7677 | CLoss: 0.6255 | FLoss: 0.2843 | LR: 2.86e-05\n",
      "  Batch 060/898 | Loss: 0.7404 | CLoss: 0.5668 | FLoss: 0.3472 | LR: 2.86e-05\n",
      "  Batch 070/898 | Loss: 0.6115 | CLoss: 0.4968 | FLoss: 0.2294 | LR: 2.86e-05\n",
      "  Batch 080/898 | Loss: 1.0373 | CLoss: 0.8477 | FLoss: 0.3791 | LR: 2.86e-05\n",
      "  Batch 090/898 | Loss: 0.5608 | CLoss: 0.4575 | FLoss: 0.2066 | LR: 2.86e-05\n",
      "  Batch 100/898 | Loss: 0.9388 | CLoss: 0.7288 | FLoss: 0.4199 | LR: 2.86e-05\n",
      "  Batch 110/898 | Loss: 0.7574 | CLoss: 0.5922 | FLoss: 0.3305 | LR: 2.86e-05\n",
      "  Batch 120/898 | Loss: 0.5839 | CLoss: 0.4613 | FLoss: 0.2452 | LR: 2.86e-05\n",
      "  Batch 130/898 | Loss: 0.6752 | CLoss: 0.5597 | FLoss: 0.2310 | LR: 2.86e-05\n",
      "  Batch 140/898 | Loss: 0.6060 | CLoss: 0.4748 | FLoss: 0.2624 | LR: 2.86e-05\n",
      "  Batch 150/898 | Loss: 1.1578 | CLoss: 0.9087 | FLoss: 0.4980 | LR: 2.86e-05\n",
      "  Batch 160/898 | Loss: 1.1205 | CLoss: 0.9165 | FLoss: 0.4080 | LR: 2.86e-05\n",
      "  Batch 170/898 | Loss: 0.7235 | CLoss: 0.6065 | FLoss: 0.2340 | LR: 2.86e-05\n",
      "  Batch 180/898 | Loss: 0.6257 | CLoss: 0.4677 | FLoss: 0.3160 | LR: 2.86e-05\n",
      "  Batch 190/898 | Loss: 0.6342 | CLoss: 0.4002 | FLoss: 0.4681 | LR: 2.86e-05\n",
      "  Batch 200/898 | Loss: 0.6310 | CLoss: 0.4164 | FLoss: 0.4292 | LR: 2.86e-05\n",
      "  Batch 210/898 | Loss: 0.6745 | CLoss: 0.5257 | FLoss: 0.2976 | LR: 2.86e-05\n",
      "  Batch 220/898 | Loss: 0.7904 | CLoss: 0.6359 | FLoss: 0.3091 | LR: 2.86e-05\n",
      "  Batch 230/898 | Loss: 0.5068 | CLoss: 0.3535 | FLoss: 0.3067 | LR: 2.86e-05\n",
      "  Batch 240/898 | Loss: 0.6454 | CLoss: 0.4782 | FLoss: 0.3343 | LR: 2.86e-05\n",
      "  Batch 250/898 | Loss: 0.8167 | CLoss: 0.6608 | FLoss: 0.3119 | LR: 2.86e-05\n",
      "  Batch 260/898 | Loss: 0.8435 | CLoss: 0.6659 | FLoss: 0.3551 | LR: 2.86e-05\n",
      "  Batch 270/898 | Loss: 0.4489 | CLoss: 0.3285 | FLoss: 0.2408 | LR: 2.86e-05\n",
      "  Batch 280/898 | Loss: 0.5512 | CLoss: 0.4228 | FLoss: 0.2568 | LR: 2.86e-05\n",
      "  Batch 290/898 | Loss: 0.6694 | CLoss: 0.5607 | FLoss: 0.2174 | LR: 2.86e-05\n",
      "  Batch 300/898 | Loss: 0.5772 | CLoss: 0.4122 | FLoss: 0.3301 | LR: 2.86e-05\n",
      "  Batch 310/898 | Loss: 0.2848 | CLoss: 0.1626 | FLoss: 0.2445 | LR: 2.86e-05\n",
      "  Batch 320/898 | Loss: 0.3505 | CLoss: 0.2748 | FLoss: 0.1515 | LR: 2.86e-05\n",
      "  Batch 330/898 | Loss: 0.5834 | CLoss: 0.4451 | FLoss: 0.2766 | LR: 2.86e-05\n",
      "  Batch 340/898 | Loss: 0.6897 | CLoss: 0.5558 | FLoss: 0.2678 | LR: 2.86e-05\n",
      "  Batch 350/898 | Loss: 0.5955 | CLoss: 0.4876 | FLoss: 0.2158 | LR: 2.86e-05\n",
      "  Batch 360/898 | Loss: 0.7169 | CLoss: 0.5885 | FLoss: 0.2568 | LR: 2.86e-05\n",
      "  Batch 370/898 | Loss: 0.4953 | CLoss: 0.3155 | FLoss: 0.3596 | LR: 2.86e-05\n",
      "  Batch 380/898 | Loss: 0.3794 | CLoss: 0.2550 | FLoss: 0.2487 | LR: 2.86e-05\n",
      "  Batch 390/898 | Loss: 0.4977 | CLoss: 0.4122 | FLoss: 0.1709 | LR: 2.86e-05\n",
      "  Batch 400/898 | Loss: 0.6558 | CLoss: 0.5072 | FLoss: 0.2973 | LR: 2.86e-05\n",
      "  Batch 410/898 | Loss: 0.5877 | CLoss: 0.4547 | FLoss: 0.2659 | LR: 2.86e-05\n",
      "  Batch 420/898 | Loss: 0.6778 | CLoss: 0.5219 | FLoss: 0.3117 | LR: 2.86e-05\n",
      "  Batch 430/898 | Loss: 0.6286 | CLoss: 0.3895 | FLoss: 0.4781 | LR: 2.86e-05\n",
      "  Batch 440/898 | Loss: 0.8668 | CLoss: 0.6959 | FLoss: 0.3419 | LR: 2.86e-05\n",
      "  Batch 450/898 | Loss: 0.4123 | CLoss: 0.2073 | FLoss: 0.4099 | LR: 2.86e-05\n",
      "  Batch 460/898 | Loss: 0.8066 | CLoss: 0.6502 | FLoss: 0.3128 | LR: 2.86e-05\n",
      "  Batch 470/898 | Loss: 1.1190 | CLoss: 0.8057 | FLoss: 0.6266 | LR: 2.86e-05\n",
      "  Batch 480/898 | Loss: 0.6290 | CLoss: 0.4768 | FLoss: 0.3045 | LR: 2.86e-05\n",
      "  Batch 490/898 | Loss: 0.3211 | CLoss: 0.2322 | FLoss: 0.1778 | LR: 2.86e-05\n",
      "  Batch 500/898 | Loss: 0.6590 | CLoss: 0.4524 | FLoss: 0.4133 | LR: 2.86e-05\n",
      "  Batch 510/898 | Loss: 0.6044 | CLoss: 0.4690 | FLoss: 0.2708 | LR: 2.86e-05\n",
      "  Batch 520/898 | Loss: 0.6729 | CLoss: 0.5473 | FLoss: 0.2511 | LR: 2.86e-05\n",
      "  Batch 530/898 | Loss: 0.7870 | CLoss: 0.5842 | FLoss: 0.4055 | LR: 2.86e-05\n",
      "  Batch 540/898 | Loss: 0.4146 | CLoss: 0.3175 | FLoss: 0.1942 | LR: 2.86e-05\n",
      "  Batch 550/898 | Loss: 0.4418 | CLoss: 0.3492 | FLoss: 0.1850 | LR: 2.86e-05\n",
      "  Batch 560/898 | Loss: 0.7705 | CLoss: 0.5894 | FLoss: 0.3622 | LR: 2.86e-05\n",
      "  Batch 570/898 | Loss: 0.7925 | CLoss: 0.5735 | FLoss: 0.4381 | LR: 2.86e-05\n",
      "  Batch 580/898 | Loss: 0.5127 | CLoss: 0.4180 | FLoss: 0.1894 | LR: 2.86e-05\n",
      "  Batch 590/898 | Loss: 0.6513 | CLoss: 0.4787 | FLoss: 0.3450 | LR: 2.86e-05\n",
      "  Batch 600/898 | Loss: 0.7115 | CLoss: 0.5708 | FLoss: 0.2813 | LR: 2.86e-05\n",
      "  Batch 610/898 | Loss: 0.6631 | CLoss: 0.5199 | FLoss: 0.2864 | LR: 2.86e-05\n",
      "  Batch 620/898 | Loss: 0.4804 | CLoss: 0.3248 | FLoss: 0.3112 | LR: 2.86e-05\n",
      "  Batch 630/898 | Loss: 0.4510 | CLoss: 0.3696 | FLoss: 0.1627 | LR: 2.86e-05\n",
      "  Batch 640/898 | Loss: 0.6361 | CLoss: 0.5047 | FLoss: 0.2629 | LR: 2.86e-05\n",
      "  Batch 650/898 | Loss: 0.8523 | CLoss: 0.6383 | FLoss: 0.4280 | LR: 2.86e-05\n",
      "  Batch 660/898 | Loss: 0.3958 | CLoss: 0.3086 | FLoss: 0.1743 | LR: 2.86e-05\n",
      "  Batch 670/898 | Loss: 0.3759 | CLoss: 0.2823 | FLoss: 0.1871 | LR: 2.86e-05\n",
      "  Batch 680/898 | Loss: 0.4314 | CLoss: 0.3217 | FLoss: 0.2192 | LR: 2.86e-05\n",
      "  Batch 690/898 | Loss: 1.0278 | CLoss: 0.7974 | FLoss: 0.4608 | LR: 2.86e-05\n",
      "  Batch 700/898 | Loss: 0.4999 | CLoss: 0.3783 | FLoss: 0.2432 | LR: 2.86e-05\n",
      "  Batch 710/898 | Loss: 0.8345 | CLoss: 0.6970 | FLoss: 0.2751 | LR: 2.86e-05\n",
      "  Batch 720/898 | Loss: 0.9950 | CLoss: 0.7766 | FLoss: 0.4367 | LR: 2.86e-05\n",
      "  Batch 730/898 | Loss: 0.7773 | CLoss: 0.6538 | FLoss: 0.2470 | LR: 2.86e-05\n",
      "  Batch 740/898 | Loss: 0.9944 | CLoss: 0.7855 | FLoss: 0.4179 | LR: 2.86e-05\n",
      "  Batch 750/898 | Loss: 0.4643 | CLoss: 0.3601 | FLoss: 0.2085 | LR: 2.86e-05\n",
      "  Batch 760/898 | Loss: 0.6786 | CLoss: 0.5188 | FLoss: 0.3196 | LR: 2.86e-05\n",
      "  Batch 770/898 | Loss: 0.5465 | CLoss: 0.4665 | FLoss: 0.1600 | LR: 2.86e-05\n",
      "  Batch 780/898 | Loss: 0.7660 | CLoss: 0.6333 | FLoss: 0.2654 | LR: 2.86e-05\n",
      "  Batch 790/898 | Loss: 0.8189 | CLoss: 0.6832 | FLoss: 0.2714 | LR: 2.86e-05\n",
      "  Batch 800/898 | Loss: 0.4618 | CLoss: 0.3507 | FLoss: 0.2222 | LR: 2.86e-05\n",
      "  Batch 810/898 | Loss: 0.4853 | CLoss: 0.2965 | FLoss: 0.3777 | LR: 2.86e-05\n",
      "  Batch 820/898 | Loss: 0.4565 | CLoss: 0.3295 | FLoss: 0.2539 | LR: 2.86e-05\n",
      "  Batch 830/898 | Loss: 0.6718 | CLoss: 0.5481 | FLoss: 0.2474 | LR: 2.86e-05\n",
      "  Batch 840/898 | Loss: 0.4942 | CLoss: 0.3961 | FLoss: 0.1961 | LR: 2.86e-05\n",
      "  Batch 850/898 | Loss: 0.3805 | CLoss: 0.2934 | FLoss: 0.1741 | LR: 2.86e-05\n",
      "  Batch 860/898 | Loss: 0.6569 | CLoss: 0.5250 | FLoss: 0.2638 | LR: 2.86e-05\n",
      "  Batch 870/898 | Loss: 0.6901 | CLoss: 0.5357 | FLoss: 0.3088 | LR: 2.86e-05\n",
      "  Batch 880/898 | Loss: 0.4734 | CLoss: 0.3795 | FLoss: 0.1877 | LR: 2.86e-05\n",
      "  Batch 890/898 | Loss: 0.7684 | CLoss: 0.5719 | FLoss: 0.3931 | LR: 2.86e-05\n",
      "  Batch 898/898 | Loss: 0.5476 | CLoss: 0.1911 | FLoss: 0.7131 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.6480\n",
      "  Last Batch Loss: 0.5476\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 1.2335 | Batch Acc: 51.72%\n",
      "    Val Batch 010/102 | Loss: 0.2965 | Batch Acc: 89.66%\n",
      "    Val Batch 015/102 | Loss: 0.2646 | Batch Acc: 87.93%\n",
      "    Val Batch 020/102 | Loss: 0.4750 | Batch Acc: 86.21%\n",
      "    Val Batch 025/102 | Loss: 0.3769 | Batch Acc: 89.66%\n",
      "    Val Batch 030/102 | Loss: 0.5365 | Batch Acc: 82.76%\n",
      "    Val Batch 035/102 | Loss: 0.2275 | Batch Acc: 91.38%\n",
      "    Val Batch 040/102 | Loss: 0.6895 | Batch Acc: 48.28%\n",
      "    Val Batch 045/102 | Loss: 0.8642 | Batch Acc: 34.48%\n",
      "    Val Batch 050/102 | Loss: 0.4789 | Batch Acc: 82.76%\n",
      "    Val Batch 055/102 | Loss: 0.4123 | Batch Acc: 87.93%\n",
      "    Val Batch 060/102 | Loss: 0.8129 | Batch Acc: 72.41%\n",
      "    Val Batch 065/102 | Loss: 1.3567 | Batch Acc: 58.62%\n",
      "    Val Batch 070/102 | Loss: 1.0856 | Batch Acc: 56.90%\n",
      "    Val Batch 075/102 | Loss: 0.2523 | Batch Acc: 94.83%\n",
      "    Val Batch 080/102 | Loss: 0.1675 | Batch Acc: 94.83%\n",
      "    Val Batch 085/102 | Loss: 0.2563 | Batch Acc: 91.38%\n",
      "    Val Batch 090/102 | Loss: 0.1747 | Batch Acc: 96.55%\n",
      "    Val Batch 095/102 | Loss: 0.0252 | Batch Acc: 100.00%\n",
      "    Val Batch 100/102 | Loss: 0.0164 | Batch Acc: 100.00%\n",
      "    Val Batch 102/102 | Loss: 0.0783 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.4998 | Accuracy: 81.22%\n",
      "  Current Best Acc: 81.99%\n",
      "\n",
      "========================================\n",
      "=== Fold 5 Completed ===\n",
      "Best Validation Accuracy: 81.99%\n",
      "\n",
      "========================================\n",
      "=== Fold 6/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/899 | Loss: 2.1816 | CLoss: 1.2209 | FLoss: 1.9213 | LR: 3.00e-04\n",
      "  Batch 020/899 | Loss: 1.8160 | CLoss: 1.0576 | FLoss: 1.5167 | LR: 3.00e-04\n",
      "  Batch 030/899 | Loss: 1.4721 | CLoss: 0.8531 | FLoss: 1.2380 | LR: 3.00e-04\n",
      "  Batch 040/899 | Loss: 1.7112 | CLoss: 1.2639 | FLoss: 0.8946 | LR: 3.00e-04\n",
      "  Batch 050/899 | Loss: 1.3162 | CLoss: 1.0180 | FLoss: 0.5963 | LR: 3.00e-04\n",
      "  Batch 060/899 | Loss: 0.7139 | CLoss: 0.4774 | FLoss: 0.4730 | LR: 3.00e-04\n",
      "  Batch 070/899 | Loss: 0.6870 | CLoss: 0.4306 | FLoss: 0.5128 | LR: 3.00e-04\n",
      "  Batch 080/899 | Loss: 1.0793 | CLoss: 0.8504 | FLoss: 0.4578 | LR: 3.00e-04\n",
      "  Batch 090/899 | Loss: 1.1892 | CLoss: 0.8449 | FLoss: 0.6885 | LR: 3.00e-04\n",
      "  Batch 100/899 | Loss: 1.2918 | CLoss: 1.0058 | FLoss: 0.5720 | LR: 3.00e-04\n",
      "  Batch 110/899 | Loss: 0.7193 | CLoss: 0.5392 | FLoss: 0.3602 | LR: 3.00e-04\n",
      "  Batch 120/899 | Loss: 1.2652 | CLoss: 0.8963 | FLoss: 0.7379 | LR: 3.00e-04\n",
      "  Batch 130/899 | Loss: 1.2033 | CLoss: 0.8976 | FLoss: 0.6115 | LR: 3.00e-04\n",
      "  Batch 140/899 | Loss: 0.6147 | CLoss: 0.4108 | FLoss: 0.4078 | LR: 3.00e-04\n",
      "  Batch 150/899 | Loss: 1.1411 | CLoss: 0.9147 | FLoss: 0.4529 | LR: 3.00e-04\n",
      "  Batch 160/899 | Loss: 1.3115 | CLoss: 1.0223 | FLoss: 0.5786 | LR: 3.00e-04\n",
      "  Batch 170/899 | Loss: 1.0870 | CLoss: 0.8626 | FLoss: 0.4486 | LR: 3.00e-04\n",
      "  Batch 180/899 | Loss: 1.0759 | CLoss: 0.8144 | FLoss: 0.5229 | LR: 3.00e-04\n",
      "  Batch 190/899 | Loss: 1.2114 | CLoss: 0.9126 | FLoss: 0.5975 | LR: 3.00e-04\n",
      "  Batch 200/899 | Loss: 0.9251 | CLoss: 0.6774 | FLoss: 0.4953 | LR: 3.00e-04\n",
      "  Batch 210/899 | Loss: 0.9516 | CLoss: 0.7369 | FLoss: 0.4294 | LR: 3.00e-04\n",
      "  Batch 220/899 | Loss: 0.8779 | CLoss: 0.6776 | FLoss: 0.4006 | LR: 3.00e-04\n",
      "  Batch 230/899 | Loss: 1.0135 | CLoss: 0.7355 | FLoss: 0.5559 | LR: 3.00e-04\n",
      "  Batch 240/899 | Loss: 1.1426 | CLoss: 0.9193 | FLoss: 0.4466 | LR: 3.00e-04\n",
      "  Batch 250/899 | Loss: 0.7798 | CLoss: 0.5701 | FLoss: 0.4193 | LR: 3.00e-04\n",
      "  Batch 260/899 | Loss: 1.1117 | CLoss: 0.8251 | FLoss: 0.5731 | LR: 3.00e-04\n",
      "  Batch 270/899 | Loss: 0.5327 | CLoss: 0.3493 | FLoss: 0.3668 | LR: 3.00e-04\n",
      "  Batch 280/899 | Loss: 0.7990 | CLoss: 0.5743 | FLoss: 0.4494 | LR: 3.00e-04\n",
      "  Batch 290/899 | Loss: 1.2886 | CLoss: 1.0767 | FLoss: 0.4236 | LR: 3.00e-04\n",
      "  Batch 300/899 | Loss: 0.9862 | CLoss: 0.7857 | FLoss: 0.4010 | LR: 3.00e-04\n",
      "  Batch 310/899 | Loss: 0.7677 | CLoss: 0.6023 | FLoss: 0.3307 | LR: 3.00e-04\n",
      "  Batch 320/899 | Loss: 0.7040 | CLoss: 0.5407 | FLoss: 0.3267 | LR: 3.00e-04\n",
      "  Batch 330/899 | Loss: 0.6553 | CLoss: 0.4622 | FLoss: 0.3863 | LR: 3.00e-04\n",
      "  Batch 340/899 | Loss: 0.3424 | CLoss: 0.2333 | FLoss: 0.2182 | LR: 3.00e-04\n",
      "  Batch 350/899 | Loss: 1.0607 | CLoss: 0.8938 | FLoss: 0.3338 | LR: 3.00e-04\n",
      "  Batch 360/899 | Loss: 0.9625 | CLoss: 0.7731 | FLoss: 0.3788 | LR: 3.00e-04\n",
      "  Batch 370/899 | Loss: 0.9607 | CLoss: 0.7689 | FLoss: 0.3835 | LR: 3.00e-04\n",
      "  Batch 380/899 | Loss: 1.1979 | CLoss: 0.8129 | FLoss: 0.7699 | LR: 3.00e-04\n",
      "  Batch 390/899 | Loss: 1.1796 | CLoss: 0.9205 | FLoss: 0.5181 | LR: 3.00e-04\n",
      "  Batch 400/899 | Loss: 0.7254 | CLoss: 0.4638 | FLoss: 0.5233 | LR: 3.00e-04\n",
      "  Batch 410/899 | Loss: 1.4052 | CLoss: 1.0518 | FLoss: 0.7067 | LR: 3.00e-04\n",
      "  Batch 420/899 | Loss: 1.2788 | CLoss: 1.0440 | FLoss: 0.4698 | LR: 3.00e-04\n",
      "  Batch 430/899 | Loss: 0.8606 | CLoss: 0.5213 | FLoss: 0.6786 | LR: 3.00e-04\n",
      "  Batch 440/899 | Loss: 0.5886 | CLoss: 0.3737 | FLoss: 0.4299 | LR: 3.00e-04\n",
      "  Batch 450/899 | Loss: 0.7516 | CLoss: 0.5430 | FLoss: 0.4171 | LR: 3.00e-04\n",
      "  Batch 460/899 | Loss: 0.9142 | CLoss: 0.7421 | FLoss: 0.3442 | LR: 3.00e-04\n",
      "  Batch 470/899 | Loss: 0.9009 | CLoss: 0.7333 | FLoss: 0.3352 | LR: 3.00e-04\n",
      "  Batch 480/899 | Loss: 0.9597 | CLoss: 0.7204 | FLoss: 0.4787 | LR: 3.00e-04\n",
      "  Batch 490/899 | Loss: 1.4766 | CLoss: 1.1669 | FLoss: 0.6194 | LR: 3.00e-04\n",
      "  Batch 500/899 | Loss: 1.2148 | CLoss: 1.0319 | FLoss: 0.3658 | LR: 3.00e-04\n",
      "  Batch 510/899 | Loss: 0.8543 | CLoss: 0.5641 | FLoss: 0.5805 | LR: 3.00e-04\n",
      "  Batch 520/899 | Loss: 0.9270 | CLoss: 0.6335 | FLoss: 0.5871 | LR: 3.00e-04\n",
      "  Batch 530/899 | Loss: 0.9469 | CLoss: 0.7406 | FLoss: 0.4125 | LR: 3.00e-04\n",
      "  Batch 540/899 | Loss: 1.0415 | CLoss: 0.8130 | FLoss: 0.4570 | LR: 3.00e-04\n",
      "  Batch 550/899 | Loss: 0.9943 | CLoss: 0.7074 | FLoss: 0.5738 | LR: 3.00e-04\n",
      "  Batch 560/899 | Loss: 0.9898 | CLoss: 0.8062 | FLoss: 0.3671 | LR: 3.00e-04\n",
      "  Batch 570/899 | Loss: 0.8867 | CLoss: 0.6937 | FLoss: 0.3859 | LR: 3.00e-04\n",
      "  Batch 580/899 | Loss: 1.2053 | CLoss: 0.9100 | FLoss: 0.5906 | LR: 3.00e-04\n",
      "  Batch 590/899 | Loss: 0.8337 | CLoss: 0.6569 | FLoss: 0.3536 | LR: 3.00e-04\n",
      "  Batch 600/899 | Loss: 0.8788 | CLoss: 0.6854 | FLoss: 0.3869 | LR: 3.00e-04\n",
      "  Batch 610/899 | Loss: 0.6725 | CLoss: 0.4658 | FLoss: 0.4134 | LR: 3.00e-04\n",
      "  Batch 620/899 | Loss: 1.3033 | CLoss: 1.0584 | FLoss: 0.4898 | LR: 3.00e-04\n",
      "  Batch 630/899 | Loss: 0.6792 | CLoss: 0.4547 | FLoss: 0.4489 | LR: 3.00e-04\n",
      "  Batch 640/899 | Loss: 0.9358 | CLoss: 0.7260 | FLoss: 0.4196 | LR: 3.00e-04\n",
      "  Batch 650/899 | Loss: 0.9192 | CLoss: 0.7470 | FLoss: 0.3444 | LR: 3.00e-04\n",
      "  Batch 660/899 | Loss: 0.7657 | CLoss: 0.5493 | FLoss: 0.4327 | LR: 3.00e-04\n",
      "  Batch 670/899 | Loss: 0.8871 | CLoss: 0.6813 | FLoss: 0.4115 | LR: 3.00e-04\n",
      "  Batch 680/899 | Loss: 1.1548 | CLoss: 0.8359 | FLoss: 0.6378 | LR: 3.00e-04\n",
      "  Batch 690/899 | Loss: 0.9346 | CLoss: 0.6679 | FLoss: 0.5334 | LR: 3.00e-04\n",
      "  Batch 700/899 | Loss: 0.9627 | CLoss: 0.7432 | FLoss: 0.4390 | LR: 3.00e-04\n",
      "  Batch 710/899 | Loss: 1.2238 | CLoss: 1.0177 | FLoss: 0.4123 | LR: 3.00e-04\n",
      "  Batch 720/899 | Loss: 0.8504 | CLoss: 0.6357 | FLoss: 0.4294 | LR: 3.00e-04\n",
      "  Batch 730/899 | Loss: 0.8584 | CLoss: 0.6565 | FLoss: 0.4037 | LR: 3.00e-04\n",
      "  Batch 740/899 | Loss: 1.0268 | CLoss: 0.8061 | FLoss: 0.4415 | LR: 3.00e-04\n",
      "  Batch 750/899 | Loss: 0.7917 | CLoss: 0.5309 | FLoss: 0.5217 | LR: 3.00e-04\n",
      "  Batch 760/899 | Loss: 0.9837 | CLoss: 0.7569 | FLoss: 0.4536 | LR: 3.00e-04\n",
      "  Batch 770/899 | Loss: 0.6419 | CLoss: 0.4785 | FLoss: 0.3268 | LR: 3.00e-04\n",
      "  Batch 780/899 | Loss: 1.1564 | CLoss: 0.8839 | FLoss: 0.5451 | LR: 3.00e-04\n",
      "  Batch 790/899 | Loss: 0.7790 | CLoss: 0.6175 | FLoss: 0.3231 | LR: 3.00e-04\n",
      "  Batch 800/899 | Loss: 1.1849 | CLoss: 0.9829 | FLoss: 0.4041 | LR: 3.00e-04\n",
      "  Batch 810/899 | Loss: 0.9360 | CLoss: 0.8172 | FLoss: 0.2376 | LR: 3.00e-04\n",
      "  Batch 820/899 | Loss: 0.8644 | CLoss: 0.6153 | FLoss: 0.4982 | LR: 3.00e-04\n",
      "  Batch 830/899 | Loss: 0.6820 | CLoss: 0.5292 | FLoss: 0.3056 | LR: 3.00e-04\n",
      "  Batch 840/899 | Loss: 0.8149 | CLoss: 0.6182 | FLoss: 0.3934 | LR: 3.00e-04\n",
      "  Batch 850/899 | Loss: 1.1562 | CLoss: 0.9817 | FLoss: 0.3490 | LR: 3.00e-04\n",
      "  Batch 860/899 | Loss: 0.8806 | CLoss: 0.5858 | FLoss: 0.5897 | LR: 3.00e-04\n",
      "  Batch 870/899 | Loss: 0.9359 | CLoss: 0.7124 | FLoss: 0.4470 | LR: 3.00e-04\n",
      "  Batch 880/899 | Loss: 1.1293 | CLoss: 0.9043 | FLoss: 0.4500 | LR: 3.00e-04\n",
      "  Batch 890/899 | Loss: 0.7432 | CLoss: 0.4880 | FLoss: 0.5104 | LR: 3.00e-04\n",
      "  Batch 899/899 | Loss: 0.7095 | CLoss: 0.0006 | FLoss: 1.4179 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 1.0186\n",
      "  Last Batch Loss: 0.7095\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.9417 | Batch Acc: 67.24%\n",
      "    Val Batch 010/101 | Loss: 0.2628 | Batch Acc: 93.10%\n",
      "    Val Batch 015/101 | Loss: 0.3407 | Batch Acc: 93.10%\n",
      "    Val Batch 020/101 | Loss: 0.7045 | Batch Acc: 55.17%\n",
      "    Val Batch 025/101 | Loss: 0.3218 | Batch Acc: 91.38%\n",
      "    Val Batch 030/101 | Loss: 0.8043 | Batch Acc: 82.76%\n",
      "    Val Batch 035/101 | Loss: 0.5347 | Batch Acc: 81.03%\n",
      "    Val Batch 040/101 | Loss: 0.5959 | Batch Acc: 87.93%\n",
      "    Val Batch 045/101 | Loss: 0.3704 | Batch Acc: 91.38%\n",
      "    Val Batch 050/101 | Loss: 0.3024 | Batch Acc: 86.21%\n",
      "    Val Batch 055/101 | Loss: 0.0601 | Batch Acc: 98.28%\n",
      "    Val Batch 060/101 | Loss: 0.2268 | Batch Acc: 89.66%\n",
      "    Val Batch 065/101 | Loss: 0.8556 | Batch Acc: 70.69%\n",
      "    Val Batch 070/101 | Loss: 0.4242 | Batch Acc: 84.48%\n",
      "    Val Batch 075/101 | Loss: 0.1724 | Batch Acc: 93.10%\n",
      "    Val Batch 080/101 | Loss: 0.4222 | Batch Acc: 82.76%\n",
      "    Val Batch 085/101 | Loss: 0.4753 | Batch Acc: 86.21%\n",
      "    Val Batch 090/101 | Loss: 0.2689 | Batch Acc: 91.38%\n",
      "    Val Batch 095/101 | Loss: 0.4280 | Batch Acc: 84.48%\n",
      "    Val Batch 100/101 | Loss: 0.3763 | Batch Acc: 86.21%\n",
      "    Val Batch 101/101 | Loss: 0.3916 | Batch Acc: 77.78%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.4608 | Accuracy: 84.25%\n",
      "  Current Best Acc: 84.25%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/899 | Loss: 0.9934 | CLoss: 0.7211 | FLoss: 0.5445 | LR: 2.71e-04\n",
      "  Batch 020/899 | Loss: 1.0500 | CLoss: 0.7831 | FLoss: 0.5338 | LR: 2.71e-04\n",
      "  Batch 030/899 | Loss: 0.8465 | CLoss: 0.5653 | FLoss: 0.5626 | LR: 2.71e-04\n",
      "  Batch 040/899 | Loss: 1.0411 | CLoss: 0.8335 | FLoss: 0.4152 | LR: 2.71e-04\n",
      "  Batch 050/899 | Loss: 0.9897 | CLoss: 0.7726 | FLoss: 0.4343 | LR: 2.71e-04\n",
      "  Batch 060/899 | Loss: 1.0982 | CLoss: 0.9066 | FLoss: 0.3831 | LR: 2.71e-04\n",
      "  Batch 070/899 | Loss: 0.7982 | CLoss: 0.5496 | FLoss: 0.4973 | LR: 2.71e-04\n",
      "  Batch 080/899 | Loss: 1.1807 | CLoss: 0.9131 | FLoss: 0.5352 | LR: 2.71e-04\n",
      "  Batch 090/899 | Loss: 0.4544 | CLoss: 0.3817 | FLoss: 0.1454 | LR: 2.71e-04\n",
      "  Batch 100/899 | Loss: 1.4619 | CLoss: 1.2066 | FLoss: 0.5105 | LR: 2.71e-04\n",
      "  Batch 110/899 | Loss: 0.8548 | CLoss: 0.6892 | FLoss: 0.3313 | LR: 2.71e-04\n",
      "  Batch 120/899 | Loss: 1.1587 | CLoss: 1.0098 | FLoss: 0.2976 | LR: 2.71e-04\n",
      "  Batch 130/899 | Loss: 0.9696 | CLoss: 0.6480 | FLoss: 0.6432 | LR: 2.71e-04\n",
      "  Batch 140/899 | Loss: 0.8285 | CLoss: 0.6979 | FLoss: 0.2612 | LR: 2.71e-04\n",
      "  Batch 150/899 | Loss: 1.1968 | CLoss: 1.0374 | FLoss: 0.3187 | LR: 2.71e-04\n",
      "  Batch 160/899 | Loss: 1.1073 | CLoss: 0.8658 | FLoss: 0.4830 | LR: 2.71e-04\n",
      "  Batch 170/899 | Loss: 1.2863 | CLoss: 1.1491 | FLoss: 0.2743 | LR: 2.71e-04\n",
      "  Batch 180/899 | Loss: 0.9044 | CLoss: 0.7463 | FLoss: 0.3162 | LR: 2.71e-04\n",
      "  Batch 190/899 | Loss: 1.2189 | CLoss: 0.9976 | FLoss: 0.4424 | LR: 2.71e-04\n",
      "  Batch 200/899 | Loss: 1.0577 | CLoss: 0.8664 | FLoss: 0.3828 | LR: 2.71e-04\n",
      "  Batch 210/899 | Loss: 0.6657 | CLoss: 0.4432 | FLoss: 0.4451 | LR: 2.71e-04\n",
      "  Batch 220/899 | Loss: 1.0087 | CLoss: 0.7079 | FLoss: 0.6016 | LR: 2.71e-04\n",
      "  Batch 230/899 | Loss: 0.9559 | CLoss: 0.6458 | FLoss: 0.6201 | LR: 2.71e-04\n",
      "  Batch 240/899 | Loss: 0.7867 | CLoss: 0.4662 | FLoss: 0.6411 | LR: 2.71e-04\n",
      "  Batch 250/899 | Loss: 0.6468 | CLoss: 0.5108 | FLoss: 0.2720 | LR: 2.71e-04\n",
      "  Batch 260/899 | Loss: 0.6528 | CLoss: 0.4700 | FLoss: 0.3657 | LR: 2.71e-04\n",
      "  Batch 270/899 | Loss: 0.8100 | CLoss: 0.5737 | FLoss: 0.4727 | LR: 2.71e-04\n",
      "  Batch 280/899 | Loss: 0.8180 | CLoss: 0.6797 | FLoss: 0.2764 | LR: 2.71e-04\n",
      "  Batch 290/899 | Loss: 0.4785 | CLoss: 0.3314 | FLoss: 0.2942 | LR: 2.71e-04\n",
      "  Batch 300/899 | Loss: 0.9462 | CLoss: 0.7759 | FLoss: 0.3405 | LR: 2.71e-04\n",
      "  Batch 310/899 | Loss: 0.9311 | CLoss: 0.7058 | FLoss: 0.4507 | LR: 2.71e-04\n",
      "  Batch 320/899 | Loss: 0.3349 | CLoss: 0.2546 | FLoss: 0.1607 | LR: 2.71e-04\n",
      "  Batch 330/899 | Loss: 0.7642 | CLoss: 0.6227 | FLoss: 0.2829 | LR: 2.71e-04\n",
      "  Batch 340/899 | Loss: 0.9704 | CLoss: 0.7585 | FLoss: 0.4237 | LR: 2.71e-04\n",
      "  Batch 350/899 | Loss: 1.0036 | CLoss: 0.7200 | FLoss: 0.5671 | LR: 2.71e-04\n",
      "  Batch 360/899 | Loss: 1.3409 | CLoss: 1.0817 | FLoss: 0.5183 | LR: 2.71e-04\n",
      "  Batch 370/899 | Loss: 0.8998 | CLoss: 0.7494 | FLoss: 0.3010 | LR: 2.71e-04\n",
      "  Batch 380/899 | Loss: 1.1281 | CLoss: 0.9007 | FLoss: 0.4549 | LR: 2.71e-04\n",
      "  Batch 390/899 | Loss: 1.1931 | CLoss: 1.0327 | FLoss: 0.3207 | LR: 2.71e-04\n",
      "  Batch 400/899 | Loss: 0.7803 | CLoss: 0.6021 | FLoss: 0.3563 | LR: 2.71e-04\n",
      "  Batch 410/899 | Loss: 0.7571 | CLoss: 0.5453 | FLoss: 0.4235 | LR: 2.71e-04\n",
      "  Batch 420/899 | Loss: 0.5822 | CLoss: 0.4154 | FLoss: 0.3335 | LR: 2.71e-04\n",
      "  Batch 430/899 | Loss: 0.8360 | CLoss: 0.5979 | FLoss: 0.4762 | LR: 2.71e-04\n",
      "  Batch 440/899 | Loss: 1.1994 | CLoss: 0.9374 | FLoss: 0.5240 | LR: 2.71e-04\n",
      "  Batch 450/899 | Loss: 0.7874 | CLoss: 0.6089 | FLoss: 0.3570 | LR: 2.71e-04\n",
      "  Batch 460/899 | Loss: 0.6081 | CLoss: 0.4300 | FLoss: 0.3561 | LR: 2.71e-04\n",
      "  Batch 470/899 | Loss: 0.4747 | CLoss: 0.3607 | FLoss: 0.2280 | LR: 2.71e-04\n",
      "  Batch 480/899 | Loss: 1.0533 | CLoss: 0.8278 | FLoss: 0.4509 | LR: 2.71e-04\n",
      "  Batch 490/899 | Loss: 0.9701 | CLoss: 0.7661 | FLoss: 0.4080 | LR: 2.71e-04\n",
      "  Batch 500/899 | Loss: 1.0679 | CLoss: 0.7828 | FLoss: 0.5701 | LR: 2.71e-04\n",
      "  Batch 510/899 | Loss: 0.8613 | CLoss: 0.6598 | FLoss: 0.4029 | LR: 2.71e-04\n",
      "  Batch 520/899 | Loss: 0.9409 | CLoss: 0.7062 | FLoss: 0.4694 | LR: 2.71e-04\n",
      "  Batch 530/899 | Loss: 0.7170 | CLoss: 0.5107 | FLoss: 0.4127 | LR: 2.71e-04\n",
      "  Batch 540/899 | Loss: 1.3407 | CLoss: 1.1101 | FLoss: 0.4613 | LR: 2.71e-04\n",
      "  Batch 550/899 | Loss: 0.7420 | CLoss: 0.5548 | FLoss: 0.3744 | LR: 2.71e-04\n",
      "  Batch 560/899 | Loss: 0.9722 | CLoss: 0.7707 | FLoss: 0.4030 | LR: 2.71e-04\n",
      "  Batch 570/899 | Loss: 1.0177 | CLoss: 0.8623 | FLoss: 0.3109 | LR: 2.71e-04\n",
      "  Batch 580/899 | Loss: 0.8233 | CLoss: 0.6222 | FLoss: 0.4023 | LR: 2.71e-04\n",
      "  Batch 590/899 | Loss: 0.9270 | CLoss: 0.7346 | FLoss: 0.3849 | LR: 2.71e-04\n",
      "  Batch 600/899 | Loss: 0.7868 | CLoss: 0.4509 | FLoss: 0.6717 | LR: 2.71e-04\n",
      "  Batch 610/899 | Loss: 0.7638 | CLoss: 0.5960 | FLoss: 0.3355 | LR: 2.71e-04\n",
      "  Batch 620/899 | Loss: 0.7903 | CLoss: 0.6382 | FLoss: 0.3043 | LR: 2.71e-04\n",
      "  Batch 630/899 | Loss: 0.9396 | CLoss: 0.6707 | FLoss: 0.5378 | LR: 2.71e-04\n",
      "  Batch 640/899 | Loss: 1.0446 | CLoss: 0.8121 | FLoss: 0.4651 | LR: 2.71e-04\n",
      "  Batch 650/899 | Loss: 0.7895 | CLoss: 0.4961 | FLoss: 0.5869 | LR: 2.71e-04\n",
      "  Batch 660/899 | Loss: 0.8190 | CLoss: 0.6035 | FLoss: 0.4310 | LR: 2.71e-04\n",
      "  Batch 670/899 | Loss: 0.6366 | CLoss: 0.4635 | FLoss: 0.3463 | LR: 2.71e-04\n",
      "  Batch 680/899 | Loss: 0.7677 | CLoss: 0.5717 | FLoss: 0.3921 | LR: 2.71e-04\n",
      "  Batch 690/899 | Loss: 0.6459 | CLoss: 0.5043 | FLoss: 0.2833 | LR: 2.71e-04\n",
      "  Batch 700/899 | Loss: 1.0147 | CLoss: 0.7984 | FLoss: 0.4326 | LR: 2.71e-04\n",
      "  Batch 710/899 | Loss: 1.0303 | CLoss: 0.7727 | FLoss: 0.5151 | LR: 2.71e-04\n",
      "  Batch 720/899 | Loss: 0.7567 | CLoss: 0.5506 | FLoss: 0.4122 | LR: 2.71e-04\n",
      "  Batch 730/899 | Loss: 0.8345 | CLoss: 0.6737 | FLoss: 0.3217 | LR: 2.71e-04\n",
      "  Batch 740/899 | Loss: 0.7190 | CLoss: 0.5058 | FLoss: 0.4265 | LR: 2.71e-04\n",
      "  Batch 750/899 | Loss: 0.5453 | CLoss: 0.4300 | FLoss: 0.2305 | LR: 2.71e-04\n",
      "  Batch 760/899 | Loss: 0.8030 | CLoss: 0.6429 | FLoss: 0.3202 | LR: 2.71e-04\n",
      "  Batch 770/899 | Loss: 0.9185 | CLoss: 0.6720 | FLoss: 0.4930 | LR: 2.71e-04\n",
      "  Batch 780/899 | Loss: 0.9459 | CLoss: 0.7185 | FLoss: 0.4549 | LR: 2.71e-04\n",
      "  Batch 790/899 | Loss: 1.1671 | CLoss: 0.9620 | FLoss: 0.4103 | LR: 2.71e-04\n",
      "  Batch 800/899 | Loss: 0.8536 | CLoss: 0.6617 | FLoss: 0.3837 | LR: 2.71e-04\n",
      "  Batch 810/899 | Loss: 0.6663 | CLoss: 0.4907 | FLoss: 0.3511 | LR: 2.71e-04\n",
      "  Batch 820/899 | Loss: 0.6019 | CLoss: 0.4172 | FLoss: 0.3694 | LR: 2.71e-04\n",
      "  Batch 830/899 | Loss: 0.4872 | CLoss: 0.3532 | FLoss: 0.2680 | LR: 2.71e-04\n",
      "  Batch 840/899 | Loss: 0.8358 | CLoss: 0.5495 | FLoss: 0.5726 | LR: 2.71e-04\n",
      "  Batch 850/899 | Loss: 0.9410 | CLoss: 0.7177 | FLoss: 0.4465 | LR: 2.71e-04\n",
      "  Batch 860/899 | Loss: 0.5014 | CLoss: 0.3314 | FLoss: 0.3399 | LR: 2.71e-04\n",
      "  Batch 870/899 | Loss: 0.7252 | CLoss: 0.5847 | FLoss: 0.2808 | LR: 2.71e-04\n",
      "  Batch 880/899 | Loss: 0.8461 | CLoss: 0.6616 | FLoss: 0.3690 | LR: 2.71e-04\n",
      "  Batch 890/899 | Loss: 0.8803 | CLoss: 0.6727 | FLoss: 0.4153 | LR: 2.71e-04\n",
      "  Batch 899/899 | Loss: 0.2375 | CLoss: 0.0023 | FLoss: 0.4703 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.8856\n",
      "  Last Batch Loss: 0.2375\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 1.2501 | Batch Acc: 55.17%\n",
      "    Val Batch 010/101 | Loss: 0.2707 | Batch Acc: 89.66%\n",
      "    Val Batch 015/101 | Loss: 0.2536 | Batch Acc: 93.10%\n",
      "    Val Batch 020/101 | Loss: 1.1067 | Batch Acc: 48.28%\n",
      "    Val Batch 025/101 | Loss: 0.4527 | Batch Acc: 84.48%\n",
      "    Val Batch 030/101 | Loss: 0.5946 | Batch Acc: 84.48%\n",
      "    Val Batch 035/101 | Loss: 0.5969 | Batch Acc: 77.59%\n",
      "    Val Batch 040/101 | Loss: 0.5983 | Batch Acc: 86.21%\n",
      "    Val Batch 045/101 | Loss: 0.2954 | Batch Acc: 100.00%\n",
      "    Val Batch 050/101 | Loss: 0.2096 | Batch Acc: 94.83%\n",
      "    Val Batch 055/101 | Loss: 0.2814 | Batch Acc: 91.38%\n",
      "    Val Batch 060/101 | Loss: 0.3046 | Batch Acc: 87.93%\n",
      "    Val Batch 065/101 | Loss: 0.8145 | Batch Acc: 68.97%\n",
      "    Val Batch 070/101 | Loss: 0.3500 | Batch Acc: 84.48%\n",
      "    Val Batch 075/101 | Loss: 0.1777 | Batch Acc: 93.10%\n",
      "    Val Batch 080/101 | Loss: 0.5320 | Batch Acc: 82.76%\n",
      "    Val Batch 085/101 | Loss: 0.5292 | Batch Acc: 82.76%\n",
      "    Val Batch 090/101 | Loss: 0.1424 | Batch Acc: 96.55%\n",
      "    Val Batch 095/101 | Loss: 0.3820 | Batch Acc: 89.66%\n",
      "    Val Batch 100/101 | Loss: 0.1568 | Batch Acc: 96.55%\n",
      "    Val Batch 101/101 | Loss: 0.3737 | Batch Acc: 88.89%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.4779 | Accuracy: 84.04%\n",
      "  Current Best Acc: 84.25%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/899 | Loss: 0.9199 | CLoss: 0.7446 | FLoss: 0.3505 | LR: 1.96e-04\n",
      "  Batch 020/899 | Loss: 0.7755 | CLoss: 0.6389 | FLoss: 0.2733 | LR: 1.96e-04\n",
      "  Batch 030/899 | Loss: 0.9172 | CLoss: 0.7017 | FLoss: 0.4311 | LR: 1.96e-04\n",
      "  Batch 040/899 | Loss: 0.8163 | CLoss: 0.6061 | FLoss: 0.4204 | LR: 1.96e-04\n",
      "  Batch 050/899 | Loss: 1.0293 | CLoss: 0.7749 | FLoss: 0.5087 | LR: 1.96e-04\n",
      "  Batch 060/899 | Loss: 0.8711 | CLoss: 0.7035 | FLoss: 0.3352 | LR: 1.96e-04\n",
      "  Batch 070/899 | Loss: 0.7201 | CLoss: 0.5379 | FLoss: 0.3645 | LR: 1.96e-04\n",
      "  Batch 080/899 | Loss: 0.6545 | CLoss: 0.5309 | FLoss: 0.2472 | LR: 1.96e-04\n",
      "  Batch 090/899 | Loss: 0.6229 | CLoss: 0.4877 | FLoss: 0.2705 | LR: 1.96e-04\n",
      "  Batch 100/899 | Loss: 1.3048 | CLoss: 0.9741 | FLoss: 0.6614 | LR: 1.96e-04\n",
      "  Batch 110/899 | Loss: 0.6782 | CLoss: 0.4536 | FLoss: 0.4492 | LR: 1.96e-04\n",
      "  Batch 120/899 | Loss: 0.4040 | CLoss: 0.2851 | FLoss: 0.2377 | LR: 1.96e-04\n",
      "  Batch 130/899 | Loss: 0.5633 | CLoss: 0.4853 | FLoss: 0.1560 | LR: 1.96e-04\n",
      "  Batch 140/899 | Loss: 0.6951 | CLoss: 0.5124 | FLoss: 0.3655 | LR: 1.96e-04\n",
      "  Batch 150/899 | Loss: 1.0090 | CLoss: 0.7578 | FLoss: 0.5023 | LR: 1.96e-04\n",
      "  Batch 160/899 | Loss: 0.9389 | CLoss: 0.6435 | FLoss: 0.5908 | LR: 1.96e-04\n",
      "  Batch 170/899 | Loss: 1.2565 | CLoss: 1.1268 | FLoss: 0.2595 | LR: 1.96e-04\n",
      "  Batch 180/899 | Loss: 0.9403 | CLoss: 0.6696 | FLoss: 0.5414 | LR: 1.96e-04\n",
      "  Batch 190/899 | Loss: 0.5346 | CLoss: 0.3943 | FLoss: 0.2805 | LR: 1.96e-04\n",
      "  Batch 200/899 | Loss: 0.6430 | CLoss: 0.4988 | FLoss: 0.2885 | LR: 1.96e-04\n",
      "  Batch 210/899 | Loss: 0.8349 | CLoss: 0.6786 | FLoss: 0.3126 | LR: 1.96e-04\n",
      "  Batch 220/899 | Loss: 0.7535 | CLoss: 0.4798 | FLoss: 0.5475 | LR: 1.96e-04\n",
      "  Batch 230/899 | Loss: 0.6386 | CLoss: 0.5228 | FLoss: 0.2318 | LR: 1.96e-04\n",
      "  Batch 240/899 | Loss: 0.8602 | CLoss: 0.6894 | FLoss: 0.3417 | LR: 1.96e-04\n",
      "  Batch 250/899 | Loss: 0.7311 | CLoss: 0.6107 | FLoss: 0.2409 | LR: 1.96e-04\n",
      "  Batch 260/899 | Loss: 0.7975 | CLoss: 0.6155 | FLoss: 0.3640 | LR: 1.96e-04\n",
      "  Batch 270/899 | Loss: 0.8745 | CLoss: 0.6881 | FLoss: 0.3729 | LR: 1.96e-04\n",
      "  Batch 280/899 | Loss: 0.4452 | CLoss: 0.3208 | FLoss: 0.2487 | LR: 1.96e-04\n",
      "  Batch 290/899 | Loss: 0.8205 | CLoss: 0.5614 | FLoss: 0.5182 | LR: 1.96e-04\n",
      "  Batch 300/899 | Loss: 1.0714 | CLoss: 0.8473 | FLoss: 0.4481 | LR: 1.96e-04\n",
      "  Batch 310/899 | Loss: 0.6531 | CLoss: 0.5227 | FLoss: 0.2608 | LR: 1.96e-04\n",
      "  Batch 320/899 | Loss: 0.9338 | CLoss: 0.7331 | FLoss: 0.4015 | LR: 1.96e-04\n",
      "  Batch 330/899 | Loss: 0.5819 | CLoss: 0.4057 | FLoss: 0.3523 | LR: 1.96e-04\n",
      "  Batch 340/899 | Loss: 1.1166 | CLoss: 0.8043 | FLoss: 0.6245 | LR: 1.96e-04\n",
      "  Batch 350/899 | Loss: 0.5092 | CLoss: 0.3742 | FLoss: 0.2701 | LR: 1.96e-04\n",
      "  Batch 360/899 | Loss: 1.0575 | CLoss: 0.8350 | FLoss: 0.4449 | LR: 1.96e-04\n",
      "  Batch 370/899 | Loss: 0.9501 | CLoss: 0.6887 | FLoss: 0.5227 | LR: 1.96e-04\n",
      "  Batch 380/899 | Loss: 1.3317 | CLoss: 0.9906 | FLoss: 0.6822 | LR: 1.96e-04\n",
      "  Batch 390/899 | Loss: 0.9307 | CLoss: 0.7745 | FLoss: 0.3124 | LR: 1.96e-04\n",
      "  Batch 400/899 | Loss: 0.7127 | CLoss: 0.5345 | FLoss: 0.3562 | LR: 1.96e-04\n",
      "  Batch 410/899 | Loss: 0.9566 | CLoss: 0.7707 | FLoss: 0.3717 | LR: 1.96e-04\n",
      "  Batch 420/899 | Loss: 0.7058 | CLoss: 0.5442 | FLoss: 0.3232 | LR: 1.96e-04\n",
      "  Batch 430/899 | Loss: 0.4803 | CLoss: 0.3525 | FLoss: 0.2558 | LR: 1.96e-04\n",
      "  Batch 440/899 | Loss: 0.3746 | CLoss: 0.2931 | FLoss: 0.1630 | LR: 1.96e-04\n",
      "  Batch 450/899 | Loss: 0.7860 | CLoss: 0.5267 | FLoss: 0.5186 | LR: 1.96e-04\n",
      "  Batch 460/899 | Loss: 0.6941 | CLoss: 0.5628 | FLoss: 0.2626 | LR: 1.96e-04\n",
      "  Batch 470/899 | Loss: 0.5035 | CLoss: 0.3797 | FLoss: 0.2476 | LR: 1.96e-04\n",
      "  Batch 480/899 | Loss: 0.9436 | CLoss: 0.7267 | FLoss: 0.4339 | LR: 1.96e-04\n",
      "  Batch 490/899 | Loss: 0.8211 | CLoss: 0.5818 | FLoss: 0.4786 | LR: 1.96e-04\n",
      "  Batch 500/899 | Loss: 1.1549 | CLoss: 0.9081 | FLoss: 0.4935 | LR: 1.96e-04\n",
      "  Batch 510/899 | Loss: 0.7261 | CLoss: 0.5675 | FLoss: 0.3172 | LR: 1.96e-04\n",
      "  Batch 520/899 | Loss: 0.7384 | CLoss: 0.5886 | FLoss: 0.2996 | LR: 1.96e-04\n",
      "  Batch 530/899 | Loss: 1.3299 | CLoss: 1.0348 | FLoss: 0.5902 | LR: 1.96e-04\n",
      "  Batch 540/899 | Loss: 0.5526 | CLoss: 0.4145 | FLoss: 0.2761 | LR: 1.96e-04\n",
      "  Batch 550/899 | Loss: 0.8148 | CLoss: 0.6654 | FLoss: 0.2987 | LR: 1.96e-04\n",
      "  Batch 560/899 | Loss: 0.3778 | CLoss: 0.2875 | FLoss: 0.1806 | LR: 1.96e-04\n",
      "  Batch 570/899 | Loss: 1.0158 | CLoss: 0.8131 | FLoss: 0.4053 | LR: 1.96e-04\n",
      "  Batch 580/899 | Loss: 0.7570 | CLoss: 0.6619 | FLoss: 0.1902 | LR: 1.96e-04\n",
      "  Batch 590/899 | Loss: 0.5828 | CLoss: 0.4040 | FLoss: 0.3576 | LR: 1.96e-04\n",
      "  Batch 600/899 | Loss: 1.1322 | CLoss: 0.9215 | FLoss: 0.4215 | LR: 1.96e-04\n",
      "  Batch 610/899 | Loss: 0.6155 | CLoss: 0.4872 | FLoss: 0.2566 | LR: 1.96e-04\n",
      "  Batch 620/899 | Loss: 0.5200 | CLoss: 0.3468 | FLoss: 0.3464 | LR: 1.96e-04\n",
      "  Batch 630/899 | Loss: 0.7447 | CLoss: 0.5502 | FLoss: 0.3890 | LR: 1.96e-04\n",
      "  Batch 640/899 | Loss: 1.1224 | CLoss: 0.8796 | FLoss: 0.4856 | LR: 1.96e-04\n",
      "  Batch 650/899 | Loss: 0.4539 | CLoss: 0.2981 | FLoss: 0.3116 | LR: 1.96e-04\n",
      "  Batch 660/899 | Loss: 0.5606 | CLoss: 0.4077 | FLoss: 0.3057 | LR: 1.96e-04\n",
      "  Batch 670/899 | Loss: 0.7475 | CLoss: 0.6002 | FLoss: 0.2946 | LR: 1.96e-04\n",
      "  Batch 680/899 | Loss: 1.0134 | CLoss: 0.7363 | FLoss: 0.5541 | LR: 1.96e-04\n",
      "  Batch 690/899 | Loss: 0.7630 | CLoss: 0.6051 | FLoss: 0.3158 | LR: 1.96e-04\n",
      "  Batch 700/899 | Loss: 1.0525 | CLoss: 0.8458 | FLoss: 0.4133 | LR: 1.96e-04\n",
      "  Batch 710/899 | Loss: 0.7488 | CLoss: 0.5336 | FLoss: 0.4304 | LR: 1.96e-04\n",
      "  Batch 720/899 | Loss: 0.9096 | CLoss: 0.7477 | FLoss: 0.3238 | LR: 1.96e-04\n",
      "  Batch 730/899 | Loss: 0.5755 | CLoss: 0.4254 | FLoss: 0.3002 | LR: 1.96e-04\n",
      "  Batch 740/899 | Loss: 0.6118 | CLoss: 0.4429 | FLoss: 0.3377 | LR: 1.96e-04\n",
      "  Batch 750/899 | Loss: 0.7533 | CLoss: 0.5646 | FLoss: 0.3774 | LR: 1.96e-04\n",
      "  Batch 760/899 | Loss: 0.8753 | CLoss: 0.6562 | FLoss: 0.4382 | LR: 1.96e-04\n",
      "  Batch 770/899 | Loss: 0.5871 | CLoss: 0.4798 | FLoss: 0.2148 | LR: 1.96e-04\n",
      "  Batch 780/899 | Loss: 0.7261 | CLoss: 0.5998 | FLoss: 0.2526 | LR: 1.96e-04\n",
      "  Batch 790/899 | Loss: 0.4138 | CLoss: 0.3160 | FLoss: 0.1956 | LR: 1.96e-04\n",
      "  Batch 800/899 | Loss: 1.0805 | CLoss: 0.8316 | FLoss: 0.4980 | LR: 1.96e-04\n",
      "  Batch 810/899 | Loss: 0.6982 | CLoss: 0.5966 | FLoss: 0.2032 | LR: 1.96e-04\n",
      "  Batch 820/899 | Loss: 0.4423 | CLoss: 0.3688 | FLoss: 0.1470 | LR: 1.96e-04\n",
      "  Batch 830/899 | Loss: 1.4180 | CLoss: 1.0621 | FLoss: 0.7118 | LR: 1.96e-04\n",
      "  Batch 840/899 | Loss: 1.3268 | CLoss: 1.0362 | FLoss: 0.5813 | LR: 1.96e-04\n",
      "  Batch 850/899 | Loss: 1.2153 | CLoss: 0.9302 | FLoss: 0.5703 | LR: 1.96e-04\n",
      "  Batch 860/899 | Loss: 0.7924 | CLoss: 0.5829 | FLoss: 0.4191 | LR: 1.96e-04\n",
      "  Batch 870/899 | Loss: 0.4993 | CLoss: 0.3821 | FLoss: 0.2345 | LR: 1.96e-04\n",
      "  Batch 880/899 | Loss: 0.8822 | CLoss: 0.7601 | FLoss: 0.2440 | LR: 1.96e-04\n",
      "  Batch 890/899 | Loss: 0.5690 | CLoss: 0.4272 | FLoss: 0.2835 | LR: 1.96e-04\n",
      "  Batch 899/899 | Loss: 0.1691 | CLoss: 0.0056 | FLoss: 0.3270 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.7781\n",
      "  Last Batch Loss: 0.1691\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.7317 | Batch Acc: 72.41%\n",
      "    Val Batch 010/101 | Loss: 0.3317 | Batch Acc: 82.76%\n",
      "    Val Batch 015/101 | Loss: 0.5082 | Batch Acc: 82.76%\n",
      "    Val Batch 020/101 | Loss: 1.3280 | Batch Acc: 39.66%\n",
      "    Val Batch 025/101 | Loss: 0.3800 | Batch Acc: 91.38%\n",
      "    Val Batch 030/101 | Loss: 0.6400 | Batch Acc: 84.48%\n",
      "    Val Batch 035/101 | Loss: 0.6419 | Batch Acc: 82.76%\n",
      "    Val Batch 040/101 | Loss: 0.5353 | Batch Acc: 94.83%\n",
      "    Val Batch 045/101 | Loss: 0.3804 | Batch Acc: 96.55%\n",
      "    Val Batch 050/101 | Loss: 0.2087 | Batch Acc: 94.83%\n",
      "    Val Batch 055/101 | Loss: 0.2623 | Batch Acc: 89.66%\n",
      "    Val Batch 060/101 | Loss: 0.2085 | Batch Acc: 93.10%\n",
      "    Val Batch 065/101 | Loss: 0.8837 | Batch Acc: 70.69%\n",
      "    Val Batch 070/101 | Loss: 0.2642 | Batch Acc: 89.66%\n",
      "    Val Batch 075/101 | Loss: 0.1894 | Batch Acc: 93.10%\n",
      "    Val Batch 080/101 | Loss: 0.2871 | Batch Acc: 89.66%\n",
      "    Val Batch 085/101 | Loss: 0.3743 | Batch Acc: 87.93%\n",
      "    Val Batch 090/101 | Loss: 0.0923 | Batch Acc: 96.55%\n",
      "    Val Batch 095/101 | Loss: 0.2875 | Batch Acc: 93.10%\n",
      "    Val Batch 100/101 | Loss: 0.1512 | Batch Acc: 96.55%\n",
      "    Val Batch 101/101 | Loss: 0.2953 | Batch Acc: 92.59%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.4740 | Accuracy: 83.82%\n",
      "  Current Best Acc: 84.25%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/899 | Loss: 0.7113 | CLoss: 0.5326 | FLoss: 0.3572 | LR: 1.04e-04\n",
      "  Batch 020/899 | Loss: 0.3468 | CLoss: 0.2662 | FLoss: 0.1611 | LR: 1.04e-04\n",
      "  Batch 030/899 | Loss: 0.5064 | CLoss: 0.3688 | FLoss: 0.2752 | LR: 1.04e-04\n",
      "  Batch 040/899 | Loss: 0.8311 | CLoss: 0.7068 | FLoss: 0.2487 | LR: 1.04e-04\n",
      "  Batch 050/899 | Loss: 0.9108 | CLoss: 0.7544 | FLoss: 0.3128 | LR: 1.04e-04\n",
      "  Batch 060/899 | Loss: 0.6241 | CLoss: 0.4796 | FLoss: 0.2892 | LR: 1.04e-04\n",
      "  Batch 070/899 | Loss: 0.7881 | CLoss: 0.6341 | FLoss: 0.3079 | LR: 1.04e-04\n",
      "  Batch 080/899 | Loss: 0.6108 | CLoss: 0.4901 | FLoss: 0.2414 | LR: 1.04e-04\n",
      "  Batch 090/899 | Loss: 0.4016 | CLoss: 0.3126 | FLoss: 0.1780 | LR: 1.04e-04\n",
      "  Batch 100/899 | Loss: 0.3003 | CLoss: 0.1943 | FLoss: 0.2120 | LR: 1.04e-04\n",
      "  Batch 110/899 | Loss: 1.0669 | CLoss: 0.8305 | FLoss: 0.4729 | LR: 1.04e-04\n",
      "  Batch 120/899 | Loss: 0.2919 | CLoss: 0.2061 | FLoss: 0.1715 | LR: 1.04e-04\n",
      "  Batch 130/899 | Loss: 1.1372 | CLoss: 0.9028 | FLoss: 0.4687 | LR: 1.04e-04\n",
      "  Batch 140/899 | Loss: 0.5821 | CLoss: 0.4329 | FLoss: 0.2984 | LR: 1.04e-04\n",
      "  Batch 150/899 | Loss: 0.7016 | CLoss: 0.5298 | FLoss: 0.3437 | LR: 1.04e-04\n",
      "  Batch 160/899 | Loss: 0.4720 | CLoss: 0.3205 | FLoss: 0.3031 | LR: 1.04e-04\n",
      "  Batch 170/899 | Loss: 0.4058 | CLoss: 0.2746 | FLoss: 0.2624 | LR: 1.04e-04\n",
      "  Batch 180/899 | Loss: 0.5386 | CLoss: 0.3710 | FLoss: 0.3352 | LR: 1.04e-04\n",
      "  Batch 190/899 | Loss: 0.6224 | CLoss: 0.4558 | FLoss: 0.3334 | LR: 1.04e-04\n",
      "  Batch 200/899 | Loss: 0.7907 | CLoss: 0.6047 | FLoss: 0.3719 | LR: 1.04e-04\n",
      "  Batch 210/899 | Loss: 0.8844 | CLoss: 0.7178 | FLoss: 0.3332 | LR: 1.04e-04\n",
      "  Batch 220/899 | Loss: 0.6078 | CLoss: 0.4811 | FLoss: 0.2534 | LR: 1.04e-04\n",
      "  Batch 230/899 | Loss: 0.6988 | CLoss: 0.5518 | FLoss: 0.2940 | LR: 1.04e-04\n",
      "  Batch 240/899 | Loss: 0.5586 | CLoss: 0.4460 | FLoss: 0.2252 | LR: 1.04e-04\n",
      "  Batch 250/899 | Loss: 0.4321 | CLoss: 0.2637 | FLoss: 0.3368 | LR: 1.04e-04\n",
      "  Batch 260/899 | Loss: 0.7050 | CLoss: 0.5155 | FLoss: 0.3789 | LR: 1.04e-04\n",
      "  Batch 270/899 | Loss: 0.4225 | CLoss: 0.3325 | FLoss: 0.1799 | LR: 1.04e-04\n",
      "  Batch 280/899 | Loss: 0.4560 | CLoss: 0.3113 | FLoss: 0.2894 | LR: 1.04e-04\n",
      "  Batch 290/899 | Loss: 0.5226 | CLoss: 0.4508 | FLoss: 0.1437 | LR: 1.04e-04\n",
      "  Batch 300/899 | Loss: 0.7790 | CLoss: 0.6359 | FLoss: 0.2861 | LR: 1.04e-04\n",
      "  Batch 310/899 | Loss: 0.8247 | CLoss: 0.6111 | FLoss: 0.4271 | LR: 1.04e-04\n",
      "  Batch 320/899 | Loss: 1.1268 | CLoss: 0.9327 | FLoss: 0.3883 | LR: 1.04e-04\n",
      "  Batch 330/899 | Loss: 0.9544 | CLoss: 0.7081 | FLoss: 0.4926 | LR: 1.04e-04\n",
      "  Batch 340/899 | Loss: 0.5509 | CLoss: 0.4432 | FLoss: 0.2154 | LR: 1.04e-04\n",
      "  Batch 350/899 | Loss: 0.9448 | CLoss: 0.7062 | FLoss: 0.4772 | LR: 1.04e-04\n",
      "  Batch 360/899 | Loss: 0.6789 | CLoss: 0.4972 | FLoss: 0.3634 | LR: 1.04e-04\n",
      "  Batch 370/899 | Loss: 0.5964 | CLoss: 0.4811 | FLoss: 0.2306 | LR: 1.04e-04\n",
      "  Batch 380/899 | Loss: 0.7156 | CLoss: 0.5424 | FLoss: 0.3463 | LR: 1.04e-04\n",
      "  Batch 390/899 | Loss: 0.5806 | CLoss: 0.4580 | FLoss: 0.2452 | LR: 1.04e-04\n",
      "  Batch 400/899 | Loss: 0.5875 | CLoss: 0.4674 | FLoss: 0.2403 | LR: 1.04e-04\n",
      "  Batch 410/899 | Loss: 0.7743 | CLoss: 0.5627 | FLoss: 0.4232 | LR: 1.04e-04\n",
      "  Batch 420/899 | Loss: 0.5061 | CLoss: 0.3752 | FLoss: 0.2618 | LR: 1.04e-04\n",
      "  Batch 430/899 | Loss: 0.3761 | CLoss: 0.3003 | FLoss: 0.1515 | LR: 1.04e-04\n",
      "  Batch 440/899 | Loss: 0.5190 | CLoss: 0.3563 | FLoss: 0.3254 | LR: 1.04e-04\n",
      "  Batch 450/899 | Loss: 0.4301 | CLoss: 0.2800 | FLoss: 0.3001 | LR: 1.04e-04\n",
      "  Batch 460/899 | Loss: 0.4048 | CLoss: 0.3286 | FLoss: 0.1525 | LR: 1.04e-04\n",
      "  Batch 470/899 | Loss: 0.7313 | CLoss: 0.6007 | FLoss: 0.2613 | LR: 1.04e-04\n",
      "  Batch 480/899 | Loss: 0.6513 | CLoss: 0.5108 | FLoss: 0.2810 | LR: 1.04e-04\n",
      "  Batch 490/899 | Loss: 0.6477 | CLoss: 0.4756 | FLoss: 0.3441 | LR: 1.04e-04\n",
      "  Batch 500/899 | Loss: 1.3509 | CLoss: 1.0255 | FLoss: 0.6510 | LR: 1.04e-04\n",
      "  Batch 510/899 | Loss: 1.2485 | CLoss: 0.9468 | FLoss: 0.6034 | LR: 1.04e-04\n",
      "  Batch 520/899 | Loss: 1.0060 | CLoss: 0.8252 | FLoss: 0.3616 | LR: 1.04e-04\n",
      "  Batch 530/899 | Loss: 0.7098 | CLoss: 0.5570 | FLoss: 0.3056 | LR: 1.04e-04\n",
      "  Batch 540/899 | Loss: 0.8056 | CLoss: 0.6320 | FLoss: 0.3471 | LR: 1.04e-04\n",
      "  Batch 550/899 | Loss: 0.7701 | CLoss: 0.5852 | FLoss: 0.3698 | LR: 1.04e-04\n",
      "  Batch 560/899 | Loss: 0.5534 | CLoss: 0.3433 | FLoss: 0.4202 | LR: 1.04e-04\n",
      "  Batch 570/899 | Loss: 0.5277 | CLoss: 0.4026 | FLoss: 0.2501 | LR: 1.04e-04\n",
      "  Batch 580/899 | Loss: 0.7259 | CLoss: 0.5577 | FLoss: 0.3364 | LR: 1.04e-04\n",
      "  Batch 590/899 | Loss: 0.4863 | CLoss: 0.3788 | FLoss: 0.2150 | LR: 1.04e-04\n",
      "  Batch 600/899 | Loss: 0.3912 | CLoss: 0.2918 | FLoss: 0.1989 | LR: 1.04e-04\n",
      "  Batch 610/899 | Loss: 0.6051 | CLoss: 0.5307 | FLoss: 0.1488 | LR: 1.04e-04\n",
      "  Batch 620/899 | Loss: 0.7924 | CLoss: 0.6271 | FLoss: 0.3305 | LR: 1.04e-04\n",
      "  Batch 630/899 | Loss: 0.6983 | CLoss: 0.5763 | FLoss: 0.2439 | LR: 1.04e-04\n",
      "  Batch 640/899 | Loss: 1.0813 | CLoss: 0.7969 | FLoss: 0.5688 | LR: 1.04e-04\n",
      "  Batch 650/899 | Loss: 0.4436 | CLoss: 0.3280 | FLoss: 0.2311 | LR: 1.04e-04\n",
      "  Batch 660/899 | Loss: 0.7135 | CLoss: 0.5487 | FLoss: 0.3296 | LR: 1.04e-04\n",
      "  Batch 670/899 | Loss: 0.6336 | CLoss: 0.4918 | FLoss: 0.2835 | LR: 1.04e-04\n",
      "  Batch 680/899 | Loss: 0.6437 | CLoss: 0.4673 | FLoss: 0.3527 | LR: 1.04e-04\n",
      "  Batch 690/899 | Loss: 0.3231 | CLoss: 0.2384 | FLoss: 0.1693 | LR: 1.04e-04\n",
      "  Batch 700/899 | Loss: 1.0029 | CLoss: 0.8234 | FLoss: 0.3589 | LR: 1.04e-04\n",
      "  Batch 710/899 | Loss: 0.7374 | CLoss: 0.5823 | FLoss: 0.3101 | LR: 1.04e-04\n",
      "  Batch 720/899 | Loss: 0.6260 | CLoss: 0.4613 | FLoss: 0.3294 | LR: 1.04e-04\n",
      "  Batch 730/899 | Loss: 0.8551 | CLoss: 0.6962 | FLoss: 0.3178 | LR: 1.04e-04\n",
      "  Batch 740/899 | Loss: 0.8477 | CLoss: 0.7026 | FLoss: 0.2903 | LR: 1.04e-04\n",
      "  Batch 750/899 | Loss: 0.8493 | CLoss: 0.6821 | FLoss: 0.3345 | LR: 1.04e-04\n",
      "  Batch 760/899 | Loss: 1.0446 | CLoss: 0.7949 | FLoss: 0.4995 | LR: 1.04e-04\n",
      "  Batch 770/899 | Loss: 0.6614 | CLoss: 0.4620 | FLoss: 0.3987 | LR: 1.04e-04\n",
      "  Batch 780/899 | Loss: 1.0897 | CLoss: 0.9031 | FLoss: 0.3731 | LR: 1.04e-04\n",
      "  Batch 790/899 | Loss: 1.2245 | CLoss: 1.0129 | FLoss: 0.4232 | LR: 1.04e-04\n",
      "  Batch 800/899 | Loss: 0.8913 | CLoss: 0.7396 | FLoss: 0.3034 | LR: 1.04e-04\n",
      "  Batch 810/899 | Loss: 0.6736 | CLoss: 0.5973 | FLoss: 0.1526 | LR: 1.04e-04\n",
      "  Batch 820/899 | Loss: 0.7843 | CLoss: 0.6192 | FLoss: 0.3303 | LR: 1.04e-04\n",
      "  Batch 830/899 | Loss: 0.6149 | CLoss: 0.4366 | FLoss: 0.3566 | LR: 1.04e-04\n",
      "  Batch 840/899 | Loss: 0.5511 | CLoss: 0.3543 | FLoss: 0.3937 | LR: 1.04e-04\n",
      "  Batch 850/899 | Loss: 0.4311 | CLoss: 0.2841 | FLoss: 0.2940 | LR: 1.04e-04\n",
      "  Batch 860/899 | Loss: 0.3780 | CLoss: 0.2518 | FLoss: 0.2524 | LR: 1.04e-04\n",
      "  Batch 870/899 | Loss: 1.0379 | CLoss: 0.7662 | FLoss: 0.5434 | LR: 1.04e-04\n",
      "  Batch 880/899 | Loss: 0.7138 | CLoss: 0.5326 | FLoss: 0.3623 | LR: 1.04e-04\n",
      "  Batch 890/899 | Loss: 0.7510 | CLoss: 0.6315 | FLoss: 0.2391 | LR: 1.04e-04\n",
      "  Batch 899/899 | Loss: 0.6412 | CLoss: 0.3308 | FLoss: 0.6207 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.6998\n",
      "  Last Batch Loss: 0.6412\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.8412 | Batch Acc: 65.52%\n",
      "    Val Batch 010/101 | Loss: 0.0894 | Batch Acc: 96.55%\n",
      "    Val Batch 015/101 | Loss: 0.3727 | Batch Acc: 91.38%\n",
      "    Val Batch 020/101 | Loss: 1.0174 | Batch Acc: 77.59%\n",
      "    Val Batch 025/101 | Loss: 0.8358 | Batch Acc: 68.97%\n",
      "    Val Batch 030/101 | Loss: 0.9562 | Batch Acc: 81.03%\n",
      "    Val Batch 035/101 | Loss: 0.4991 | Batch Acc: 81.03%\n",
      "    Val Batch 040/101 | Loss: 0.7383 | Batch Acc: 56.90%\n",
      "    Val Batch 045/101 | Loss: 0.3626 | Batch Acc: 75.86%\n",
      "    Val Batch 050/101 | Loss: 0.0850 | Batch Acc: 96.55%\n",
      "    Val Batch 055/101 | Loss: 0.2445 | Batch Acc: 91.38%\n",
      "    Val Batch 060/101 | Loss: 0.4559 | Batch Acc: 81.03%\n",
      "    Val Batch 065/101 | Loss: 0.5283 | Batch Acc: 77.59%\n",
      "    Val Batch 070/101 | Loss: 0.3809 | Batch Acc: 84.48%\n",
      "    Val Batch 075/101 | Loss: 0.0689 | Batch Acc: 98.28%\n",
      "    Val Batch 080/101 | Loss: 0.2713 | Batch Acc: 87.93%\n",
      "    Val Batch 085/101 | Loss: 0.1738 | Batch Acc: 94.83%\n",
      "    Val Batch 090/101 | Loss: 0.3205 | Batch Acc: 93.10%\n",
      "    Val Batch 095/101 | Loss: 0.6048 | Batch Acc: 86.21%\n",
      "    Val Batch 100/101 | Loss: 0.2575 | Batch Acc: 93.10%\n",
      "    Val Batch 101/101 | Loss: 0.5685 | Batch Acc: 85.19%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.4691 | Accuracy: 83.71%\n",
      "  Current Best Acc: 84.25%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/899 | Loss: 0.5011 | CLoss: 0.3864 | FLoss: 0.2293 | LR: 2.86e-05\n",
      "  Batch 020/899 | Loss: 0.7597 | CLoss: 0.6056 | FLoss: 0.3082 | LR: 2.86e-05\n",
      "  Batch 030/899 | Loss: 0.7560 | CLoss: 0.5089 | FLoss: 0.4941 | LR: 2.86e-05\n",
      "  Batch 040/899 | Loss: 0.3726 | CLoss: 0.2920 | FLoss: 0.1612 | LR: 2.86e-05\n",
      "  Batch 050/899 | Loss: 0.8078 | CLoss: 0.6174 | FLoss: 0.3809 | LR: 2.86e-05\n",
      "  Batch 060/899 | Loss: 0.8873 | CLoss: 0.6904 | FLoss: 0.3937 | LR: 2.86e-05\n",
      "  Batch 070/899 | Loss: 0.6495 | CLoss: 0.4800 | FLoss: 0.3390 | LR: 2.86e-05\n",
      "  Batch 080/899 | Loss: 0.4606 | CLoss: 0.4250 | FLoss: 0.0711 | LR: 2.86e-05\n",
      "  Batch 090/899 | Loss: 0.7427 | CLoss: 0.6197 | FLoss: 0.2461 | LR: 2.86e-05\n",
      "  Batch 100/899 | Loss: 0.5122 | CLoss: 0.4028 | FLoss: 0.2187 | LR: 2.86e-05\n",
      "  Batch 110/899 | Loss: 1.2042 | CLoss: 0.9481 | FLoss: 0.5123 | LR: 2.86e-05\n",
      "  Batch 120/899 | Loss: 0.6626 | CLoss: 0.4524 | FLoss: 0.4205 | LR: 2.86e-05\n",
      "  Batch 130/899 | Loss: 0.6789 | CLoss: 0.5775 | FLoss: 0.2029 | LR: 2.86e-05\n",
      "  Batch 140/899 | Loss: 0.9371 | CLoss: 0.7326 | FLoss: 0.4091 | LR: 2.86e-05\n",
      "  Batch 150/899 | Loss: 0.6809 | CLoss: 0.5160 | FLoss: 0.3296 | LR: 2.86e-05\n",
      "  Batch 160/899 | Loss: 0.4290 | CLoss: 0.3182 | FLoss: 0.2215 | LR: 2.86e-05\n",
      "  Batch 170/899 | Loss: 0.7911 | CLoss: 0.5930 | FLoss: 0.3961 | LR: 2.86e-05\n",
      "  Batch 180/899 | Loss: 0.8668 | CLoss: 0.7253 | FLoss: 0.2831 | LR: 2.86e-05\n",
      "  Batch 190/899 | Loss: 0.5818 | CLoss: 0.4218 | FLoss: 0.3200 | LR: 2.86e-05\n",
      "  Batch 200/899 | Loss: 0.3780 | CLoss: 0.2041 | FLoss: 0.3477 | LR: 2.86e-05\n",
      "  Batch 210/899 | Loss: 1.0522 | CLoss: 0.7475 | FLoss: 0.6094 | LR: 2.86e-05\n",
      "  Batch 220/899 | Loss: 0.6507 | CLoss: 0.5085 | FLoss: 0.2843 | LR: 2.86e-05\n",
      "  Batch 230/899 | Loss: 0.5882 | CLoss: 0.4563 | FLoss: 0.2638 | LR: 2.86e-05\n",
      "  Batch 240/899 | Loss: 0.9085 | CLoss: 0.7445 | FLoss: 0.3279 | LR: 2.86e-05\n",
      "  Batch 250/899 | Loss: 0.7453 | CLoss: 0.6147 | FLoss: 0.2612 | LR: 2.86e-05\n",
      "  Batch 260/899 | Loss: 0.5423 | CLoss: 0.3286 | FLoss: 0.4273 | LR: 2.86e-05\n",
      "  Batch 270/899 | Loss: 0.4137 | CLoss: 0.3279 | FLoss: 0.1716 | LR: 2.86e-05\n",
      "  Batch 280/899 | Loss: 0.7512 | CLoss: 0.6551 | FLoss: 0.1922 | LR: 2.86e-05\n",
      "  Batch 290/899 | Loss: 0.4965 | CLoss: 0.3597 | FLoss: 0.2735 | LR: 2.86e-05\n",
      "  Batch 300/899 | Loss: 0.3033 | CLoss: 0.2164 | FLoss: 0.1738 | LR: 2.86e-05\n",
      "  Batch 310/899 | Loss: 1.2144 | CLoss: 1.0428 | FLoss: 0.3431 | LR: 2.86e-05\n",
      "  Batch 320/899 | Loss: 0.6595 | CLoss: 0.5528 | FLoss: 0.2134 | LR: 2.86e-05\n",
      "  Batch 330/899 | Loss: 0.3964 | CLoss: 0.2907 | FLoss: 0.2113 | LR: 2.86e-05\n",
      "  Batch 340/899 | Loss: 0.7900 | CLoss: 0.5655 | FLoss: 0.4491 | LR: 2.86e-05\n",
      "  Batch 350/899 | Loss: 0.6503 | CLoss: 0.5260 | FLoss: 0.2486 | LR: 2.86e-05\n",
      "  Batch 360/899 | Loss: 0.5724 | CLoss: 0.3341 | FLoss: 0.4765 | LR: 2.86e-05\n",
      "  Batch 370/899 | Loss: 0.8136 | CLoss: 0.6034 | FLoss: 0.4205 | LR: 2.86e-05\n",
      "  Batch 380/899 | Loss: 0.5539 | CLoss: 0.4374 | FLoss: 0.2329 | LR: 2.86e-05\n",
      "  Batch 390/899 | Loss: 0.5377 | CLoss: 0.4151 | FLoss: 0.2451 | LR: 2.86e-05\n",
      "  Batch 400/899 | Loss: 0.8974 | CLoss: 0.6840 | FLoss: 0.4268 | LR: 2.86e-05\n",
      "  Batch 410/899 | Loss: 0.7659 | CLoss: 0.5644 | FLoss: 0.4030 | LR: 2.86e-05\n",
      "  Batch 420/899 | Loss: 0.3523 | CLoss: 0.2695 | FLoss: 0.1655 | LR: 2.86e-05\n",
      "  Batch 430/899 | Loss: 1.0544 | CLoss: 0.8491 | FLoss: 0.4107 | LR: 2.86e-05\n",
      "  Batch 440/899 | Loss: 0.5418 | CLoss: 0.4246 | FLoss: 0.2345 | LR: 2.86e-05\n",
      "  Batch 450/899 | Loss: 0.8575 | CLoss: 0.6508 | FLoss: 0.4133 | LR: 2.86e-05\n",
      "  Batch 460/899 | Loss: 0.7313 | CLoss: 0.5025 | FLoss: 0.4576 | LR: 2.86e-05\n",
      "  Batch 470/899 | Loss: 0.4440 | CLoss: 0.3863 | FLoss: 0.1153 | LR: 2.86e-05\n",
      "  Batch 480/899 | Loss: 0.2537 | CLoss: 0.1841 | FLoss: 0.1390 | LR: 2.86e-05\n",
      "  Batch 490/899 | Loss: 0.6768 | CLoss: 0.5155 | FLoss: 0.3226 | LR: 2.86e-05\n",
      "  Batch 500/899 | Loss: 0.4277 | CLoss: 0.3447 | FLoss: 0.1660 | LR: 2.86e-05\n",
      "  Batch 510/899 | Loss: 0.9232 | CLoss: 0.6746 | FLoss: 0.4972 | LR: 2.86e-05\n",
      "  Batch 520/899 | Loss: 0.8602 | CLoss: 0.6489 | FLoss: 0.4224 | LR: 2.86e-05\n",
      "  Batch 530/899 | Loss: 0.5155 | CLoss: 0.4003 | FLoss: 0.2302 | LR: 2.86e-05\n",
      "  Batch 540/899 | Loss: 0.7003 | CLoss: 0.5479 | FLoss: 0.3048 | LR: 2.86e-05\n",
      "  Batch 550/899 | Loss: 0.3826 | CLoss: 0.2723 | FLoss: 0.2205 | LR: 2.86e-05\n",
      "  Batch 560/899 | Loss: 0.5846 | CLoss: 0.4293 | FLoss: 0.3107 | LR: 2.86e-05\n",
      "  Batch 570/899 | Loss: 0.4676 | CLoss: 0.3279 | FLoss: 0.2794 | LR: 2.86e-05\n",
      "  Batch 580/899 | Loss: 0.8616 | CLoss: 0.6778 | FLoss: 0.3677 | LR: 2.86e-05\n",
      "  Batch 590/899 | Loss: 0.8689 | CLoss: 0.6154 | FLoss: 0.5071 | LR: 2.86e-05\n",
      "  Batch 600/899 | Loss: 0.3257 | CLoss: 0.2322 | FLoss: 0.1869 | LR: 2.86e-05\n",
      "  Batch 610/899 | Loss: 0.8601 | CLoss: 0.6684 | FLoss: 0.3833 | LR: 2.86e-05\n",
      "  Batch 620/899 | Loss: 0.7018 | CLoss: 0.5172 | FLoss: 0.3692 | LR: 2.86e-05\n",
      "  Batch 630/899 | Loss: 0.6434 | CLoss: 0.4976 | FLoss: 0.2916 | LR: 2.86e-05\n",
      "  Batch 640/899 | Loss: 0.5614 | CLoss: 0.4189 | FLoss: 0.2850 | LR: 2.86e-05\n",
      "  Batch 650/899 | Loss: 0.5180 | CLoss: 0.4187 | FLoss: 0.1987 | LR: 2.86e-05\n",
      "  Batch 660/899 | Loss: 0.7513 | CLoss: 0.5670 | FLoss: 0.3686 | LR: 2.86e-05\n",
      "  Batch 670/899 | Loss: 0.4256 | CLoss: 0.3459 | FLoss: 0.1594 | LR: 2.86e-05\n",
      "  Batch 680/899 | Loss: 0.4764 | CLoss: 0.3121 | FLoss: 0.3286 | LR: 2.86e-05\n",
      "  Batch 690/899 | Loss: 0.6591 | CLoss: 0.4651 | FLoss: 0.3881 | LR: 2.86e-05\n",
      "  Batch 700/899 | Loss: 1.0135 | CLoss: 0.8475 | FLoss: 0.3322 | LR: 2.86e-05\n",
      "  Batch 710/899 | Loss: 0.5247 | CLoss: 0.3926 | FLoss: 0.2643 | LR: 2.86e-05\n",
      "  Batch 720/899 | Loss: 0.6241 | CLoss: 0.4482 | FLoss: 0.3518 | LR: 2.86e-05\n",
      "  Batch 730/899 | Loss: 0.5539 | CLoss: 0.4174 | FLoss: 0.2730 | LR: 2.86e-05\n",
      "  Batch 740/899 | Loss: 0.7729 | CLoss: 0.6854 | FLoss: 0.1748 | LR: 2.86e-05\n",
      "  Batch 750/899 | Loss: 0.7369 | CLoss: 0.5074 | FLoss: 0.4591 | LR: 2.86e-05\n",
      "  Batch 760/899 | Loss: 0.5187 | CLoss: 0.3749 | FLoss: 0.2876 | LR: 2.86e-05\n",
      "  Batch 770/899 | Loss: 0.6395 | CLoss: 0.5029 | FLoss: 0.2732 | LR: 2.86e-05\n",
      "  Batch 780/899 | Loss: 0.6320 | CLoss: 0.4842 | FLoss: 0.2955 | LR: 2.86e-05\n",
      "  Batch 790/899 | Loss: 0.6370 | CLoss: 0.4640 | FLoss: 0.3460 | LR: 2.86e-05\n",
      "  Batch 800/899 | Loss: 0.6679 | CLoss: 0.5143 | FLoss: 0.3070 | LR: 2.86e-05\n",
      "  Batch 810/899 | Loss: 0.4982 | CLoss: 0.3665 | FLoss: 0.2634 | LR: 2.86e-05\n",
      "  Batch 820/899 | Loss: 0.7798 | CLoss: 0.6190 | FLoss: 0.3216 | LR: 2.86e-05\n",
      "  Batch 830/899 | Loss: 0.3873 | CLoss: 0.2980 | FLoss: 0.1785 | LR: 2.86e-05\n",
      "  Batch 840/899 | Loss: 0.6846 | CLoss: 0.5081 | FLoss: 0.3530 | LR: 2.86e-05\n",
      "  Batch 850/899 | Loss: 0.7775 | CLoss: 0.6129 | FLoss: 0.3293 | LR: 2.86e-05\n",
      "  Batch 860/899 | Loss: 0.5089 | CLoss: 0.4000 | FLoss: 0.2179 | LR: 2.86e-05\n",
      "  Batch 870/899 | Loss: 0.3783 | CLoss: 0.2958 | FLoss: 0.1649 | LR: 2.86e-05\n",
      "  Batch 880/899 | Loss: 0.6441 | CLoss: 0.5371 | FLoss: 0.2138 | LR: 2.86e-05\n",
      "  Batch 890/899 | Loss: 0.5253 | CLoss: 0.3869 | FLoss: 0.2769 | LR: 2.86e-05\n",
      "  Batch 899/899 | Loss: 0.6300 | CLoss: 0.3452 | FLoss: 0.5695 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.6339\n",
      "  Last Batch Loss: 0.6300\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/101 | Loss: 0.9895 | Batch Acc: 55.17%\n",
      "    Val Batch 010/101 | Loss: 0.2169 | Batch Acc: 91.38%\n",
      "    Val Batch 015/101 | Loss: 0.1871 | Batch Acc: 93.10%\n",
      "    Val Batch 020/101 | Loss: 0.6930 | Batch Acc: 56.90%\n",
      "    Val Batch 025/101 | Loss: 0.2017 | Batch Acc: 94.83%\n",
      "    Val Batch 030/101 | Loss: 0.2977 | Batch Acc: 89.66%\n",
      "    Val Batch 035/101 | Loss: 0.4455 | Batch Acc: 81.03%\n",
      "    Val Batch 040/101 | Loss: 0.6815 | Batch Acc: 98.28%\n",
      "    Val Batch 045/101 | Loss: 0.2516 | Batch Acc: 100.00%\n",
      "    Val Batch 050/101 | Loss: 0.2462 | Batch Acc: 93.10%\n",
      "    Val Batch 055/101 | Loss: 0.0904 | Batch Acc: 96.55%\n",
      "    Val Batch 060/101 | Loss: 0.2329 | Batch Acc: 89.66%\n",
      "    Val Batch 065/101 | Loss: 1.2282 | Batch Acc: 51.72%\n",
      "    Val Batch 070/101 | Loss: 0.5203 | Batch Acc: 82.76%\n",
      "    Val Batch 075/101 | Loss: 0.3311 | Batch Acc: 93.10%\n",
      "    Val Batch 080/101 | Loss: 0.4327 | Batch Acc: 86.21%\n",
      "    Val Batch 085/101 | Loss: 0.1777 | Batch Acc: 94.83%\n",
      "    Val Batch 090/101 | Loss: 0.2546 | Batch Acc: 93.10%\n",
      "    Val Batch 095/101 | Loss: 0.3434 | Batch Acc: 89.66%\n",
      "    Val Batch 100/101 | Loss: 0.5129 | Batch Acc: 86.21%\n",
      "    Val Batch 101/101 | Loss: 0.3646 | Batch Acc: 92.59%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.4106 | Accuracy: 85.96%\n",
      "  Current Best Acc: 85.96%\n",
      "\n",
      "========================================\n",
      "=== Fold 6 Completed ===\n",
      "Best Validation Accuracy: 85.96%\n",
      "\n",
      "========================================\n",
      "=== Fold 7/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/898 | Loss: 1.7318 | CLoss: 0.7861 | FLoss: 1.8915 | LR: 3.00e-04\n",
      "  Batch 020/898 | Loss: 2.0512 | CLoss: 1.3620 | FLoss: 1.3785 | LR: 3.00e-04\n",
      "  Batch 030/898 | Loss: 1.4908 | CLoss: 0.9702 | FLoss: 1.0411 | LR: 3.00e-04\n",
      "  Batch 040/898 | Loss: 1.2279 | CLoss: 0.7837 | FLoss: 0.8884 | LR: 3.00e-04\n",
      "  Batch 050/898 | Loss: 1.0508 | CLoss: 0.6665 | FLoss: 0.7686 | LR: 3.00e-04\n",
      "  Batch 060/898 | Loss: 1.5835 | CLoss: 1.1893 | FLoss: 0.7886 | LR: 3.00e-04\n",
      "  Batch 070/898 | Loss: 0.9588 | CLoss: 0.6443 | FLoss: 0.6291 | LR: 3.00e-04\n",
      "  Batch 080/898 | Loss: 0.5842 | CLoss: 0.3568 | FLoss: 0.4548 | LR: 3.00e-04\n",
      "  Batch 090/898 | Loss: 0.8726 | CLoss: 0.6219 | FLoss: 0.5015 | LR: 3.00e-04\n",
      "  Batch 100/898 | Loss: 0.6447 | CLoss: 0.4040 | FLoss: 0.4814 | LR: 3.00e-04\n",
      "  Batch 110/898 | Loss: 0.7531 | CLoss: 0.5052 | FLoss: 0.4957 | LR: 3.00e-04\n",
      "  Batch 120/898 | Loss: 1.4305 | CLoss: 1.1165 | FLoss: 0.6280 | LR: 3.00e-04\n",
      "  Batch 130/898 | Loss: 1.4805 | CLoss: 1.2631 | FLoss: 0.4347 | LR: 3.00e-04\n",
      "  Batch 140/898 | Loss: 0.7153 | CLoss: 0.4921 | FLoss: 0.4465 | LR: 3.00e-04\n",
      "  Batch 150/898 | Loss: 0.8993 | CLoss: 0.7016 | FLoss: 0.3954 | LR: 3.00e-04\n",
      "  Batch 160/898 | Loss: 0.6056 | CLoss: 0.3788 | FLoss: 0.4535 | LR: 3.00e-04\n",
      "  Batch 170/898 | Loss: 0.8212 | CLoss: 0.6013 | FLoss: 0.4398 | LR: 3.00e-04\n",
      "  Batch 180/898 | Loss: 0.6180 | CLoss: 0.4762 | FLoss: 0.2836 | LR: 3.00e-04\n",
      "  Batch 190/898 | Loss: 0.4007 | CLoss: 0.2381 | FLoss: 0.3252 | LR: 3.00e-04\n",
      "  Batch 200/898 | Loss: 0.7618 | CLoss: 0.5770 | FLoss: 0.3695 | LR: 3.00e-04\n",
      "  Batch 210/898 | Loss: 0.8368 | CLoss: 0.6593 | FLoss: 0.3549 | LR: 3.00e-04\n",
      "  Batch 220/898 | Loss: 1.0753 | CLoss: 0.8428 | FLoss: 0.4649 | LR: 3.00e-04\n",
      "  Batch 230/898 | Loss: 0.7500 | CLoss: 0.5496 | FLoss: 0.4009 | LR: 3.00e-04\n",
      "  Batch 240/898 | Loss: 0.9373 | CLoss: 0.7682 | FLoss: 0.3381 | LR: 3.00e-04\n",
      "  Batch 250/898 | Loss: 1.0011 | CLoss: 0.8149 | FLoss: 0.3726 | LR: 3.00e-04\n",
      "  Batch 260/898 | Loss: 0.8600 | CLoss: 0.5851 | FLoss: 0.5498 | LR: 3.00e-04\n",
      "  Batch 270/898 | Loss: 1.2815 | CLoss: 1.0499 | FLoss: 0.4632 | LR: 3.00e-04\n",
      "  Batch 280/898 | Loss: 0.7934 | CLoss: 0.4643 | FLoss: 0.6580 | LR: 3.00e-04\n",
      "  Batch 290/898 | Loss: 1.0906 | CLoss: 0.8639 | FLoss: 0.4533 | LR: 3.00e-04\n",
      "  Batch 300/898 | Loss: 0.9740 | CLoss: 0.8133 | FLoss: 0.3214 | LR: 3.00e-04\n",
      "  Batch 310/898 | Loss: 0.6105 | CLoss: 0.4692 | FLoss: 0.2825 | LR: 3.00e-04\n",
      "  Batch 320/898 | Loss: 1.0051 | CLoss: 0.8399 | FLoss: 0.3304 | LR: 3.00e-04\n",
      "  Batch 330/898 | Loss: 1.1538 | CLoss: 0.8721 | FLoss: 0.5634 | LR: 3.00e-04\n",
      "  Batch 340/898 | Loss: 1.2984 | CLoss: 1.0239 | FLoss: 0.5490 | LR: 3.00e-04\n",
      "  Batch 350/898 | Loss: 1.0081 | CLoss: 0.7875 | FLoss: 0.4413 | LR: 3.00e-04\n",
      "  Batch 360/898 | Loss: 1.0057 | CLoss: 0.7567 | FLoss: 0.4982 | LR: 3.00e-04\n",
      "  Batch 370/898 | Loss: 1.1702 | CLoss: 0.8585 | FLoss: 0.6232 | LR: 3.00e-04\n",
      "  Batch 380/898 | Loss: 1.0190 | CLoss: 0.7795 | FLoss: 0.4789 | LR: 3.00e-04\n",
      "  Batch 390/898 | Loss: 0.7858 | CLoss: 0.5647 | FLoss: 0.4421 | LR: 3.00e-04\n",
      "  Batch 400/898 | Loss: 0.8643 | CLoss: 0.6409 | FLoss: 0.4468 | LR: 3.00e-04\n",
      "  Batch 410/898 | Loss: 0.5940 | CLoss: 0.3849 | FLoss: 0.4182 | LR: 3.00e-04\n",
      "  Batch 420/898 | Loss: 0.6442 | CLoss: 0.4515 | FLoss: 0.3854 | LR: 3.00e-04\n",
      "  Batch 430/898 | Loss: 0.8268 | CLoss: 0.6522 | FLoss: 0.3492 | LR: 3.00e-04\n",
      "  Batch 440/898 | Loss: 1.6276 | CLoss: 1.3123 | FLoss: 0.6305 | LR: 3.00e-04\n",
      "  Batch 450/898 | Loss: 0.8785 | CLoss: 0.6835 | FLoss: 0.3900 | LR: 3.00e-04\n",
      "  Batch 460/898 | Loss: 1.0859 | CLoss: 0.7894 | FLoss: 0.5930 | LR: 3.00e-04\n",
      "  Batch 470/898 | Loss: 0.6849 | CLoss: 0.5273 | FLoss: 0.3151 | LR: 3.00e-04\n",
      "  Batch 480/898 | Loss: 1.3430 | CLoss: 1.0496 | FLoss: 0.5868 | LR: 3.00e-04\n",
      "  Batch 490/898 | Loss: 0.9275 | CLoss: 0.6830 | FLoss: 0.4891 | LR: 3.00e-04\n",
      "  Batch 500/898 | Loss: 0.7872 | CLoss: 0.5437 | FLoss: 0.4870 | LR: 3.00e-04\n",
      "  Batch 510/898 | Loss: 0.8528 | CLoss: 0.6723 | FLoss: 0.3609 | LR: 3.00e-04\n",
      "  Batch 520/898 | Loss: 0.9178 | CLoss: 0.6948 | FLoss: 0.4460 | LR: 3.00e-04\n",
      "  Batch 530/898 | Loss: 0.8225 | CLoss: 0.6201 | FLoss: 0.4049 | LR: 3.00e-04\n",
      "  Batch 540/898 | Loss: 0.8909 | CLoss: 0.6792 | FLoss: 0.4235 | LR: 3.00e-04\n",
      "  Batch 550/898 | Loss: 1.0787 | CLoss: 0.8140 | FLoss: 0.5293 | LR: 3.00e-04\n",
      "  Batch 560/898 | Loss: 0.8299 | CLoss: 0.6792 | FLoss: 0.3016 | LR: 3.00e-04\n",
      "  Batch 570/898 | Loss: 0.8776 | CLoss: 0.6359 | FLoss: 0.4834 | LR: 3.00e-04\n",
      "  Batch 580/898 | Loss: 1.0274 | CLoss: 0.8546 | FLoss: 0.3456 | LR: 3.00e-04\n",
      "  Batch 590/898 | Loss: 0.8289 | CLoss: 0.6806 | FLoss: 0.2967 | LR: 3.00e-04\n",
      "  Batch 600/898 | Loss: 0.8105 | CLoss: 0.6217 | FLoss: 0.3775 | LR: 3.00e-04\n",
      "  Batch 610/898 | Loss: 1.0566 | CLoss: 0.8218 | FLoss: 0.4696 | LR: 3.00e-04\n",
      "  Batch 620/898 | Loss: 1.3649 | CLoss: 1.0055 | FLoss: 0.7187 | LR: 3.00e-04\n",
      "  Batch 630/898 | Loss: 0.5376 | CLoss: 0.4286 | FLoss: 0.2179 | LR: 3.00e-04\n",
      "  Batch 640/898 | Loss: 1.2414 | CLoss: 1.0440 | FLoss: 0.3949 | LR: 3.00e-04\n",
      "  Batch 650/898 | Loss: 0.7263 | CLoss: 0.5258 | FLoss: 0.4011 | LR: 3.00e-04\n",
      "  Batch 660/898 | Loss: 0.6845 | CLoss: 0.5963 | FLoss: 0.1764 | LR: 3.00e-04\n",
      "  Batch 670/898 | Loss: 1.3994 | CLoss: 1.1834 | FLoss: 0.4320 | LR: 3.00e-04\n",
      "  Batch 680/898 | Loss: 1.2946 | CLoss: 1.1269 | FLoss: 0.3355 | LR: 3.00e-04\n",
      "  Batch 690/898 | Loss: 1.1163 | CLoss: 0.8287 | FLoss: 0.5752 | LR: 3.00e-04\n",
      "  Batch 700/898 | Loss: 0.9663 | CLoss: 0.7406 | FLoss: 0.4513 | LR: 3.00e-04\n",
      "  Batch 710/898 | Loss: 0.7017 | CLoss: 0.4829 | FLoss: 0.4377 | LR: 3.00e-04\n",
      "  Batch 720/898 | Loss: 0.9467 | CLoss: 0.7250 | FLoss: 0.4434 | LR: 3.00e-04\n",
      "  Batch 730/898 | Loss: 0.9480 | CLoss: 0.7989 | FLoss: 0.2981 | LR: 3.00e-04\n",
      "  Batch 740/898 | Loss: 0.9219 | CLoss: 0.7600 | FLoss: 0.3239 | LR: 3.00e-04\n",
      "  Batch 750/898 | Loss: 0.6546 | CLoss: 0.4936 | FLoss: 0.3220 | LR: 3.00e-04\n",
      "  Batch 760/898 | Loss: 0.7589 | CLoss: 0.6032 | FLoss: 0.3115 | LR: 3.00e-04\n",
      "  Batch 770/898 | Loss: 0.7616 | CLoss: 0.6082 | FLoss: 0.3068 | LR: 3.00e-04\n",
      "  Batch 780/898 | Loss: 0.8788 | CLoss: 0.6762 | FLoss: 0.4052 | LR: 3.00e-04\n",
      "  Batch 790/898 | Loss: 0.7564 | CLoss: 0.5380 | FLoss: 0.4370 | LR: 3.00e-04\n",
      "  Batch 800/898 | Loss: 0.6320 | CLoss: 0.5245 | FLoss: 0.2149 | LR: 3.00e-04\n",
      "  Batch 810/898 | Loss: 1.0358 | CLoss: 0.7881 | FLoss: 0.4955 | LR: 3.00e-04\n",
      "  Batch 820/898 | Loss: 0.9205 | CLoss: 0.7840 | FLoss: 0.2729 | LR: 3.00e-04\n",
      "  Batch 830/898 | Loss: 1.2413 | CLoss: 0.9717 | FLoss: 0.5392 | LR: 3.00e-04\n",
      "  Batch 840/898 | Loss: 0.4877 | CLoss: 0.3264 | FLoss: 0.3226 | LR: 3.00e-04\n",
      "  Batch 850/898 | Loss: 0.6905 | CLoss: 0.5388 | FLoss: 0.3035 | LR: 3.00e-04\n",
      "  Batch 860/898 | Loss: 1.3285 | CLoss: 1.1581 | FLoss: 0.3408 | LR: 3.00e-04\n",
      "  Batch 870/898 | Loss: 0.8914 | CLoss: 0.6934 | FLoss: 0.3961 | LR: 3.00e-04\n",
      "  Batch 880/898 | Loss: 1.1374 | CLoss: 0.9173 | FLoss: 0.4402 | LR: 3.00e-04\n",
      "  Batch 890/898 | Loss: 0.9522 | CLoss: 0.8153 | FLoss: 0.2738 | LR: 3.00e-04\n",
      "  Batch 898/898 | Loss: 0.4156 | CLoss: 0.0036 | FLoss: 0.8240 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 0.9837\n",
      "  Last Batch Loss: 0.4156\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 0.6814 | Batch Acc: 72.41%\n",
      "    Val Batch 010/102 | Loss: 0.2286 | Batch Acc: 94.83%\n",
      "    Val Batch 015/102 | Loss: 0.4360 | Batch Acc: 89.66%\n",
      "    Val Batch 020/102 | Loss: 0.5299 | Batch Acc: 89.66%\n",
      "    Val Batch 025/102 | Loss: 0.2570 | Batch Acc: 89.66%\n",
      "    Val Batch 030/102 | Loss: 0.2298 | Batch Acc: 93.10%\n",
      "    Val Batch 035/102 | Loss: 0.3462 | Batch Acc: 93.10%\n",
      "    Val Batch 040/102 | Loss: 0.4490 | Batch Acc: 74.14%\n",
      "    Val Batch 045/102 | Loss: 1.0630 | Batch Acc: 25.86%\n",
      "    Val Batch 050/102 | Loss: 0.9553 | Batch Acc: 74.14%\n",
      "    Val Batch 055/102 | Loss: 1.1029 | Batch Acc: 63.79%\n",
      "    Val Batch 060/102 | Loss: 0.7838 | Batch Acc: 70.69%\n",
      "    Val Batch 065/102 | Loss: 0.4944 | Batch Acc: 86.21%\n",
      "    Val Batch 070/102 | Loss: 0.2253 | Batch Acc: 94.83%\n",
      "    Val Batch 075/102 | Loss: 0.2730 | Batch Acc: 91.38%\n",
      "    Val Batch 080/102 | Loss: 0.2346 | Batch Acc: 94.83%\n",
      "    Val Batch 085/102 | Loss: 0.4633 | Batch Acc: 91.38%\n",
      "    Val Batch 090/102 | Loss: 0.5524 | Batch Acc: 84.48%\n",
      "    Val Batch 095/102 | Loss: 0.5654 | Batch Acc: 86.21%\n",
      "    Val Batch 100/102 | Loss: 0.5246 | Batch Acc: 84.48%\n",
      "    Val Batch 102/102 | Loss: 0.3233 | Batch Acc: 86.21%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.4920 | Accuracy: 82.30%\n",
      "  Current Best Acc: 82.30%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/898 | Loss: 0.6683 | CLoss: 0.5298 | FLoss: 0.2771 | LR: 2.71e-04\n",
      "  Batch 020/898 | Loss: 1.2330 | CLoss: 0.9671 | FLoss: 0.5318 | LR: 2.71e-04\n",
      "  Batch 030/898 | Loss: 0.9052 | CLoss: 0.7087 | FLoss: 0.3931 | LR: 2.71e-04\n",
      "  Batch 040/898 | Loss: 0.8756 | CLoss: 0.6819 | FLoss: 0.3874 | LR: 2.71e-04\n",
      "  Batch 050/898 | Loss: 1.0954 | CLoss: 0.8110 | FLoss: 0.5690 | LR: 2.71e-04\n",
      "  Batch 060/898 | Loss: 0.7888 | CLoss: 0.5782 | FLoss: 0.4212 | LR: 2.71e-04\n",
      "  Batch 070/898 | Loss: 0.6269 | CLoss: 0.4514 | FLoss: 0.3510 | LR: 2.71e-04\n",
      "  Batch 080/898 | Loss: 0.7563 | CLoss: 0.5415 | FLoss: 0.4295 | LR: 2.71e-04\n",
      "  Batch 090/898 | Loss: 1.1853 | CLoss: 0.9176 | FLoss: 0.5356 | LR: 2.71e-04\n",
      "  Batch 100/898 | Loss: 0.9622 | CLoss: 0.7810 | FLoss: 0.3625 | LR: 2.71e-04\n",
      "  Batch 110/898 | Loss: 0.8863 | CLoss: 0.6835 | FLoss: 0.4056 | LR: 2.71e-04\n",
      "  Batch 120/898 | Loss: 0.6721 | CLoss: 0.4133 | FLoss: 0.5176 | LR: 2.71e-04\n",
      "  Batch 130/898 | Loss: 1.2125 | CLoss: 0.9265 | FLoss: 0.5721 | LR: 2.71e-04\n",
      "  Batch 140/898 | Loss: 1.0433 | CLoss: 0.8312 | FLoss: 0.4242 | LR: 2.71e-04\n",
      "  Batch 150/898 | Loss: 0.7942 | CLoss: 0.4775 | FLoss: 0.6333 | LR: 2.71e-04\n",
      "  Batch 160/898 | Loss: 0.7798 | CLoss: 0.6089 | FLoss: 0.3418 | LR: 2.71e-04\n",
      "  Batch 170/898 | Loss: 1.0344 | CLoss: 0.7646 | FLoss: 0.5394 | LR: 2.71e-04\n",
      "  Batch 180/898 | Loss: 0.9005 | CLoss: 0.6767 | FLoss: 0.4475 | LR: 2.71e-04\n",
      "  Batch 190/898 | Loss: 0.8474 | CLoss: 0.6883 | FLoss: 0.3183 | LR: 2.71e-04\n",
      "  Batch 200/898 | Loss: 0.9078 | CLoss: 0.7698 | FLoss: 0.2760 | LR: 2.71e-04\n",
      "  Batch 210/898 | Loss: 0.9698 | CLoss: 0.6428 | FLoss: 0.6540 | LR: 2.71e-04\n",
      "  Batch 220/898 | Loss: 0.8377 | CLoss: 0.6250 | FLoss: 0.4255 | LR: 2.71e-04\n",
      "  Batch 230/898 | Loss: 0.7599 | CLoss: 0.5429 | FLoss: 0.4341 | LR: 2.71e-04\n",
      "  Batch 240/898 | Loss: 0.7286 | CLoss: 0.5836 | FLoss: 0.2900 | LR: 2.71e-04\n",
      "  Batch 250/898 | Loss: 0.9704 | CLoss: 0.7384 | FLoss: 0.4641 | LR: 2.71e-04\n",
      "  Batch 260/898 | Loss: 0.7017 | CLoss: 0.5024 | FLoss: 0.3986 | LR: 2.71e-04\n",
      "  Batch 270/898 | Loss: 0.6295 | CLoss: 0.4144 | FLoss: 0.4302 | LR: 2.71e-04\n",
      "  Batch 280/898 | Loss: 1.3482 | CLoss: 1.1072 | FLoss: 0.4820 | LR: 2.71e-04\n",
      "  Batch 290/898 | Loss: 0.8882 | CLoss: 0.7218 | FLoss: 0.3327 | LR: 2.71e-04\n",
      "  Batch 300/898 | Loss: 1.0844 | CLoss: 0.7864 | FLoss: 0.5960 | LR: 2.71e-04\n",
      "  Batch 310/898 | Loss: 0.5370 | CLoss: 0.4116 | FLoss: 0.2508 | LR: 2.71e-04\n",
      "  Batch 320/898 | Loss: 0.6645 | CLoss: 0.4968 | FLoss: 0.3354 | LR: 2.71e-04\n",
      "  Batch 330/898 | Loss: 1.2487 | CLoss: 0.9129 | FLoss: 0.6716 | LR: 2.71e-04\n",
      "  Batch 340/898 | Loss: 0.6943 | CLoss: 0.5173 | FLoss: 0.3539 | LR: 2.71e-04\n",
      "  Batch 350/898 | Loss: 0.8762 | CLoss: 0.6170 | FLoss: 0.5183 | LR: 2.71e-04\n",
      "  Batch 360/898 | Loss: 0.7792 | CLoss: 0.5763 | FLoss: 0.4058 | LR: 2.71e-04\n",
      "  Batch 370/898 | Loss: 1.0168 | CLoss: 0.8900 | FLoss: 0.2536 | LR: 2.71e-04\n",
      "  Batch 380/898 | Loss: 0.9372 | CLoss: 0.7085 | FLoss: 0.4575 | LR: 2.71e-04\n",
      "  Batch 390/898 | Loss: 0.9501 | CLoss: 0.7078 | FLoss: 0.4846 | LR: 2.71e-04\n",
      "  Batch 400/898 | Loss: 0.5242 | CLoss: 0.4071 | FLoss: 0.2343 | LR: 2.71e-04\n",
      "  Batch 410/898 | Loss: 1.0443 | CLoss: 0.7926 | FLoss: 0.5034 | LR: 2.71e-04\n",
      "  Batch 420/898 | Loss: 0.9158 | CLoss: 0.7120 | FLoss: 0.4075 | LR: 2.71e-04\n",
      "  Batch 430/898 | Loss: 0.8578 | CLoss: 0.6439 | FLoss: 0.4279 | LR: 2.71e-04\n",
      "  Batch 440/898 | Loss: 0.7257 | CLoss: 0.5502 | FLoss: 0.3511 | LR: 2.71e-04\n",
      "  Batch 450/898 | Loss: 0.8074 | CLoss: 0.6140 | FLoss: 0.3868 | LR: 2.71e-04\n",
      "  Batch 460/898 | Loss: 1.1317 | CLoss: 0.8855 | FLoss: 0.4924 | LR: 2.71e-04\n",
      "  Batch 470/898 | Loss: 1.3669 | CLoss: 1.0990 | FLoss: 0.5358 | LR: 2.71e-04\n",
      "  Batch 480/898 | Loss: 0.8438 | CLoss: 0.5770 | FLoss: 0.5337 | LR: 2.71e-04\n",
      "  Batch 490/898 | Loss: 1.1263 | CLoss: 0.9170 | FLoss: 0.4186 | LR: 2.71e-04\n",
      "  Batch 500/898 | Loss: 0.9129 | CLoss: 0.6612 | FLoss: 0.5034 | LR: 2.71e-04\n",
      "  Batch 510/898 | Loss: 0.9589 | CLoss: 0.7986 | FLoss: 0.3207 | LR: 2.71e-04\n",
      "  Batch 520/898 | Loss: 0.8502 | CLoss: 0.6025 | FLoss: 0.4955 | LR: 2.71e-04\n",
      "  Batch 530/898 | Loss: 0.9756 | CLoss: 0.7719 | FLoss: 0.4074 | LR: 2.71e-04\n",
      "  Batch 540/898 | Loss: 0.8387 | CLoss: 0.6290 | FLoss: 0.4195 | LR: 2.71e-04\n",
      "  Batch 550/898 | Loss: 0.8318 | CLoss: 0.7078 | FLoss: 0.2480 | LR: 2.71e-04\n",
      "  Batch 560/898 | Loss: 0.6936 | CLoss: 0.4964 | FLoss: 0.3943 | LR: 2.71e-04\n",
      "  Batch 570/898 | Loss: 0.6155 | CLoss: 0.5206 | FLoss: 0.1898 | LR: 2.71e-04\n",
      "  Batch 580/898 | Loss: 0.9734 | CLoss: 0.8145 | FLoss: 0.3179 | LR: 2.71e-04\n",
      "  Batch 590/898 | Loss: 1.5173 | CLoss: 1.2019 | FLoss: 0.6308 | LR: 2.71e-04\n",
      "  Batch 600/898 | Loss: 0.6249 | CLoss: 0.4728 | FLoss: 0.3041 | LR: 2.71e-04\n",
      "  Batch 610/898 | Loss: 0.8620 | CLoss: 0.6667 | FLoss: 0.3905 | LR: 2.71e-04\n",
      "  Batch 620/898 | Loss: 1.0561 | CLoss: 0.7662 | FLoss: 0.5797 | LR: 2.71e-04\n",
      "  Batch 630/898 | Loss: 0.5853 | CLoss: 0.4598 | FLoss: 0.2510 | LR: 2.71e-04\n",
      "  Batch 640/898 | Loss: 0.9863 | CLoss: 0.7715 | FLoss: 0.4297 | LR: 2.71e-04\n",
      "  Batch 650/898 | Loss: 0.9581 | CLoss: 0.7126 | FLoss: 0.4911 | LR: 2.71e-04\n",
      "  Batch 660/898 | Loss: 0.6015 | CLoss: 0.4620 | FLoss: 0.2791 | LR: 2.71e-04\n",
      "  Batch 670/898 | Loss: 0.9138 | CLoss: 0.7336 | FLoss: 0.3605 | LR: 2.71e-04\n",
      "  Batch 680/898 | Loss: 0.6585 | CLoss: 0.5145 | FLoss: 0.2879 | LR: 2.71e-04\n",
      "  Batch 690/898 | Loss: 1.0028 | CLoss: 0.7870 | FLoss: 0.4316 | LR: 2.71e-04\n",
      "  Batch 700/898 | Loss: 0.8441 | CLoss: 0.6456 | FLoss: 0.3971 | LR: 2.71e-04\n",
      "  Batch 710/898 | Loss: 0.9974 | CLoss: 0.7518 | FLoss: 0.4912 | LR: 2.71e-04\n",
      "  Batch 720/898 | Loss: 0.9540 | CLoss: 0.6587 | FLoss: 0.5906 | LR: 2.71e-04\n",
      "  Batch 730/898 | Loss: 0.8872 | CLoss: 0.6943 | FLoss: 0.3858 | LR: 2.71e-04\n",
      "  Batch 740/898 | Loss: 0.6614 | CLoss: 0.4436 | FLoss: 0.4357 | LR: 2.71e-04\n",
      "  Batch 750/898 | Loss: 0.7963 | CLoss: 0.5946 | FLoss: 0.4035 | LR: 2.71e-04\n",
      "  Batch 760/898 | Loss: 0.5307 | CLoss: 0.3753 | FLoss: 0.3107 | LR: 2.71e-04\n",
      "  Batch 770/898 | Loss: 0.8263 | CLoss: 0.6130 | FLoss: 0.4266 | LR: 2.71e-04\n",
      "  Batch 780/898 | Loss: 1.4021 | CLoss: 1.1594 | FLoss: 0.4854 | LR: 2.71e-04\n",
      "  Batch 790/898 | Loss: 0.7556 | CLoss: 0.5789 | FLoss: 0.3534 | LR: 2.71e-04\n",
      "  Batch 800/898 | Loss: 0.7050 | CLoss: 0.5927 | FLoss: 0.2246 | LR: 2.71e-04\n",
      "  Batch 810/898 | Loss: 0.7519 | CLoss: 0.5639 | FLoss: 0.3760 | LR: 2.71e-04\n",
      "  Batch 820/898 | Loss: 0.7099 | CLoss: 0.4776 | FLoss: 0.4646 | LR: 2.71e-04\n",
      "  Batch 830/898 | Loss: 0.8838 | CLoss: 0.6529 | FLoss: 0.4618 | LR: 2.71e-04\n",
      "  Batch 840/898 | Loss: 1.3963 | CLoss: 1.0785 | FLoss: 0.6355 | LR: 2.71e-04\n",
      "  Batch 850/898 | Loss: 0.9903 | CLoss: 0.7900 | FLoss: 0.4006 | LR: 2.71e-04\n",
      "  Batch 860/898 | Loss: 1.0147 | CLoss: 0.8264 | FLoss: 0.3766 | LR: 2.71e-04\n",
      "  Batch 870/898 | Loss: 0.5448 | CLoss: 0.4370 | FLoss: 0.2158 | LR: 2.71e-04\n",
      "  Batch 880/898 | Loss: 0.6319 | CLoss: 0.4273 | FLoss: 0.4093 | LR: 2.71e-04\n",
      "  Batch 890/898 | Loss: 0.9062 | CLoss: 0.7851 | FLoss: 0.2422 | LR: 2.71e-04\n",
      "  Batch 898/898 | Loss: 0.7154 | CLoss: 0.0053 | FLoss: 1.4202 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.8579\n",
      "  Last Batch Loss: 0.7154\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 0.5445 | Batch Acc: 77.59%\n",
      "    Val Batch 010/102 | Loss: 0.2796 | Batch Acc: 93.10%\n",
      "    Val Batch 015/102 | Loss: 0.4349 | Batch Acc: 84.48%\n",
      "    Val Batch 020/102 | Loss: 0.7005 | Batch Acc: 62.07%\n",
      "    Val Batch 025/102 | Loss: 0.1650 | Batch Acc: 94.83%\n",
      "    Val Batch 030/102 | Loss: 0.1416 | Batch Acc: 94.83%\n",
      "    Val Batch 035/102 | Loss: 0.4446 | Batch Acc: 84.48%\n",
      "    Val Batch 040/102 | Loss: 0.2704 | Batch Acc: 93.10%\n",
      "    Val Batch 045/102 | Loss: 0.4670 | Batch Acc: 98.28%\n",
      "    Val Batch 050/102 | Loss: 0.4311 | Batch Acc: 87.93%\n",
      "    Val Batch 055/102 | Loss: 0.3509 | Batch Acc: 91.38%\n",
      "    Val Batch 060/102 | Loss: 1.3832 | Batch Acc: 56.90%\n",
      "    Val Batch 065/102 | Loss: 0.2023 | Batch Acc: 93.10%\n",
      "    Val Batch 070/102 | Loss: 0.2406 | Batch Acc: 89.66%\n",
      "    Val Batch 075/102 | Loss: 0.1216 | Batch Acc: 96.55%\n",
      "    Val Batch 080/102 | Loss: 0.2776 | Batch Acc: 91.38%\n",
      "    Val Batch 085/102 | Loss: 0.3021 | Batch Acc: 86.21%\n",
      "    Val Batch 090/102 | Loss: 0.6456 | Batch Acc: 81.03%\n",
      "    Val Batch 095/102 | Loss: 0.2121 | Batch Acc: 94.83%\n",
      "    Val Batch 100/102 | Loss: 0.4855 | Batch Acc: 84.48%\n",
      "    Val Batch 102/102 | Loss: 0.3173 | Batch Acc: 89.66%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.4448 | Accuracy: 85.97%\n",
      "  Current Best Acc: 85.97%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/898 | Loss: 0.9331 | CLoss: 0.7141 | FLoss: 0.4381 | LR: 1.96e-04\n",
      "  Batch 020/898 | Loss: 0.8999 | CLoss: 0.6485 | FLoss: 0.5027 | LR: 1.96e-04\n",
      "  Batch 030/898 | Loss: 0.7751 | CLoss: 0.5791 | FLoss: 0.3920 | LR: 1.96e-04\n",
      "  Batch 040/898 | Loss: 1.0147 | CLoss: 0.7674 | FLoss: 0.4947 | LR: 1.96e-04\n",
      "  Batch 050/898 | Loss: 0.5390 | CLoss: 0.4244 | FLoss: 0.2291 | LR: 1.96e-04\n",
      "  Batch 060/898 | Loss: 0.8939 | CLoss: 0.6568 | FLoss: 0.4741 | LR: 1.96e-04\n",
      "  Batch 070/898 | Loss: 0.6362 | CLoss: 0.4880 | FLoss: 0.2964 | LR: 1.96e-04\n",
      "  Batch 080/898 | Loss: 0.5485 | CLoss: 0.4137 | FLoss: 0.2697 | LR: 1.96e-04\n",
      "  Batch 090/898 | Loss: 0.6067 | CLoss: 0.4816 | FLoss: 0.2501 | LR: 1.96e-04\n",
      "  Batch 100/898 | Loss: 0.7936 | CLoss: 0.6131 | FLoss: 0.3610 | LR: 1.96e-04\n",
      "  Batch 110/898 | Loss: 0.6681 | CLoss: 0.4638 | FLoss: 0.4087 | LR: 1.96e-04\n",
      "  Batch 120/898 | Loss: 0.7157 | CLoss: 0.5776 | FLoss: 0.2760 | LR: 1.96e-04\n",
      "  Batch 130/898 | Loss: 1.1131 | CLoss: 0.8850 | FLoss: 0.4562 | LR: 1.96e-04\n",
      "  Batch 140/898 | Loss: 0.9025 | CLoss: 0.7122 | FLoss: 0.3806 | LR: 1.96e-04\n",
      "  Batch 150/898 | Loss: 0.6997 | CLoss: 0.5701 | FLoss: 0.2593 | LR: 1.96e-04\n",
      "  Batch 160/898 | Loss: 0.6040 | CLoss: 0.4446 | FLoss: 0.3187 | LR: 1.96e-04\n",
      "  Batch 170/898 | Loss: 0.8962 | CLoss: 0.6386 | FLoss: 0.5152 | LR: 1.96e-04\n",
      "  Batch 180/898 | Loss: 0.5942 | CLoss: 0.4471 | FLoss: 0.2942 | LR: 1.96e-04\n",
      "  Batch 190/898 | Loss: 0.8955 | CLoss: 0.7972 | FLoss: 0.1967 | LR: 1.96e-04\n",
      "  Batch 200/898 | Loss: 0.5073 | CLoss: 0.3615 | FLoss: 0.2917 | LR: 1.96e-04\n",
      "  Batch 210/898 | Loss: 0.5827 | CLoss: 0.3949 | FLoss: 0.3756 | LR: 1.96e-04\n",
      "  Batch 220/898 | Loss: 0.6496 | CLoss: 0.4756 | FLoss: 0.3481 | LR: 1.96e-04\n",
      "  Batch 230/898 | Loss: 0.6887 | CLoss: 0.5280 | FLoss: 0.3215 | LR: 1.96e-04\n",
      "  Batch 240/898 | Loss: 1.5030 | CLoss: 1.1974 | FLoss: 0.6111 | LR: 1.96e-04\n",
      "  Batch 250/898 | Loss: 0.7324 | CLoss: 0.5434 | FLoss: 0.3779 | LR: 1.96e-04\n",
      "  Batch 260/898 | Loss: 0.6938 | CLoss: 0.5795 | FLoss: 0.2287 | LR: 1.96e-04\n",
      "  Batch 270/898 | Loss: 0.7036 | CLoss: 0.5156 | FLoss: 0.3761 | LR: 1.96e-04\n",
      "  Batch 280/898 | Loss: 1.4379 | CLoss: 1.1155 | FLoss: 0.6448 | LR: 1.96e-04\n",
      "  Batch 290/898 | Loss: 0.7210 | CLoss: 0.4911 | FLoss: 0.4599 | LR: 1.96e-04\n",
      "  Batch 300/898 | Loss: 0.4609 | CLoss: 0.3790 | FLoss: 0.1638 | LR: 1.96e-04\n",
      "  Batch 310/898 | Loss: 0.7594 | CLoss: 0.5352 | FLoss: 0.4484 | LR: 1.96e-04\n",
      "  Batch 320/898 | Loss: 0.5249 | CLoss: 0.4036 | FLoss: 0.2425 | LR: 1.96e-04\n",
      "  Batch 330/898 | Loss: 1.2530 | CLoss: 1.0433 | FLoss: 0.4195 | LR: 1.96e-04\n",
      "  Batch 340/898 | Loss: 0.5549 | CLoss: 0.4397 | FLoss: 0.2303 | LR: 1.96e-04\n",
      "  Batch 350/898 | Loss: 1.1372 | CLoss: 0.9041 | FLoss: 0.4662 | LR: 1.96e-04\n",
      "  Batch 360/898 | Loss: 0.6553 | CLoss: 0.4115 | FLoss: 0.4877 | LR: 1.96e-04\n",
      "  Batch 370/898 | Loss: 0.7289 | CLoss: 0.5634 | FLoss: 0.3310 | LR: 1.96e-04\n",
      "  Batch 380/898 | Loss: 0.5931 | CLoss: 0.4540 | FLoss: 0.2783 | LR: 1.96e-04\n",
      "  Batch 390/898 | Loss: 1.0015 | CLoss: 0.8188 | FLoss: 0.3654 | LR: 1.96e-04\n",
      "  Batch 400/898 | Loss: 0.6587 | CLoss: 0.4886 | FLoss: 0.3403 | LR: 1.96e-04\n",
      "  Batch 410/898 | Loss: 0.6931 | CLoss: 0.6026 | FLoss: 0.1811 | LR: 1.96e-04\n",
      "  Batch 420/898 | Loss: 0.8161 | CLoss: 0.6245 | FLoss: 0.3832 | LR: 1.96e-04\n",
      "  Batch 430/898 | Loss: 0.7152 | CLoss: 0.5460 | FLoss: 0.3384 | LR: 1.96e-04\n",
      "  Batch 440/898 | Loss: 1.1054 | CLoss: 0.8464 | FLoss: 0.5182 | LR: 1.96e-04\n",
      "  Batch 450/898 | Loss: 0.9215 | CLoss: 0.6850 | FLoss: 0.4731 | LR: 1.96e-04\n",
      "  Batch 460/898 | Loss: 0.7133 | CLoss: 0.5034 | FLoss: 0.4197 | LR: 1.96e-04\n",
      "  Batch 470/898 | Loss: 0.7625 | CLoss: 0.5074 | FLoss: 0.5102 | LR: 1.96e-04\n",
      "  Batch 480/898 | Loss: 0.7416 | CLoss: 0.5818 | FLoss: 0.3196 | LR: 1.96e-04\n",
      "  Batch 490/898 | Loss: 0.7400 | CLoss: 0.6004 | FLoss: 0.2793 | LR: 1.96e-04\n",
      "  Batch 500/898 | Loss: 1.1418 | CLoss: 0.9078 | FLoss: 0.4680 | LR: 1.96e-04\n",
      "  Batch 510/898 | Loss: 0.9066 | CLoss: 0.7837 | FLoss: 0.2458 | LR: 1.96e-04\n",
      "  Batch 520/898 | Loss: 0.5974 | CLoss: 0.4023 | FLoss: 0.3902 | LR: 1.96e-04\n",
      "  Batch 530/898 | Loss: 0.5407 | CLoss: 0.2921 | FLoss: 0.4972 | LR: 1.96e-04\n",
      "  Batch 540/898 | Loss: 0.9976 | CLoss: 0.8162 | FLoss: 0.3628 | LR: 1.96e-04\n",
      "  Batch 550/898 | Loss: 0.6271 | CLoss: 0.5241 | FLoss: 0.2059 | LR: 1.96e-04\n",
      "  Batch 560/898 | Loss: 0.6232 | CLoss: 0.5264 | FLoss: 0.1936 | LR: 1.96e-04\n",
      "  Batch 570/898 | Loss: 0.7232 | CLoss: 0.5399 | FLoss: 0.3667 | LR: 1.96e-04\n",
      "  Batch 580/898 | Loss: 0.5270 | CLoss: 0.3764 | FLoss: 0.3012 | LR: 1.96e-04\n",
      "  Batch 590/898 | Loss: 1.0942 | CLoss: 0.8233 | FLoss: 0.5419 | LR: 1.96e-04\n",
      "  Batch 600/898 | Loss: 0.5984 | CLoss: 0.4765 | FLoss: 0.2439 | LR: 1.96e-04\n",
      "  Batch 610/898 | Loss: 1.3450 | CLoss: 1.1362 | FLoss: 0.4176 | LR: 1.96e-04\n",
      "  Batch 620/898 | Loss: 0.5418 | CLoss: 0.3728 | FLoss: 0.3379 | LR: 1.96e-04\n",
      "  Batch 630/898 | Loss: 0.6023 | CLoss: 0.4364 | FLoss: 0.3317 | LR: 1.96e-04\n",
      "  Batch 640/898 | Loss: 0.5168 | CLoss: 0.4001 | FLoss: 0.2335 | LR: 1.96e-04\n",
      "  Batch 650/898 | Loss: 0.5842 | CLoss: 0.4367 | FLoss: 0.2950 | LR: 1.96e-04\n",
      "  Batch 660/898 | Loss: 0.5601 | CLoss: 0.4555 | FLoss: 0.2092 | LR: 1.96e-04\n",
      "  Batch 670/898 | Loss: 0.4450 | CLoss: 0.3585 | FLoss: 0.1730 | LR: 1.96e-04\n",
      "  Batch 680/898 | Loss: 0.6743 | CLoss: 0.4632 | FLoss: 0.4222 | LR: 1.96e-04\n",
      "  Batch 690/898 | Loss: 0.5269 | CLoss: 0.3134 | FLoss: 0.4270 | LR: 1.96e-04\n",
      "  Batch 700/898 | Loss: 0.6140 | CLoss: 0.4467 | FLoss: 0.3346 | LR: 1.96e-04\n",
      "  Batch 710/898 | Loss: 0.5558 | CLoss: 0.3616 | FLoss: 0.3885 | LR: 1.96e-04\n",
      "  Batch 720/898 | Loss: 0.9743 | CLoss: 0.7693 | FLoss: 0.4099 | LR: 1.96e-04\n",
      "  Batch 730/898 | Loss: 0.6405 | CLoss: 0.5062 | FLoss: 0.2685 | LR: 1.96e-04\n",
      "  Batch 740/898 | Loss: 0.8940 | CLoss: 0.7743 | FLoss: 0.2395 | LR: 1.96e-04\n",
      "  Batch 750/898 | Loss: 0.9812 | CLoss: 0.7856 | FLoss: 0.3913 | LR: 1.96e-04\n",
      "  Batch 760/898 | Loss: 0.9885 | CLoss: 0.7748 | FLoss: 0.4274 | LR: 1.96e-04\n",
      "  Batch 770/898 | Loss: 0.7901 | CLoss: 0.5991 | FLoss: 0.3820 | LR: 1.96e-04\n",
      "  Batch 780/898 | Loss: 0.7953 | CLoss: 0.5844 | FLoss: 0.4218 | LR: 1.96e-04\n",
      "  Batch 790/898 | Loss: 0.8538 | CLoss: 0.7628 | FLoss: 0.1818 | LR: 1.96e-04\n",
      "  Batch 800/898 | Loss: 0.4540 | CLoss: 0.3476 | FLoss: 0.2129 | LR: 1.96e-04\n",
      "  Batch 810/898 | Loss: 0.5906 | CLoss: 0.4015 | FLoss: 0.3782 | LR: 1.96e-04\n",
      "  Batch 820/898 | Loss: 0.7256 | CLoss: 0.5626 | FLoss: 0.3260 | LR: 1.96e-04\n",
      "  Batch 830/898 | Loss: 0.8461 | CLoss: 0.6469 | FLoss: 0.3983 | LR: 1.96e-04\n",
      "  Batch 840/898 | Loss: 0.6562 | CLoss: 0.5293 | FLoss: 0.2536 | LR: 1.96e-04\n",
      "  Batch 850/898 | Loss: 0.9012 | CLoss: 0.6722 | FLoss: 0.4581 | LR: 1.96e-04\n",
      "  Batch 860/898 | Loss: 0.7885 | CLoss: 0.6249 | FLoss: 0.3272 | LR: 1.96e-04\n",
      "  Batch 870/898 | Loss: 0.9361 | CLoss: 0.7744 | FLoss: 0.3233 | LR: 1.96e-04\n",
      "  Batch 880/898 | Loss: 0.7312 | CLoss: 0.5412 | FLoss: 0.3800 | LR: 1.96e-04\n",
      "  Batch 890/898 | Loss: 0.5167 | CLoss: 0.3015 | FLoss: 0.4306 | LR: 1.96e-04\n",
      "  Batch 898/898 | Loss: 0.2905 | CLoss: 0.0013 | FLoss: 0.5783 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.7657\n",
      "  Last Batch Loss: 0.2905\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 0.5195 | Batch Acc: 82.76%\n",
      "    Val Batch 010/102 | Loss: 0.2171 | Batch Acc: 94.83%\n",
      "    Val Batch 015/102 | Loss: 0.3772 | Batch Acc: 87.93%\n",
      "    Val Batch 020/102 | Loss: 0.5457 | Batch Acc: 87.93%\n",
      "    Val Batch 025/102 | Loss: 0.1300 | Batch Acc: 96.55%\n",
      "    Val Batch 030/102 | Loss: 0.2579 | Batch Acc: 89.66%\n",
      "    Val Batch 035/102 | Loss: 0.2781 | Batch Acc: 91.38%\n",
      "    Val Batch 040/102 | Loss: 0.4195 | Batch Acc: 72.41%\n",
      "    Val Batch 045/102 | Loss: 0.8325 | Batch Acc: 31.03%\n",
      "    Val Batch 050/102 | Loss: 0.5961 | Batch Acc: 81.03%\n",
      "    Val Batch 055/102 | Loss: 0.6291 | Batch Acc: 79.31%\n",
      "    Val Batch 060/102 | Loss: 1.6747 | Batch Acc: 58.62%\n",
      "    Val Batch 065/102 | Loss: 0.3416 | Batch Acc: 86.21%\n",
      "    Val Batch 070/102 | Loss: 0.3974 | Batch Acc: 84.48%\n",
      "    Val Batch 075/102 | Loss: 0.3226 | Batch Acc: 89.66%\n",
      "    Val Batch 080/102 | Loss: 0.0856 | Batch Acc: 94.83%\n",
      "    Val Batch 085/102 | Loss: 0.2980 | Batch Acc: 91.38%\n",
      "    Val Batch 090/102 | Loss: 0.0787 | Batch Acc: 94.83%\n",
      "    Val Batch 095/102 | Loss: 0.1947 | Batch Acc: 96.55%\n",
      "    Val Batch 100/102 | Loss: 0.2393 | Batch Acc: 93.10%\n",
      "    Val Batch 102/102 | Loss: 0.1979 | Batch Acc: 96.55%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.4428 | Accuracy: 85.37%\n",
      "  Current Best Acc: 85.97%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/898 | Loss: 0.7442 | CLoss: 0.5947 | FLoss: 0.2990 | LR: 1.04e-04\n",
      "  Batch 020/898 | Loss: 0.8755 | CLoss: 0.6500 | FLoss: 0.4510 | LR: 1.04e-04\n",
      "  Batch 030/898 | Loss: 1.0160 | CLoss: 0.8095 | FLoss: 0.4130 | LR: 1.04e-04\n",
      "  Batch 040/898 | Loss: 0.4142 | CLoss: 0.2948 | FLoss: 0.2388 | LR: 1.04e-04\n",
      "  Batch 050/898 | Loss: 0.6477 | CLoss: 0.4894 | FLoss: 0.3166 | LR: 1.04e-04\n",
      "  Batch 060/898 | Loss: 0.7499 | CLoss: 0.6002 | FLoss: 0.2994 | LR: 1.04e-04\n",
      "  Batch 070/898 | Loss: 0.9655 | CLoss: 0.7810 | FLoss: 0.3692 | LR: 1.04e-04\n",
      "  Batch 080/898 | Loss: 0.6363 | CLoss: 0.5329 | FLoss: 0.2068 | LR: 1.04e-04\n",
      "  Batch 090/898 | Loss: 0.6121 | CLoss: 0.4507 | FLoss: 0.3229 | LR: 1.04e-04\n",
      "  Batch 100/898 | Loss: 0.5912 | CLoss: 0.4465 | FLoss: 0.2893 | LR: 1.04e-04\n",
      "  Batch 110/898 | Loss: 1.1220 | CLoss: 0.9079 | FLoss: 0.4280 | LR: 1.04e-04\n",
      "  Batch 120/898 | Loss: 0.5461 | CLoss: 0.4033 | FLoss: 0.2856 | LR: 1.04e-04\n",
      "  Batch 130/898 | Loss: 1.2761 | CLoss: 1.0401 | FLoss: 0.4721 | LR: 1.04e-04\n",
      "  Batch 140/898 | Loss: 0.8092 | CLoss: 0.5607 | FLoss: 0.4969 | LR: 1.04e-04\n",
      "  Batch 150/898 | Loss: 0.7611 | CLoss: 0.5723 | FLoss: 0.3776 | LR: 1.04e-04\n",
      "  Batch 160/898 | Loss: 0.4949 | CLoss: 0.3689 | FLoss: 0.2521 | LR: 1.04e-04\n",
      "  Batch 170/898 | Loss: 0.6589 | CLoss: 0.4893 | FLoss: 0.3393 | LR: 1.04e-04\n",
      "  Batch 180/898 | Loss: 0.7413 | CLoss: 0.5843 | FLoss: 0.3140 | LR: 1.04e-04\n",
      "  Batch 190/898 | Loss: 0.6911 | CLoss: 0.5622 | FLoss: 0.2577 | LR: 1.04e-04\n",
      "  Batch 200/898 | Loss: 0.6965 | CLoss: 0.5778 | FLoss: 0.2374 | LR: 1.04e-04\n",
      "  Batch 210/898 | Loss: 0.7692 | CLoss: 0.5970 | FLoss: 0.3443 | LR: 1.04e-04\n",
      "  Batch 220/898 | Loss: 0.7117 | CLoss: 0.5121 | FLoss: 0.3992 | LR: 1.04e-04\n",
      "  Batch 230/898 | Loss: 0.9104 | CLoss: 0.6521 | FLoss: 0.5165 | LR: 1.04e-04\n",
      "  Batch 240/898 | Loss: 1.1039 | CLoss: 0.8711 | FLoss: 0.4657 | LR: 1.04e-04\n",
      "  Batch 250/898 | Loss: 0.8478 | CLoss: 0.6383 | FLoss: 0.4190 | LR: 1.04e-04\n",
      "  Batch 260/898 | Loss: 0.4720 | CLoss: 0.3512 | FLoss: 0.2415 | LR: 1.04e-04\n",
      "  Batch 270/898 | Loss: 0.7965 | CLoss: 0.6096 | FLoss: 0.3738 | LR: 1.04e-04\n",
      "  Batch 280/898 | Loss: 0.5770 | CLoss: 0.3853 | FLoss: 0.3834 | LR: 1.04e-04\n",
      "  Batch 290/898 | Loss: 0.6673 | CLoss: 0.5501 | FLoss: 0.2344 | LR: 1.04e-04\n",
      "  Batch 300/898 | Loss: 0.6914 | CLoss: 0.5447 | FLoss: 0.2935 | LR: 1.04e-04\n",
      "  Batch 310/898 | Loss: 0.4721 | CLoss: 0.3814 | FLoss: 0.1814 | LR: 1.04e-04\n",
      "  Batch 320/898 | Loss: 0.4623 | CLoss: 0.3091 | FLoss: 0.3062 | LR: 1.04e-04\n",
      "  Batch 330/898 | Loss: 0.6275 | CLoss: 0.5024 | FLoss: 0.2503 | LR: 1.04e-04\n",
      "  Batch 340/898 | Loss: 0.4457 | CLoss: 0.3334 | FLoss: 0.2246 | LR: 1.04e-04\n",
      "  Batch 350/898 | Loss: 0.4371 | CLoss: 0.3110 | FLoss: 0.2522 | LR: 1.04e-04\n",
      "  Batch 360/898 | Loss: 0.9797 | CLoss: 0.7898 | FLoss: 0.3797 | LR: 1.04e-04\n",
      "  Batch 370/898 | Loss: 0.9258 | CLoss: 0.6636 | FLoss: 0.5244 | LR: 1.04e-04\n",
      "  Batch 380/898 | Loss: 0.6599 | CLoss: 0.5116 | FLoss: 0.2967 | LR: 1.04e-04\n",
      "  Batch 390/898 | Loss: 0.6074 | CLoss: 0.4714 | FLoss: 0.2719 | LR: 1.04e-04\n",
      "  Batch 400/898 | Loss: 0.9570 | CLoss: 0.7205 | FLoss: 0.4730 | LR: 1.04e-04\n",
      "  Batch 410/898 | Loss: 0.5871 | CLoss: 0.4390 | FLoss: 0.2963 | LR: 1.04e-04\n",
      "  Batch 420/898 | Loss: 0.7984 | CLoss: 0.6573 | FLoss: 0.2821 | LR: 1.04e-04\n",
      "  Batch 430/898 | Loss: 0.7594 | CLoss: 0.5843 | FLoss: 0.3501 | LR: 1.04e-04\n",
      "  Batch 440/898 | Loss: 0.4506 | CLoss: 0.2667 | FLoss: 0.3678 | LR: 1.04e-04\n",
      "  Batch 450/898 | Loss: 0.7197 | CLoss: 0.5943 | FLoss: 0.2509 | LR: 1.04e-04\n",
      "  Batch 460/898 | Loss: 1.1031 | CLoss: 0.8768 | FLoss: 0.4527 | LR: 1.04e-04\n",
      "  Batch 470/898 | Loss: 1.0609 | CLoss: 0.8237 | FLoss: 0.4744 | LR: 1.04e-04\n",
      "  Batch 480/898 | Loss: 0.4282 | CLoss: 0.3319 | FLoss: 0.1926 | LR: 1.04e-04\n",
      "  Batch 490/898 | Loss: 0.4883 | CLoss: 0.3723 | FLoss: 0.2319 | LR: 1.04e-04\n",
      "  Batch 500/898 | Loss: 0.6813 | CLoss: 0.5675 | FLoss: 0.2275 | LR: 1.04e-04\n",
      "  Batch 510/898 | Loss: 0.5718 | CLoss: 0.4731 | FLoss: 0.1974 | LR: 1.04e-04\n",
      "  Batch 520/898 | Loss: 0.3301 | CLoss: 0.2460 | FLoss: 0.1683 | LR: 1.04e-04\n",
      "  Batch 530/898 | Loss: 0.6544 | CLoss: 0.4762 | FLoss: 0.3564 | LR: 1.04e-04\n",
      "  Batch 540/898 | Loss: 0.4496 | CLoss: 0.2862 | FLoss: 0.3267 | LR: 1.04e-04\n",
      "  Batch 550/898 | Loss: 0.6959 | CLoss: 0.5561 | FLoss: 0.2797 | LR: 1.04e-04\n",
      "  Batch 560/898 | Loss: 0.7550 | CLoss: 0.5651 | FLoss: 0.3798 | LR: 1.04e-04\n",
      "  Batch 570/898 | Loss: 0.7506 | CLoss: 0.5080 | FLoss: 0.4852 | LR: 1.04e-04\n",
      "  Batch 580/898 | Loss: 0.9186 | CLoss: 0.7596 | FLoss: 0.3180 | LR: 1.04e-04\n",
      "  Batch 590/898 | Loss: 0.8955 | CLoss: 0.7168 | FLoss: 0.3573 | LR: 1.04e-04\n",
      "  Batch 600/898 | Loss: 0.5724 | CLoss: 0.3896 | FLoss: 0.3657 | LR: 1.04e-04\n",
      "  Batch 610/898 | Loss: 0.4401 | CLoss: 0.3181 | FLoss: 0.2441 | LR: 1.04e-04\n",
      "  Batch 620/898 | Loss: 0.4978 | CLoss: 0.3735 | FLoss: 0.2485 | LR: 1.04e-04\n",
      "  Batch 630/898 | Loss: 0.6171 | CLoss: 0.5294 | FLoss: 0.1754 | LR: 1.04e-04\n",
      "  Batch 640/898 | Loss: 0.9236 | CLoss: 0.7342 | FLoss: 0.3788 | LR: 1.04e-04\n",
      "  Batch 650/898 | Loss: 0.7989 | CLoss: 0.5726 | FLoss: 0.4525 | LR: 1.04e-04\n",
      "  Batch 660/898 | Loss: 0.3199 | CLoss: 0.2546 | FLoss: 0.1306 | LR: 1.04e-04\n",
      "  Batch 670/898 | Loss: 0.7997 | CLoss: 0.5879 | FLoss: 0.4236 | LR: 1.04e-04\n",
      "  Batch 680/898 | Loss: 0.6689 | CLoss: 0.5269 | FLoss: 0.2841 | LR: 1.04e-04\n",
      "  Batch 690/898 | Loss: 0.5104 | CLoss: 0.3916 | FLoss: 0.2374 | LR: 1.04e-04\n",
      "  Batch 700/898 | Loss: 0.7145 | CLoss: 0.5941 | FLoss: 0.2408 | LR: 1.04e-04\n",
      "  Batch 710/898 | Loss: 0.6125 | CLoss: 0.5158 | FLoss: 0.1934 | LR: 1.04e-04\n",
      "  Batch 720/898 | Loss: 0.6371 | CLoss: 0.4794 | FLoss: 0.3154 | LR: 1.04e-04\n",
      "  Batch 730/898 | Loss: 0.6123 | CLoss: 0.4989 | FLoss: 0.2268 | LR: 1.04e-04\n",
      "  Batch 740/898 | Loss: 0.7359 | CLoss: 0.5515 | FLoss: 0.3688 | LR: 1.04e-04\n",
      "  Batch 750/898 | Loss: 0.8535 | CLoss: 0.6424 | FLoss: 0.4223 | LR: 1.04e-04\n",
      "  Batch 760/898 | Loss: 0.7018 | CLoss: 0.4539 | FLoss: 0.4957 | LR: 1.04e-04\n",
      "  Batch 770/898 | Loss: 0.6164 | CLoss: 0.3559 | FLoss: 0.5210 | LR: 1.04e-04\n",
      "  Batch 780/898 | Loss: 0.6992 | CLoss: 0.5391 | FLoss: 0.3203 | LR: 1.04e-04\n",
      "  Batch 790/898 | Loss: 0.7857 | CLoss: 0.5365 | FLoss: 0.4985 | LR: 1.04e-04\n",
      "  Batch 800/898 | Loss: 0.4164 | CLoss: 0.3303 | FLoss: 0.1723 | LR: 1.04e-04\n",
      "  Batch 810/898 | Loss: 0.4761 | CLoss: 0.3013 | FLoss: 0.3496 | LR: 1.04e-04\n",
      "  Batch 820/898 | Loss: 0.5208 | CLoss: 0.4224 | FLoss: 0.1967 | LR: 1.04e-04\n",
      "  Batch 830/898 | Loss: 0.4820 | CLoss: 0.3627 | FLoss: 0.2386 | LR: 1.04e-04\n",
      "  Batch 840/898 | Loss: 0.7502 | CLoss: 0.5818 | FLoss: 0.3368 | LR: 1.04e-04\n",
      "  Batch 850/898 | Loss: 0.4586 | CLoss: 0.3436 | FLoss: 0.2299 | LR: 1.04e-04\n",
      "  Batch 860/898 | Loss: 0.7967 | CLoss: 0.6353 | FLoss: 0.3226 | LR: 1.04e-04\n",
      "  Batch 870/898 | Loss: 0.6258 | CLoss: 0.4449 | FLoss: 0.3617 | LR: 1.04e-04\n",
      "  Batch 880/898 | Loss: 0.8086 | CLoss: 0.6377 | FLoss: 0.3417 | LR: 1.04e-04\n",
      "  Batch 890/898 | Loss: 0.5712 | CLoss: 0.4587 | FLoss: 0.2250 | LR: 1.04e-04\n",
      "  Batch 898/898 | Loss: 2.6863 | CLoss: 1.5790 | FLoss: 2.2146 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.6885\n",
      "  Last Batch Loss: 2.6863\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 0.3452 | Batch Acc: 82.76%\n",
      "    Val Batch 010/102 | Loss: 0.1305 | Batch Acc: 94.83%\n",
      "    Val Batch 015/102 | Loss: 0.2078 | Batch Acc: 93.10%\n",
      "    Val Batch 020/102 | Loss: 0.5921 | Batch Acc: 60.34%\n",
      "    Val Batch 025/102 | Loss: 0.3644 | Batch Acc: 89.66%\n",
      "    Val Batch 030/102 | Loss: 0.0938 | Batch Acc: 94.83%\n",
      "    Val Batch 035/102 | Loss: 0.1766 | Batch Acc: 91.38%\n",
      "    Val Batch 040/102 | Loss: 0.5950 | Batch Acc: 91.38%\n",
      "    Val Batch 045/102 | Loss: 0.9104 | Batch Acc: 93.10%\n",
      "    Val Batch 050/102 | Loss: 0.4290 | Batch Acc: 84.48%\n",
      "    Val Batch 055/102 | Loss: 0.3876 | Batch Acc: 89.66%\n",
      "    Val Batch 060/102 | Loss: 1.0584 | Batch Acc: 74.14%\n",
      "    Val Batch 065/102 | Loss: 0.2019 | Batch Acc: 91.38%\n",
      "    Val Batch 070/102 | Loss: 0.2112 | Batch Acc: 93.10%\n",
      "    Val Batch 075/102 | Loss: 0.0967 | Batch Acc: 98.28%\n",
      "    Val Batch 080/102 | Loss: 0.3602 | Batch Acc: 87.93%\n",
      "    Val Batch 085/102 | Loss: 0.2487 | Batch Acc: 91.38%\n",
      "    Val Batch 090/102 | Loss: 0.1605 | Batch Acc: 93.10%\n",
      "    Val Batch 095/102 | Loss: 0.2232 | Batch Acc: 93.10%\n",
      "    Val Batch 100/102 | Loss: 0.1849 | Batch Acc: 93.10%\n",
      "    Val Batch 102/102 | Loss: 0.1490 | Batch Acc: 93.10%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.3993 | Accuracy: 87.87%\n",
      "  Current Best Acc: 87.87%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/898 | Loss: 0.5439 | CLoss: 0.3281 | FLoss: 0.4316 | LR: 2.86e-05\n",
      "  Batch 020/898 | Loss: 0.6266 | CLoss: 0.5533 | FLoss: 0.1466 | LR: 2.86e-05\n",
      "  Batch 030/898 | Loss: 0.6439 | CLoss: 0.4501 | FLoss: 0.3875 | LR: 2.86e-05\n",
      "  Batch 040/898 | Loss: 1.0684 | CLoss: 0.9050 | FLoss: 0.3267 | LR: 2.86e-05\n",
      "  Batch 050/898 | Loss: 0.4613 | CLoss: 0.3738 | FLoss: 0.1750 | LR: 2.86e-05\n",
      "  Batch 060/898 | Loss: 0.7464 | CLoss: 0.6005 | FLoss: 0.2916 | LR: 2.86e-05\n",
      "  Batch 070/898 | Loss: 0.8410 | CLoss: 0.6012 | FLoss: 0.4795 | LR: 2.86e-05\n",
      "  Batch 080/898 | Loss: 0.5132 | CLoss: 0.3970 | FLoss: 0.2326 | LR: 2.86e-05\n",
      "  Batch 090/898 | Loss: 0.4078 | CLoss: 0.2695 | FLoss: 0.2765 | LR: 2.86e-05\n",
      "  Batch 100/898 | Loss: 0.8125 | CLoss: 0.6772 | FLoss: 0.2705 | LR: 2.86e-05\n",
      "  Batch 110/898 | Loss: 0.7955 | CLoss: 0.6026 | FLoss: 0.3859 | LR: 2.86e-05\n",
      "  Batch 120/898 | Loss: 0.5009 | CLoss: 0.3617 | FLoss: 0.2785 | LR: 2.86e-05\n",
      "  Batch 130/898 | Loss: 0.6016 | CLoss: 0.5036 | FLoss: 0.1959 | LR: 2.86e-05\n",
      "  Batch 140/898 | Loss: 0.6204 | CLoss: 0.5077 | FLoss: 0.2254 | LR: 2.86e-05\n",
      "  Batch 150/898 | Loss: 0.9192 | CLoss: 0.7375 | FLoss: 0.3634 | LR: 2.86e-05\n",
      "  Batch 160/898 | Loss: 0.4890 | CLoss: 0.3584 | FLoss: 0.2613 | LR: 2.86e-05\n",
      "  Batch 170/898 | Loss: 0.3104 | CLoss: 0.2265 | FLoss: 0.1678 | LR: 2.86e-05\n",
      "  Batch 180/898 | Loss: 0.6863 | CLoss: 0.5884 | FLoss: 0.1957 | LR: 2.86e-05\n",
      "  Batch 190/898 | Loss: 0.5883 | CLoss: 0.4692 | FLoss: 0.2382 | LR: 2.86e-05\n",
      "  Batch 200/898 | Loss: 0.6009 | CLoss: 0.4578 | FLoss: 0.2861 | LR: 2.86e-05\n",
      "  Batch 210/898 | Loss: 0.3400 | CLoss: 0.2293 | FLoss: 0.2214 | LR: 2.86e-05\n",
      "  Batch 220/898 | Loss: 0.5027 | CLoss: 0.3728 | FLoss: 0.2597 | LR: 2.86e-05\n",
      "  Batch 230/898 | Loss: 0.6018 | CLoss: 0.5063 | FLoss: 0.1909 | LR: 2.86e-05\n",
      "  Batch 240/898 | Loss: 0.5267 | CLoss: 0.3889 | FLoss: 0.2755 | LR: 2.86e-05\n",
      "  Batch 250/898 | Loss: 0.7657 | CLoss: 0.5892 | FLoss: 0.3530 | LR: 2.86e-05\n",
      "  Batch 260/898 | Loss: 0.4950 | CLoss: 0.3467 | FLoss: 0.2966 | LR: 2.86e-05\n",
      "  Batch 270/898 | Loss: 0.6892 | CLoss: 0.5548 | FLoss: 0.2687 | LR: 2.86e-05\n",
      "  Batch 280/898 | Loss: 0.7213 | CLoss: 0.5704 | FLoss: 0.3017 | LR: 2.86e-05\n",
      "  Batch 290/898 | Loss: 0.7750 | CLoss: 0.5813 | FLoss: 0.3875 | LR: 2.86e-05\n",
      "  Batch 300/898 | Loss: 0.6490 | CLoss: 0.4533 | FLoss: 0.3914 | LR: 2.86e-05\n",
      "  Batch 310/898 | Loss: 0.7034 | CLoss: 0.5507 | FLoss: 0.3053 | LR: 2.86e-05\n",
      "  Batch 320/898 | Loss: 0.6246 | CLoss: 0.5028 | FLoss: 0.2437 | LR: 2.86e-05\n",
      "  Batch 330/898 | Loss: 0.7764 | CLoss: 0.6226 | FLoss: 0.3076 | LR: 2.86e-05\n",
      "  Batch 340/898 | Loss: 1.2862 | CLoss: 1.0381 | FLoss: 0.4960 | LR: 2.86e-05\n",
      "  Batch 350/898 | Loss: 0.5195 | CLoss: 0.3290 | FLoss: 0.3809 | LR: 2.86e-05\n",
      "  Batch 360/898 | Loss: 0.9486 | CLoss: 0.7955 | FLoss: 0.3060 | LR: 2.86e-05\n",
      "  Batch 370/898 | Loss: 0.8629 | CLoss: 0.7192 | FLoss: 0.2872 | LR: 2.86e-05\n",
      "  Batch 380/898 | Loss: 0.3836 | CLoss: 0.2289 | FLoss: 0.3095 | LR: 2.86e-05\n",
      "  Batch 390/898 | Loss: 0.6805 | CLoss: 0.5410 | FLoss: 0.2791 | LR: 2.86e-05\n",
      "  Batch 400/898 | Loss: 0.5852 | CLoss: 0.3860 | FLoss: 0.3984 | LR: 2.86e-05\n",
      "  Batch 410/898 | Loss: 0.6505 | CLoss: 0.5106 | FLoss: 0.2797 | LR: 2.86e-05\n",
      "  Batch 420/898 | Loss: 0.5296 | CLoss: 0.4530 | FLoss: 0.1531 | LR: 2.86e-05\n",
      "  Batch 430/898 | Loss: 0.9461 | CLoss: 0.7741 | FLoss: 0.3440 | LR: 2.86e-05\n",
      "  Batch 440/898 | Loss: 0.4742 | CLoss: 0.3676 | FLoss: 0.2131 | LR: 2.86e-05\n",
      "  Batch 450/898 | Loss: 0.7429 | CLoss: 0.5834 | FLoss: 0.3189 | LR: 2.86e-05\n",
      "  Batch 460/898 | Loss: 0.5138 | CLoss: 0.4502 | FLoss: 0.1273 | LR: 2.86e-05\n",
      "  Batch 470/898 | Loss: 0.4719 | CLoss: 0.3691 | FLoss: 0.2057 | LR: 2.86e-05\n",
      "  Batch 480/898 | Loss: 0.5599 | CLoss: 0.4679 | FLoss: 0.1840 | LR: 2.86e-05\n",
      "  Batch 490/898 | Loss: 0.7842 | CLoss: 0.6615 | FLoss: 0.2454 | LR: 2.86e-05\n",
      "  Batch 500/898 | Loss: 0.1720 | CLoss: 0.0982 | FLoss: 0.1476 | LR: 2.86e-05\n",
      "  Batch 510/898 | Loss: 0.4848 | CLoss: 0.3492 | FLoss: 0.2712 | LR: 2.86e-05\n",
      "  Batch 520/898 | Loss: 0.5282 | CLoss: 0.4389 | FLoss: 0.1785 | LR: 2.86e-05\n",
      "  Batch 530/898 | Loss: 0.4247 | CLoss: 0.3552 | FLoss: 0.1390 | LR: 2.86e-05\n",
      "  Batch 540/898 | Loss: 0.7454 | CLoss: 0.6386 | FLoss: 0.2135 | LR: 2.86e-05\n",
      "  Batch 550/898 | Loss: 0.3870 | CLoss: 0.2875 | FLoss: 0.1991 | LR: 2.86e-05\n",
      "  Batch 560/898 | Loss: 0.4825 | CLoss: 0.3645 | FLoss: 0.2361 | LR: 2.86e-05\n",
      "  Batch 570/898 | Loss: 0.8468 | CLoss: 0.6952 | FLoss: 0.3033 | LR: 2.86e-05\n",
      "  Batch 580/898 | Loss: 0.6364 | CLoss: 0.4937 | FLoss: 0.2854 | LR: 2.86e-05\n",
      "  Batch 590/898 | Loss: 0.4507 | CLoss: 0.3140 | FLoss: 0.2733 | LR: 2.86e-05\n",
      "  Batch 600/898 | Loss: 0.4640 | CLoss: 0.4151 | FLoss: 0.0978 | LR: 2.86e-05\n",
      "  Batch 610/898 | Loss: 0.6923 | CLoss: 0.5799 | FLoss: 0.2249 | LR: 2.86e-05\n",
      "  Batch 620/898 | Loss: 0.3925 | CLoss: 0.2616 | FLoss: 0.2618 | LR: 2.86e-05\n",
      "  Batch 630/898 | Loss: 0.7606 | CLoss: 0.6477 | FLoss: 0.2258 | LR: 2.86e-05\n",
      "  Batch 640/898 | Loss: 0.5306 | CLoss: 0.4451 | FLoss: 0.1709 | LR: 2.86e-05\n",
      "  Batch 650/898 | Loss: 0.3895 | CLoss: 0.2710 | FLoss: 0.2372 | LR: 2.86e-05\n",
      "  Batch 660/898 | Loss: 0.5235 | CLoss: 0.4054 | FLoss: 0.2362 | LR: 2.86e-05\n",
      "  Batch 670/898 | Loss: 0.9141 | CLoss: 0.7651 | FLoss: 0.2979 | LR: 2.86e-05\n",
      "  Batch 680/898 | Loss: 0.7679 | CLoss: 0.5998 | FLoss: 0.3361 | LR: 2.86e-05\n",
      "  Batch 690/898 | Loss: 0.3857 | CLoss: 0.2830 | FLoss: 0.2053 | LR: 2.86e-05\n",
      "  Batch 700/898 | Loss: 0.5324 | CLoss: 0.3967 | FLoss: 0.2713 | LR: 2.86e-05\n",
      "  Batch 710/898 | Loss: 0.8756 | CLoss: 0.6375 | FLoss: 0.4761 | LR: 2.86e-05\n",
      "  Batch 720/898 | Loss: 0.6700 | CLoss: 0.4932 | FLoss: 0.3535 | LR: 2.86e-05\n",
      "  Batch 730/898 | Loss: 0.4610 | CLoss: 0.3711 | FLoss: 0.1800 | LR: 2.86e-05\n",
      "  Batch 740/898 | Loss: 0.7245 | CLoss: 0.5988 | FLoss: 0.2514 | LR: 2.86e-05\n",
      "  Batch 750/898 | Loss: 0.7772 | CLoss: 0.6859 | FLoss: 0.1826 | LR: 2.86e-05\n",
      "  Batch 760/898 | Loss: 1.0834 | CLoss: 0.8691 | FLoss: 0.4286 | LR: 2.86e-05\n",
      "  Batch 770/898 | Loss: 0.8236 | CLoss: 0.6843 | FLoss: 0.2787 | LR: 2.86e-05\n",
      "  Batch 780/898 | Loss: 0.7278 | CLoss: 0.5650 | FLoss: 0.3256 | LR: 2.86e-05\n",
      "  Batch 790/898 | Loss: 0.6618 | CLoss: 0.5065 | FLoss: 0.3105 | LR: 2.86e-05\n",
      "  Batch 800/898 | Loss: 0.8744 | CLoss: 0.7390 | FLoss: 0.2708 | LR: 2.86e-05\n",
      "  Batch 810/898 | Loss: 0.5553 | CLoss: 0.4517 | FLoss: 0.2071 | LR: 2.86e-05\n",
      "  Batch 820/898 | Loss: 0.5729 | CLoss: 0.4172 | FLoss: 0.3115 | LR: 2.86e-05\n",
      "  Batch 830/898 | Loss: 0.4522 | CLoss: 0.3107 | FLoss: 0.2830 | LR: 2.86e-05\n",
      "  Batch 840/898 | Loss: 0.4872 | CLoss: 0.3722 | FLoss: 0.2299 | LR: 2.86e-05\n",
      "  Batch 850/898 | Loss: 0.5605 | CLoss: 0.3183 | FLoss: 0.4844 | LR: 2.86e-05\n",
      "  Batch 860/898 | Loss: 0.8767 | CLoss: 0.7282 | FLoss: 0.2970 | LR: 2.86e-05\n",
      "  Batch 870/898 | Loss: 0.6573 | CLoss: 0.4441 | FLoss: 0.4264 | LR: 2.86e-05\n",
      "  Batch 880/898 | Loss: 0.4449 | CLoss: 0.3231 | FLoss: 0.2437 | LR: 2.86e-05\n",
      "  Batch 890/898 | Loss: 0.5206 | CLoss: 0.3976 | FLoss: 0.2461 | LR: 2.86e-05\n",
      "  Batch 898/898 | Loss: 0.3373 | CLoss: 0.0003 | FLoss: 0.6739 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.6321\n",
      "  Last Batch Loss: 0.3373\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/102 | Loss: 0.4944 | Batch Acc: 77.59%\n",
      "    Val Batch 010/102 | Loss: 0.1750 | Batch Acc: 96.55%\n",
      "    Val Batch 015/102 | Loss: 0.2661 | Batch Acc: 91.38%\n",
      "    Val Batch 020/102 | Loss: 0.5626 | Batch Acc: 82.76%\n",
      "    Val Batch 025/102 | Loss: 0.1252 | Batch Acc: 98.28%\n",
      "    Val Batch 030/102 | Loss: 0.0619 | Batch Acc: 96.55%\n",
      "    Val Batch 035/102 | Loss: 0.1170 | Batch Acc: 98.28%\n",
      "    Val Batch 040/102 | Loss: 0.3382 | Batch Acc: 79.31%\n",
      "    Val Batch 045/102 | Loss: 1.0272 | Batch Acc: 17.24%\n",
      "    Val Batch 050/102 | Loss: 0.3728 | Batch Acc: 87.93%\n",
      "    Val Batch 055/102 | Loss: 0.3505 | Batch Acc: 86.21%\n",
      "    Val Batch 060/102 | Loss: 0.8577 | Batch Acc: 75.86%\n",
      "    Val Batch 065/102 | Loss: 0.3165 | Batch Acc: 93.10%\n",
      "    Val Batch 070/102 | Loss: 0.3463 | Batch Acc: 89.66%\n",
      "    Val Batch 075/102 | Loss: 0.4008 | Batch Acc: 91.38%\n",
      "    Val Batch 080/102 | Loss: 0.2528 | Batch Acc: 93.10%\n",
      "    Val Batch 085/102 | Loss: 0.5803 | Batch Acc: 86.21%\n",
      "    Val Batch 090/102 | Loss: 0.1350 | Batch Acc: 94.83%\n",
      "    Val Batch 095/102 | Loss: 0.1188 | Batch Acc: 98.28%\n",
      "    Val Batch 100/102 | Loss: 0.1369 | Batch Acc: 98.28%\n",
      "    Val Batch 102/102 | Loss: 0.0616 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.3804 | Accuracy: 86.50%\n",
      "  Current Best Acc: 87.87%\n",
      "\n",
      "========================================\n",
      "=== Fold 7 Completed ===\n",
      "Best Validation Accuracy: 87.87%\n",
      "\n",
      "========================================\n",
      "=== Fold 8/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/902 | Loss: 1.7715 | CLoss: 0.7788 | FLoss: 1.9855 | LR: 3.00e-04\n",
      "  Batch 020/902 | Loss: 1.6349 | CLoss: 0.9178 | FLoss: 1.4343 | LR: 3.00e-04\n",
      "  Batch 030/902 | Loss: 1.1872 | CLoss: 0.6901 | FLoss: 0.9942 | LR: 3.00e-04\n",
      "  Batch 040/902 | Loss: 1.6376 | CLoss: 1.1960 | FLoss: 0.8831 | LR: 3.00e-04\n",
      "  Batch 050/902 | Loss: 1.1963 | CLoss: 0.8297 | FLoss: 0.7331 | LR: 3.00e-04\n",
      "  Batch 060/902 | Loss: 0.9970 | CLoss: 0.6780 | FLoss: 0.6381 | LR: 3.00e-04\n",
      "  Batch 070/902 | Loss: 0.5481 | CLoss: 0.3043 | FLoss: 0.4876 | LR: 3.00e-04\n",
      "  Batch 080/902 | Loss: 1.4108 | CLoss: 1.0774 | FLoss: 0.6668 | LR: 3.00e-04\n",
      "  Batch 090/902 | Loss: 0.9729 | CLoss: 0.7069 | FLoss: 0.5322 | LR: 3.00e-04\n",
      "  Batch 100/902 | Loss: 0.9156 | CLoss: 0.6136 | FLoss: 0.6041 | LR: 3.00e-04\n",
      "  Batch 110/902 | Loss: 0.8104 | CLoss: 0.6571 | FLoss: 0.3066 | LR: 3.00e-04\n",
      "  Batch 120/902 | Loss: 1.3130 | CLoss: 1.0501 | FLoss: 0.5258 | LR: 3.00e-04\n",
      "  Batch 130/902 | Loss: 0.9440 | CLoss: 0.7657 | FLoss: 0.3565 | LR: 3.00e-04\n",
      "  Batch 140/902 | Loss: 0.9917 | CLoss: 0.7195 | FLoss: 0.5444 | LR: 3.00e-04\n",
      "  Batch 150/902 | Loss: 0.8752 | CLoss: 0.6916 | FLoss: 0.3672 | LR: 3.00e-04\n",
      "  Batch 160/902 | Loss: 1.0174 | CLoss: 0.7557 | FLoss: 0.5234 | LR: 3.00e-04\n",
      "  Batch 170/902 | Loss: 0.6198 | CLoss: 0.4512 | FLoss: 0.3370 | LR: 3.00e-04\n",
      "  Batch 180/902 | Loss: 1.0671 | CLoss: 0.8522 | FLoss: 0.4298 | LR: 3.00e-04\n",
      "  Batch 190/902 | Loss: 1.2525 | CLoss: 0.9356 | FLoss: 0.6339 | LR: 3.00e-04\n",
      "  Batch 200/902 | Loss: 0.7696 | CLoss: 0.6006 | FLoss: 0.3381 | LR: 3.00e-04\n",
      "  Batch 210/902 | Loss: 0.6735 | CLoss: 0.4693 | FLoss: 0.4084 | LR: 3.00e-04\n",
      "  Batch 220/902 | Loss: 1.0738 | CLoss: 0.8328 | FLoss: 0.4820 | LR: 3.00e-04\n",
      "  Batch 230/902 | Loss: 1.1567 | CLoss: 0.9100 | FLoss: 0.4935 | LR: 3.00e-04\n",
      "  Batch 240/902 | Loss: 0.7761 | CLoss: 0.6072 | FLoss: 0.3378 | LR: 3.00e-04\n",
      "  Batch 250/902 | Loss: 0.8773 | CLoss: 0.6898 | FLoss: 0.3750 | LR: 3.00e-04\n",
      "  Batch 260/902 | Loss: 1.3785 | CLoss: 1.1582 | FLoss: 0.4406 | LR: 3.00e-04\n",
      "  Batch 270/902 | Loss: 0.7352 | CLoss: 0.5229 | FLoss: 0.4246 | LR: 3.00e-04\n",
      "  Batch 280/902 | Loss: 1.1548 | CLoss: 0.9122 | FLoss: 0.4851 | LR: 3.00e-04\n",
      "  Batch 290/902 | Loss: 0.8882 | CLoss: 0.6877 | FLoss: 0.4010 | LR: 3.00e-04\n",
      "  Batch 300/902 | Loss: 0.9465 | CLoss: 0.7361 | FLoss: 0.4209 | LR: 3.00e-04\n",
      "  Batch 310/902 | Loss: 0.9382 | CLoss: 0.7465 | FLoss: 0.3834 | LR: 3.00e-04\n",
      "  Batch 320/902 | Loss: 1.3904 | CLoss: 1.1223 | FLoss: 0.5363 | LR: 3.00e-04\n",
      "  Batch 330/902 | Loss: 0.7528 | CLoss: 0.5694 | FLoss: 0.3667 | LR: 3.00e-04\n",
      "  Batch 340/902 | Loss: 0.8107 | CLoss: 0.5102 | FLoss: 0.6010 | LR: 3.00e-04\n",
      "  Batch 350/902 | Loss: 0.9804 | CLoss: 0.7565 | FLoss: 0.4478 | LR: 3.00e-04\n",
      "  Batch 360/902 | Loss: 0.5928 | CLoss: 0.4383 | FLoss: 0.3089 | LR: 3.00e-04\n",
      "  Batch 370/902 | Loss: 0.7034 | CLoss: 0.5782 | FLoss: 0.2504 | LR: 3.00e-04\n",
      "  Batch 380/902 | Loss: 0.9844 | CLoss: 0.8227 | FLoss: 0.3235 | LR: 3.00e-04\n",
      "  Batch 390/902 | Loss: 0.6321 | CLoss: 0.4930 | FLoss: 0.2783 | LR: 3.00e-04\n",
      "  Batch 400/902 | Loss: 0.9588 | CLoss: 0.7534 | FLoss: 0.4106 | LR: 3.00e-04\n",
      "  Batch 410/902 | Loss: 1.2376 | CLoss: 1.0852 | FLoss: 0.3046 | LR: 3.00e-04\n",
      "  Batch 420/902 | Loss: 0.5620 | CLoss: 0.4306 | FLoss: 0.2627 | LR: 3.00e-04\n",
      "  Batch 430/902 | Loss: 0.9333 | CLoss: 0.7198 | FLoss: 0.4271 | LR: 3.00e-04\n",
      "  Batch 440/902 | Loss: 1.0160 | CLoss: 0.8314 | FLoss: 0.3690 | LR: 3.00e-04\n",
      "  Batch 450/902 | Loss: 0.6543 | CLoss: 0.5195 | FLoss: 0.2695 | LR: 3.00e-04\n",
      "  Batch 460/902 | Loss: 0.9252 | CLoss: 0.6242 | FLoss: 0.6020 | LR: 3.00e-04\n",
      "  Batch 470/902 | Loss: 0.9345 | CLoss: 0.7525 | FLoss: 0.3641 | LR: 3.00e-04\n",
      "  Batch 480/902 | Loss: 0.5728 | CLoss: 0.4304 | FLoss: 0.2847 | LR: 3.00e-04\n",
      "  Batch 490/902 | Loss: 0.4761 | CLoss: 0.3754 | FLoss: 0.2015 | LR: 3.00e-04\n",
      "  Batch 500/902 | Loss: 0.4096 | CLoss: 0.3167 | FLoss: 0.1860 | LR: 3.00e-04\n",
      "  Batch 510/902 | Loss: 0.7140 | CLoss: 0.5735 | FLoss: 0.2809 | LR: 3.00e-04\n",
      "  Batch 520/902 | Loss: 1.3158 | CLoss: 1.0580 | FLoss: 0.5156 | LR: 3.00e-04\n",
      "  Batch 530/902 | Loss: 0.6491 | CLoss: 0.5099 | FLoss: 0.2783 | LR: 3.00e-04\n",
      "  Batch 540/902 | Loss: 0.8143 | CLoss: 0.6854 | FLoss: 0.2577 | LR: 3.00e-04\n",
      "  Batch 550/902 | Loss: 0.6109 | CLoss: 0.4295 | FLoss: 0.3629 | LR: 3.00e-04\n",
      "  Batch 560/902 | Loss: 0.6583 | CLoss: 0.5805 | FLoss: 0.1556 | LR: 3.00e-04\n",
      "  Batch 570/902 | Loss: 0.5224 | CLoss: 0.4135 | FLoss: 0.2178 | LR: 3.00e-04\n",
      "  Batch 580/902 | Loss: 0.7878 | CLoss: 0.5515 | FLoss: 0.4727 | LR: 3.00e-04\n",
      "  Batch 590/902 | Loss: 0.8735 | CLoss: 0.7243 | FLoss: 0.2985 | LR: 3.00e-04\n",
      "  Batch 600/902 | Loss: 1.1375 | CLoss: 0.8432 | FLoss: 0.5886 | LR: 3.00e-04\n",
      "  Batch 610/902 | Loss: 1.1168 | CLoss: 0.9088 | FLoss: 0.4161 | LR: 3.00e-04\n",
      "  Batch 620/902 | Loss: 1.1933 | CLoss: 0.9082 | FLoss: 0.5703 | LR: 3.00e-04\n",
      "  Batch 630/902 | Loss: 0.4808 | CLoss: 0.3114 | FLoss: 0.3388 | LR: 3.00e-04\n",
      "  Batch 640/902 | Loss: 1.0621 | CLoss: 0.8181 | FLoss: 0.4880 | LR: 3.00e-04\n",
      "  Batch 650/902 | Loss: 0.7217 | CLoss: 0.5671 | FLoss: 0.3092 | LR: 3.00e-04\n",
      "  Batch 660/902 | Loss: 1.2020 | CLoss: 0.9123 | FLoss: 0.5794 | LR: 3.00e-04\n",
      "  Batch 670/902 | Loss: 0.9763 | CLoss: 0.7476 | FLoss: 0.4574 | LR: 3.00e-04\n",
      "  Batch 680/902 | Loss: 0.3998 | CLoss: 0.2507 | FLoss: 0.2982 | LR: 3.00e-04\n",
      "  Batch 690/902 | Loss: 1.0827 | CLoss: 0.8538 | FLoss: 0.4578 | LR: 3.00e-04\n",
      "  Batch 700/902 | Loss: 0.6272 | CLoss: 0.4791 | FLoss: 0.2963 | LR: 3.00e-04\n",
      "  Batch 710/902 | Loss: 0.9245 | CLoss: 0.7472 | FLoss: 0.3546 | LR: 3.00e-04\n",
      "  Batch 720/902 | Loss: 1.0152 | CLoss: 0.8108 | FLoss: 0.4089 | LR: 3.00e-04\n",
      "  Batch 730/902 | Loss: 0.6606 | CLoss: 0.4679 | FLoss: 0.3853 | LR: 3.00e-04\n",
      "  Batch 740/902 | Loss: 0.6261 | CLoss: 0.4800 | FLoss: 0.2922 | LR: 3.00e-04\n",
      "  Batch 750/902 | Loss: 0.8174 | CLoss: 0.6611 | FLoss: 0.3126 | LR: 3.00e-04\n",
      "  Batch 760/902 | Loss: 1.4515 | CLoss: 1.0758 | FLoss: 0.7514 | LR: 3.00e-04\n",
      "  Batch 770/902 | Loss: 0.7913 | CLoss: 0.6080 | FLoss: 0.3666 | LR: 3.00e-04\n",
      "  Batch 780/902 | Loss: 0.7283 | CLoss: 0.5270 | FLoss: 0.4025 | LR: 3.00e-04\n",
      "  Batch 790/902 | Loss: 0.4746 | CLoss: 0.3192 | FLoss: 0.3108 | LR: 3.00e-04\n",
      "  Batch 800/902 | Loss: 1.2612 | CLoss: 1.0794 | FLoss: 0.3637 | LR: 3.00e-04\n",
      "  Batch 810/902 | Loss: 0.6153 | CLoss: 0.4161 | FLoss: 0.3984 | LR: 3.00e-04\n",
      "  Batch 820/902 | Loss: 1.0300 | CLoss: 0.7918 | FLoss: 0.4763 | LR: 3.00e-04\n",
      "  Batch 830/902 | Loss: 0.5547 | CLoss: 0.3795 | FLoss: 0.3504 | LR: 3.00e-04\n",
      "  Batch 840/902 | Loss: 0.7668 | CLoss: 0.5462 | FLoss: 0.4414 | LR: 3.00e-04\n",
      "  Batch 850/902 | Loss: 0.6072 | CLoss: 0.4320 | FLoss: 0.3504 | LR: 3.00e-04\n",
      "  Batch 860/902 | Loss: 0.7850 | CLoss: 0.5789 | FLoss: 0.4122 | LR: 3.00e-04\n",
      "  Batch 870/902 | Loss: 0.7368 | CLoss: 0.5193 | FLoss: 0.4350 | LR: 3.00e-04\n",
      "  Batch 880/902 | Loss: 0.7986 | CLoss: 0.6749 | FLoss: 0.2473 | LR: 3.00e-04\n",
      "  Batch 890/902 | Loss: 0.6428 | CLoss: 0.4839 | FLoss: 0.3178 | LR: 3.00e-04\n",
      "  Batch 900/902 | Loss: 0.4165 | CLoss: 0.2737 | FLoss: 0.2855 | LR: 3.00e-04\n",
      "  Batch 902/902 | Loss: 1.2261 | CLoss: 0.0000 | FLoss: 2.4521 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 0.9213\n",
      "  Last Batch Loss: 1.2261\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/98 | Loss: 0.6926 | Batch Acc: 74.14%\n",
      "    Val Batch 010/98 | Loss: 0.9529 | Batch Acc: 70.69%\n",
      "    Val Batch 015/98 | Loss: 0.8598 | Batch Acc: 77.59%\n",
      "    Val Batch 020/98 | Loss: 1.2102 | Batch Acc: 22.41%\n",
      "    Val Batch 025/98 | Loss: 0.9585 | Batch Acc: 72.41%\n",
      "    Val Batch 030/98 | Loss: 0.9495 | Batch Acc: 79.31%\n",
      "    Val Batch 035/98 | Loss: 0.6720 | Batch Acc: 79.31%\n",
      "    Val Batch 040/98 | Loss: 0.4641 | Batch Acc: 98.28%\n",
      "    Val Batch 045/98 | Loss: 1.0735 | Batch Acc: 74.14%\n",
      "    Val Batch 050/98 | Loss: 1.3706 | Batch Acc: 65.52%\n",
      "    Val Batch 055/98 | Loss: 0.6591 | Batch Acc: 77.59%\n",
      "    Val Batch 060/98 | Loss: 0.2975 | Batch Acc: 87.93%\n",
      "    Val Batch 065/98 | Loss: 0.4075 | Batch Acc: 82.76%\n",
      "    Val Batch 070/98 | Loss: 0.1556 | Batch Acc: 96.55%\n",
      "    Val Batch 075/98 | Loss: 0.2330 | Batch Acc: 91.38%\n",
      "    Val Batch 080/98 | Loss: 0.5646 | Batch Acc: 79.31%\n",
      "    Val Batch 085/98 | Loss: 0.1361 | Batch Acc: 98.28%\n",
      "    Val Batch 090/98 | Loss: 0.1563 | Batch Acc: 93.10%\n",
      "    Val Batch 095/98 | Loss: 0.1096 | Batch Acc: 96.55%\n",
      "    Val Batch 098/98 | Loss: 0.0250 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.6795 | Accuracy: 77.19%\n",
      "  Current Best Acc: 77.19%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/902 | Loss: 1.5175 | CLoss: 1.1982 | FLoss: 0.6386 | LR: 2.71e-04\n",
      "  Batch 020/902 | Loss: 1.1855 | CLoss: 0.9249 | FLoss: 0.5212 | LR: 2.71e-04\n",
      "  Batch 030/902 | Loss: 1.0211 | CLoss: 0.8283 | FLoss: 0.3857 | LR: 2.71e-04\n",
      "  Batch 040/902 | Loss: 0.5834 | CLoss: 0.4392 | FLoss: 0.2884 | LR: 2.71e-04\n",
      "  Batch 050/902 | Loss: 0.7519 | CLoss: 0.5510 | FLoss: 0.4018 | LR: 2.71e-04\n",
      "  Batch 060/902 | Loss: 0.4964 | CLoss: 0.4037 | FLoss: 0.1853 | LR: 2.71e-04\n",
      "  Batch 070/902 | Loss: 0.9213 | CLoss: 0.7529 | FLoss: 0.3367 | LR: 2.71e-04\n",
      "  Batch 080/902 | Loss: 0.6115 | CLoss: 0.5057 | FLoss: 0.2116 | LR: 2.71e-04\n",
      "  Batch 090/902 | Loss: 0.6163 | CLoss: 0.4653 | FLoss: 0.3020 | LR: 2.71e-04\n",
      "  Batch 100/902 | Loss: 0.6989 | CLoss: 0.5781 | FLoss: 0.2416 | LR: 2.71e-04\n",
      "  Batch 110/902 | Loss: 0.9027 | CLoss: 0.7207 | FLoss: 0.3641 | LR: 2.71e-04\n",
      "  Batch 120/902 | Loss: 0.8347 | CLoss: 0.6698 | FLoss: 0.3298 | LR: 2.71e-04\n",
      "  Batch 130/902 | Loss: 0.5806 | CLoss: 0.4030 | FLoss: 0.3553 | LR: 2.71e-04\n",
      "  Batch 140/902 | Loss: 0.6391 | CLoss: 0.4838 | FLoss: 0.3106 | LR: 2.71e-04\n",
      "  Batch 150/902 | Loss: 0.4575 | CLoss: 0.3303 | FLoss: 0.2545 | LR: 2.71e-04\n",
      "  Batch 160/902 | Loss: 0.6118 | CLoss: 0.5033 | FLoss: 0.2169 | LR: 2.71e-04\n",
      "  Batch 170/902 | Loss: 0.8065 | CLoss: 0.6240 | FLoss: 0.3651 | LR: 2.71e-04\n",
      "  Batch 180/902 | Loss: 0.7287 | CLoss: 0.6083 | FLoss: 0.2408 | LR: 2.71e-04\n",
      "  Batch 190/902 | Loss: 0.5036 | CLoss: 0.3821 | FLoss: 0.2429 | LR: 2.71e-04\n",
      "  Batch 200/902 | Loss: 0.3995 | CLoss: 0.2349 | FLoss: 0.3293 | LR: 2.71e-04\n",
      "  Batch 210/902 | Loss: 1.2456 | CLoss: 0.9834 | FLoss: 0.5244 | LR: 2.71e-04\n",
      "  Batch 220/902 | Loss: 0.6936 | CLoss: 0.5364 | FLoss: 0.3143 | LR: 2.71e-04\n",
      "  Batch 230/902 | Loss: 0.7312 | CLoss: 0.5820 | FLoss: 0.2983 | LR: 2.71e-04\n",
      "  Batch 240/902 | Loss: 1.1975 | CLoss: 0.9382 | FLoss: 0.5185 | LR: 2.71e-04\n",
      "  Batch 250/902 | Loss: 0.5138 | CLoss: 0.3841 | FLoss: 0.2594 | LR: 2.71e-04\n",
      "  Batch 260/902 | Loss: 0.9942 | CLoss: 0.7714 | FLoss: 0.4457 | LR: 2.71e-04\n",
      "  Batch 270/902 | Loss: 0.7820 | CLoss: 0.5457 | FLoss: 0.4726 | LR: 2.71e-04\n",
      "  Batch 280/902 | Loss: 1.0682 | CLoss: 0.9028 | FLoss: 0.3308 | LR: 2.71e-04\n",
      "  Batch 290/902 | Loss: 0.6633 | CLoss: 0.4256 | FLoss: 0.4753 | LR: 2.71e-04\n",
      "  Batch 300/902 | Loss: 0.8200 | CLoss: 0.6886 | FLoss: 0.2628 | LR: 2.71e-04\n",
      "  Batch 310/902 | Loss: 0.8548 | CLoss: 0.6822 | FLoss: 0.3452 | LR: 2.71e-04\n",
      "  Batch 320/902 | Loss: 1.0056 | CLoss: 0.7125 | FLoss: 0.5861 | LR: 2.71e-04\n",
      "  Batch 330/902 | Loss: 0.9038 | CLoss: 0.7456 | FLoss: 0.3165 | LR: 2.71e-04\n",
      "  Batch 340/902 | Loss: 0.8613 | CLoss: 0.6707 | FLoss: 0.3813 | LR: 2.71e-04\n",
      "  Batch 350/902 | Loss: 0.3037 | CLoss: 0.2178 | FLoss: 0.1718 | LR: 2.71e-04\n",
      "  Batch 360/902 | Loss: 1.1945 | CLoss: 0.8477 | FLoss: 0.6936 | LR: 2.71e-04\n",
      "  Batch 370/902 | Loss: 0.7927 | CLoss: 0.6402 | FLoss: 0.3051 | LR: 2.71e-04\n",
      "  Batch 380/902 | Loss: 0.6642 | CLoss: 0.5028 | FLoss: 0.3228 | LR: 2.71e-04\n",
      "  Batch 390/902 | Loss: 0.6705 | CLoss: 0.4890 | FLoss: 0.3631 | LR: 2.71e-04\n",
      "  Batch 400/902 | Loss: 1.0148 | CLoss: 0.7442 | FLoss: 0.5412 | LR: 2.71e-04\n",
      "  Batch 410/902 | Loss: 1.0034 | CLoss: 0.8378 | FLoss: 0.3311 | LR: 2.71e-04\n",
      "  Batch 420/902 | Loss: 0.6670 | CLoss: 0.4091 | FLoss: 0.5159 | LR: 2.71e-04\n",
      "  Batch 430/902 | Loss: 0.8434 | CLoss: 0.6892 | FLoss: 0.3084 | LR: 2.71e-04\n",
      "  Batch 440/902 | Loss: 0.9827 | CLoss: 0.7656 | FLoss: 0.4342 | LR: 2.71e-04\n",
      "  Batch 450/902 | Loss: 0.7283 | CLoss: 0.5485 | FLoss: 0.3596 | LR: 2.71e-04\n",
      "  Batch 460/902 | Loss: 0.8958 | CLoss: 0.6554 | FLoss: 0.4807 | LR: 2.71e-04\n",
      "  Batch 470/902 | Loss: 0.9823 | CLoss: 0.7730 | FLoss: 0.4186 | LR: 2.71e-04\n",
      "  Batch 480/902 | Loss: 0.4850 | CLoss: 0.3322 | FLoss: 0.3056 | LR: 2.71e-04\n",
      "  Batch 490/902 | Loss: 0.4685 | CLoss: 0.3219 | FLoss: 0.2931 | LR: 2.71e-04\n",
      "  Batch 500/902 | Loss: 0.9620 | CLoss: 0.7443 | FLoss: 0.4353 | LR: 2.71e-04\n",
      "  Batch 510/902 | Loss: 0.7798 | CLoss: 0.6067 | FLoss: 0.3462 | LR: 2.71e-04\n",
      "  Batch 520/902 | Loss: 0.8456 | CLoss: 0.7387 | FLoss: 0.2137 | LR: 2.71e-04\n",
      "  Batch 530/902 | Loss: 0.8562 | CLoss: 0.7312 | FLoss: 0.2500 | LR: 2.71e-04\n",
      "  Batch 540/902 | Loss: 0.9508 | CLoss: 0.7959 | FLoss: 0.3100 | LR: 2.71e-04\n",
      "  Batch 550/902 | Loss: 0.8040 | CLoss: 0.7057 | FLoss: 0.1966 | LR: 2.71e-04\n",
      "  Batch 560/902 | Loss: 0.6908 | CLoss: 0.5055 | FLoss: 0.3706 | LR: 2.71e-04\n",
      "  Batch 570/902 | Loss: 0.4274 | CLoss: 0.3069 | FLoss: 0.2410 | LR: 2.71e-04\n",
      "  Batch 580/902 | Loss: 0.9363 | CLoss: 0.7488 | FLoss: 0.3750 | LR: 2.71e-04\n",
      "  Batch 590/902 | Loss: 0.7214 | CLoss: 0.6025 | FLoss: 0.2377 | LR: 2.71e-04\n",
      "  Batch 600/902 | Loss: 1.0544 | CLoss: 0.7831 | FLoss: 0.5426 | LR: 2.71e-04\n",
      "  Batch 610/902 | Loss: 1.0073 | CLoss: 0.7926 | FLoss: 0.4293 | LR: 2.71e-04\n",
      "  Batch 620/902 | Loss: 0.7355 | CLoss: 0.5642 | FLoss: 0.3426 | LR: 2.71e-04\n",
      "  Batch 630/902 | Loss: 0.5564 | CLoss: 0.4469 | FLoss: 0.2189 | LR: 2.71e-04\n",
      "  Batch 640/902 | Loss: 0.9720 | CLoss: 0.7571 | FLoss: 0.4297 | LR: 2.71e-04\n",
      "  Batch 650/902 | Loss: 0.9725 | CLoss: 0.7375 | FLoss: 0.4701 | LR: 2.71e-04\n",
      "  Batch 660/902 | Loss: 0.5822 | CLoss: 0.3567 | FLoss: 0.4510 | LR: 2.71e-04\n",
      "  Batch 670/902 | Loss: 0.8013 | CLoss: 0.6931 | FLoss: 0.2164 | LR: 2.71e-04\n",
      "  Batch 680/902 | Loss: 0.7954 | CLoss: 0.6621 | FLoss: 0.2666 | LR: 2.71e-04\n",
      "  Batch 690/902 | Loss: 0.5786 | CLoss: 0.5076 | FLoss: 0.1421 | LR: 2.71e-04\n",
      "  Batch 700/902 | Loss: 0.5130 | CLoss: 0.4180 | FLoss: 0.1900 | LR: 2.71e-04\n",
      "  Batch 710/902 | Loss: 0.5522 | CLoss: 0.3769 | FLoss: 0.3507 | LR: 2.71e-04\n",
      "  Batch 720/902 | Loss: 1.0028 | CLoss: 0.6950 | FLoss: 0.6156 | LR: 2.71e-04\n",
      "  Batch 730/902 | Loss: 0.9253 | CLoss: 0.6843 | FLoss: 0.4821 | LR: 2.71e-04\n",
      "  Batch 740/902 | Loss: 1.5370 | CLoss: 1.1974 | FLoss: 0.6792 | LR: 2.71e-04\n",
      "  Batch 750/902 | Loss: 0.6635 | CLoss: 0.5580 | FLoss: 0.2109 | LR: 2.71e-04\n",
      "  Batch 760/902 | Loss: 0.6361 | CLoss: 0.4992 | FLoss: 0.2739 | LR: 2.71e-04\n",
      "  Batch 770/902 | Loss: 0.5855 | CLoss: 0.4228 | FLoss: 0.3253 | LR: 2.71e-04\n",
      "  Batch 780/902 | Loss: 0.4404 | CLoss: 0.3180 | FLoss: 0.2447 | LR: 2.71e-04\n",
      "  Batch 790/902 | Loss: 0.6874 | CLoss: 0.5318 | FLoss: 0.3112 | LR: 2.71e-04\n",
      "  Batch 800/902 | Loss: 0.6566 | CLoss: 0.5088 | FLoss: 0.2955 | LR: 2.71e-04\n",
      "  Batch 810/902 | Loss: 0.5477 | CLoss: 0.4189 | FLoss: 0.2576 | LR: 2.71e-04\n",
      "  Batch 820/902 | Loss: 0.4961 | CLoss: 0.2953 | FLoss: 0.4015 | LR: 2.71e-04\n",
      "  Batch 830/902 | Loss: 0.5576 | CLoss: 0.4615 | FLoss: 0.1922 | LR: 2.71e-04\n",
      "  Batch 840/902 | Loss: 0.8719 | CLoss: 0.6886 | FLoss: 0.3666 | LR: 2.71e-04\n",
      "  Batch 850/902 | Loss: 0.8567 | CLoss: 0.6399 | FLoss: 0.4336 | LR: 2.71e-04\n",
      "  Batch 860/902 | Loss: 1.2387 | CLoss: 1.0496 | FLoss: 0.3781 | LR: 2.71e-04\n",
      "  Batch 870/902 | Loss: 0.4221 | CLoss: 0.3100 | FLoss: 0.2243 | LR: 2.71e-04\n",
      "  Batch 880/902 | Loss: 0.8724 | CLoss: 0.6680 | FLoss: 0.4088 | LR: 2.71e-04\n",
      "  Batch 890/902 | Loss: 1.1402 | CLoss: 0.8976 | FLoss: 0.4852 | LR: 2.71e-04\n",
      "  Batch 900/902 | Loss: 0.6425 | CLoss: 0.4327 | FLoss: 0.4197 | LR: 2.71e-04\n",
      "  Batch 902/902 | Loss: 1.1114 | CLoss: 0.0000 | FLoss: 2.2229 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.7977\n",
      "  Last Batch Loss: 1.1114\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/98 | Loss: 0.6690 | Batch Acc: 74.14%\n",
      "    Val Batch 010/98 | Loss: 1.2004 | Batch Acc: 67.24%\n",
      "    Val Batch 015/98 | Loss: 1.3402 | Batch Acc: 68.97%\n",
      "    Val Batch 020/98 | Loss: 1.3168 | Batch Acc: 17.24%\n",
      "    Val Batch 025/98 | Loss: 0.4530 | Batch Acc: 89.66%\n",
      "    Val Batch 030/98 | Loss: 0.5474 | Batch Acc: 82.76%\n",
      "    Val Batch 035/98 | Loss: 0.5166 | Batch Acc: 79.31%\n",
      "    Val Batch 040/98 | Loss: 0.5390 | Batch Acc: 98.28%\n",
      "    Val Batch 045/98 | Loss: 1.1412 | Batch Acc: 70.69%\n",
      "    Val Batch 050/98 | Loss: 1.2278 | Batch Acc: 63.79%\n",
      "    Val Batch 055/98 | Loss: 0.5100 | Batch Acc: 82.76%\n",
      "    Val Batch 060/98 | Loss: 0.5352 | Batch Acc: 84.48%\n",
      "    Val Batch 065/98 | Loss: 0.9200 | Batch Acc: 84.48%\n",
      "    Val Batch 070/98 | Loss: 0.1849 | Batch Acc: 94.83%\n",
      "    Val Batch 075/98 | Loss: 0.2494 | Batch Acc: 93.10%\n",
      "    Val Batch 080/98 | Loss: 0.1435 | Batch Acc: 96.55%\n",
      "    Val Batch 085/98 | Loss: 0.0874 | Batch Acc: 96.55%\n",
      "    Val Batch 090/98 | Loss: 0.2239 | Batch Acc: 91.38%\n",
      "    Val Batch 095/98 | Loss: 0.2854 | Batch Acc: 91.38%\n",
      "    Val Batch 098/98 | Loss: 0.4458 | Batch Acc: 84.85%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.6125 | Accuracy: 79.36%\n",
      "  Current Best Acc: 79.36%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/902 | Loss: 0.5750 | CLoss: 0.4199 | FLoss: 0.3101 | LR: 1.96e-04\n",
      "  Batch 020/902 | Loss: 0.7212 | CLoss: 0.4579 | FLoss: 0.5265 | LR: 1.96e-04\n",
      "  Batch 030/902 | Loss: 0.6941 | CLoss: 0.5383 | FLoss: 0.3115 | LR: 1.96e-04\n",
      "  Batch 040/902 | Loss: 0.6942 | CLoss: 0.5223 | FLoss: 0.3438 | LR: 1.96e-04\n",
      "  Batch 050/902 | Loss: 0.7918 | CLoss: 0.6594 | FLoss: 0.2647 | LR: 1.96e-04\n",
      "  Batch 060/902 | Loss: 0.9671 | CLoss: 0.7691 | FLoss: 0.3959 | LR: 1.96e-04\n",
      "  Batch 070/902 | Loss: 0.7508 | CLoss: 0.5320 | FLoss: 0.4376 | LR: 1.96e-04\n",
      "  Batch 080/902 | Loss: 0.6561 | CLoss: 0.4954 | FLoss: 0.3214 | LR: 1.96e-04\n",
      "  Batch 090/902 | Loss: 0.6072 | CLoss: 0.4412 | FLoss: 0.3321 | LR: 1.96e-04\n",
      "  Batch 100/902 | Loss: 0.8083 | CLoss: 0.6384 | FLoss: 0.3398 | LR: 1.96e-04\n",
      "  Batch 110/902 | Loss: 1.0756 | CLoss: 0.8436 | FLoss: 0.4638 | LR: 1.96e-04\n",
      "  Batch 120/902 | Loss: 0.3428 | CLoss: 0.2543 | FLoss: 0.1771 | LR: 1.96e-04\n",
      "  Batch 130/902 | Loss: 0.8819 | CLoss: 0.6468 | FLoss: 0.4703 | LR: 1.96e-04\n",
      "  Batch 140/902 | Loss: 0.5496 | CLoss: 0.4021 | FLoss: 0.2951 | LR: 1.96e-04\n",
      "  Batch 150/902 | Loss: 1.0449 | CLoss: 0.7344 | FLoss: 0.6210 | LR: 1.96e-04\n",
      "  Batch 160/902 | Loss: 0.8845 | CLoss: 0.7521 | FLoss: 0.2648 | LR: 1.96e-04\n",
      "  Batch 170/902 | Loss: 1.1219 | CLoss: 0.8719 | FLoss: 0.4999 | LR: 1.96e-04\n",
      "  Batch 180/902 | Loss: 0.6844 | CLoss: 0.5535 | FLoss: 0.2618 | LR: 1.96e-04\n",
      "  Batch 190/902 | Loss: 0.7123 | CLoss: 0.6259 | FLoss: 0.1730 | LR: 1.96e-04\n",
      "  Batch 200/902 | Loss: 0.5700 | CLoss: 0.4781 | FLoss: 0.1837 | LR: 1.96e-04\n",
      "  Batch 210/902 | Loss: 1.2166 | CLoss: 1.0150 | FLoss: 0.4032 | LR: 1.96e-04\n",
      "  Batch 220/902 | Loss: 0.5447 | CLoss: 0.4201 | FLoss: 0.2490 | LR: 1.96e-04\n",
      "  Batch 230/902 | Loss: 0.4331 | CLoss: 0.3088 | FLoss: 0.2484 | LR: 1.96e-04\n",
      "  Batch 240/902 | Loss: 1.0390 | CLoss: 0.8771 | FLoss: 0.3237 | LR: 1.96e-04\n",
      "  Batch 250/902 | Loss: 0.8547 | CLoss: 0.6141 | FLoss: 0.4812 | LR: 1.96e-04\n",
      "  Batch 260/902 | Loss: 0.6619 | CLoss: 0.4986 | FLoss: 0.3265 | LR: 1.96e-04\n",
      "  Batch 270/902 | Loss: 0.6183 | CLoss: 0.4742 | FLoss: 0.2882 | LR: 1.96e-04\n",
      "  Batch 280/902 | Loss: 0.9053 | CLoss: 0.7564 | FLoss: 0.2979 | LR: 1.96e-04\n",
      "  Batch 290/902 | Loss: 0.7061 | CLoss: 0.5212 | FLoss: 0.3697 | LR: 1.96e-04\n",
      "  Batch 300/902 | Loss: 0.5266 | CLoss: 0.4096 | FLoss: 0.2341 | LR: 1.96e-04\n",
      "  Batch 310/902 | Loss: 0.8825 | CLoss: 0.6956 | FLoss: 0.3737 | LR: 1.96e-04\n",
      "  Batch 320/902 | Loss: 0.8892 | CLoss: 0.6794 | FLoss: 0.4195 | LR: 1.96e-04\n",
      "  Batch 330/902 | Loss: 0.6729 | CLoss: 0.5298 | FLoss: 0.2862 | LR: 1.96e-04\n",
      "  Batch 340/902 | Loss: 0.6812 | CLoss: 0.5752 | FLoss: 0.2119 | LR: 1.96e-04\n",
      "  Batch 350/902 | Loss: 0.7345 | CLoss: 0.6255 | FLoss: 0.2178 | LR: 1.96e-04\n",
      "  Batch 360/902 | Loss: 0.6128 | CLoss: 0.4332 | FLoss: 0.3591 | LR: 1.96e-04\n",
      "  Batch 370/902 | Loss: 0.4998 | CLoss: 0.3957 | FLoss: 0.2081 | LR: 1.96e-04\n",
      "  Batch 380/902 | Loss: 0.7490 | CLoss: 0.5299 | FLoss: 0.4382 | LR: 1.96e-04\n",
      "  Batch 390/902 | Loss: 0.5657 | CLoss: 0.3905 | FLoss: 0.3505 | LR: 1.96e-04\n",
      "  Batch 400/902 | Loss: 1.0495 | CLoss: 0.8133 | FLoss: 0.4725 | LR: 1.96e-04\n",
      "  Batch 410/902 | Loss: 0.2669 | CLoss: 0.2067 | FLoss: 0.1204 | LR: 1.96e-04\n",
      "  Batch 420/902 | Loss: 0.5968 | CLoss: 0.5350 | FLoss: 0.1235 | LR: 1.96e-04\n",
      "  Batch 430/902 | Loss: 0.7893 | CLoss: 0.6078 | FLoss: 0.3630 | LR: 1.96e-04\n",
      "  Batch 440/902 | Loss: 0.9867 | CLoss: 0.8014 | FLoss: 0.3707 | LR: 1.96e-04\n",
      "  Batch 450/902 | Loss: 0.7005 | CLoss: 0.5788 | FLoss: 0.2434 | LR: 1.96e-04\n",
      "  Batch 460/902 | Loss: 0.3783 | CLoss: 0.2088 | FLoss: 0.3390 | LR: 1.96e-04\n",
      "  Batch 470/902 | Loss: 0.2376 | CLoss: 0.1455 | FLoss: 0.1843 | LR: 1.96e-04\n",
      "  Batch 480/902 | Loss: 1.2705 | CLoss: 1.0062 | FLoss: 0.5287 | LR: 1.96e-04\n",
      "  Batch 490/902 | Loss: 0.5454 | CLoss: 0.4539 | FLoss: 0.1830 | LR: 1.96e-04\n",
      "  Batch 500/902 | Loss: 0.4237 | CLoss: 0.2976 | FLoss: 0.2522 | LR: 1.96e-04\n",
      "  Batch 510/902 | Loss: 0.5422 | CLoss: 0.4199 | FLoss: 0.2446 | LR: 1.96e-04\n",
      "  Batch 520/902 | Loss: 0.8037 | CLoss: 0.6895 | FLoss: 0.2284 | LR: 1.96e-04\n",
      "  Batch 530/902 | Loss: 0.7284 | CLoss: 0.5715 | FLoss: 0.3137 | LR: 1.96e-04\n",
      "  Batch 540/902 | Loss: 0.8358 | CLoss: 0.7129 | FLoss: 0.2458 | LR: 1.96e-04\n",
      "  Batch 550/902 | Loss: 1.0816 | CLoss: 0.8701 | FLoss: 0.4230 | LR: 1.96e-04\n",
      "  Batch 560/902 | Loss: 0.9942 | CLoss: 0.8681 | FLoss: 0.2520 | LR: 1.96e-04\n",
      "  Batch 570/902 | Loss: 1.0823 | CLoss: 0.8180 | FLoss: 0.5286 | LR: 1.96e-04\n",
      "  Batch 580/902 | Loss: 0.8327 | CLoss: 0.6876 | FLoss: 0.2901 | LR: 1.96e-04\n",
      "  Batch 590/902 | Loss: 0.4936 | CLoss: 0.4512 | FLoss: 0.0849 | LR: 1.96e-04\n",
      "  Batch 600/902 | Loss: 0.7771 | CLoss: 0.6482 | FLoss: 0.2577 | LR: 1.96e-04\n",
      "  Batch 610/902 | Loss: 0.6470 | CLoss: 0.4804 | FLoss: 0.3333 | LR: 1.96e-04\n",
      "  Batch 620/902 | Loss: 0.8151 | CLoss: 0.6475 | FLoss: 0.3352 | LR: 1.96e-04\n",
      "  Batch 630/902 | Loss: 0.6415 | CLoss: 0.4665 | FLoss: 0.3500 | LR: 1.96e-04\n",
      "  Batch 640/902 | Loss: 1.3122 | CLoss: 1.0528 | FLoss: 0.5189 | LR: 1.96e-04\n",
      "  Batch 650/902 | Loss: 0.4800 | CLoss: 0.3374 | FLoss: 0.2853 | LR: 1.96e-04\n",
      "  Batch 660/902 | Loss: 0.6882 | CLoss: 0.5263 | FLoss: 0.3238 | LR: 1.96e-04\n",
      "  Batch 670/902 | Loss: 0.8361 | CLoss: 0.6058 | FLoss: 0.4605 | LR: 1.96e-04\n",
      "  Batch 680/902 | Loss: 0.5485 | CLoss: 0.4566 | FLoss: 0.1838 | LR: 1.96e-04\n",
      "  Batch 690/902 | Loss: 0.6256 | CLoss: 0.3639 | FLoss: 0.5234 | LR: 1.96e-04\n",
      "  Batch 700/902 | Loss: 0.8808 | CLoss: 0.7349 | FLoss: 0.2918 | LR: 1.96e-04\n",
      "  Batch 710/902 | Loss: 0.4172 | CLoss: 0.2720 | FLoss: 0.2904 | LR: 1.96e-04\n",
      "  Batch 720/902 | Loss: 0.4339 | CLoss: 0.3101 | FLoss: 0.2476 | LR: 1.96e-04\n",
      "  Batch 730/902 | Loss: 0.4816 | CLoss: 0.3223 | FLoss: 0.3185 | LR: 1.96e-04\n",
      "  Batch 740/902 | Loss: 0.6246 | CLoss: 0.5084 | FLoss: 0.2325 | LR: 1.96e-04\n",
      "  Batch 750/902 | Loss: 0.7118 | CLoss: 0.5327 | FLoss: 0.3583 | LR: 1.96e-04\n",
      "  Batch 760/902 | Loss: 0.6041 | CLoss: 0.4892 | FLoss: 0.2297 | LR: 1.96e-04\n",
      "  Batch 770/902 | Loss: 0.4560 | CLoss: 0.3331 | FLoss: 0.2459 | LR: 1.96e-04\n",
      "  Batch 780/902 | Loss: 0.6238 | CLoss: 0.5367 | FLoss: 0.1743 | LR: 1.96e-04\n",
      "  Batch 790/902 | Loss: 0.3162 | CLoss: 0.2140 | FLoss: 0.2044 | LR: 1.96e-04\n",
      "  Batch 800/902 | Loss: 0.9157 | CLoss: 0.7413 | FLoss: 0.3487 | LR: 1.96e-04\n",
      "  Batch 810/902 | Loss: 0.5702 | CLoss: 0.4185 | FLoss: 0.3034 | LR: 1.96e-04\n",
      "  Batch 820/902 | Loss: 1.0621 | CLoss: 0.8622 | FLoss: 0.3997 | LR: 1.96e-04\n",
      "  Batch 830/902 | Loss: 0.4707 | CLoss: 0.3483 | FLoss: 0.2447 | LR: 1.96e-04\n",
      "  Batch 840/902 | Loss: 0.6568 | CLoss: 0.4621 | FLoss: 0.3894 | LR: 1.96e-04\n",
      "  Batch 850/902 | Loss: 0.8684 | CLoss: 0.6559 | FLoss: 0.4251 | LR: 1.96e-04\n",
      "  Batch 860/902 | Loss: 0.6600 | CLoss: 0.5508 | FLoss: 0.2183 | LR: 1.96e-04\n",
      "  Batch 870/902 | Loss: 0.6593 | CLoss: 0.5284 | FLoss: 0.2619 | LR: 1.96e-04\n",
      "  Batch 880/902 | Loss: 0.6638 | CLoss: 0.4538 | FLoss: 0.4200 | LR: 1.96e-04\n",
      "  Batch 890/902 | Loss: 0.5146 | CLoss: 0.3305 | FLoss: 0.3682 | LR: 1.96e-04\n",
      "  Batch 900/902 | Loss: 0.7210 | CLoss: 0.5567 | FLoss: 0.3286 | LR: 1.96e-04\n",
      "  Batch 902/902 | Loss: 1.4488 | CLoss: 0.0000 | FLoss: 2.8977 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.7187\n",
      "  Last Batch Loss: 1.4488\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/98 | Loss: 0.8774 | Batch Acc: 65.52%\n",
      "    Val Batch 010/98 | Loss: 1.1072 | Batch Acc: 72.41%\n",
      "    Val Batch 015/98 | Loss: 1.0532 | Batch Acc: 74.14%\n",
      "    Val Batch 020/98 | Loss: 2.1284 | Batch Acc: 20.69%\n",
      "    Val Batch 025/98 | Loss: 1.1624 | Batch Acc: 65.52%\n",
      "    Val Batch 030/98 | Loss: 0.9302 | Batch Acc: 70.69%\n",
      "    Val Batch 035/98 | Loss: 0.6411 | Batch Acc: 77.59%\n",
      "    Val Batch 040/98 | Loss: 0.5110 | Batch Acc: 98.28%\n",
      "    Val Batch 045/98 | Loss: 1.2885 | Batch Acc: 67.24%\n",
      "    Val Batch 050/98 | Loss: 1.4783 | Batch Acc: 65.52%\n",
      "    Val Batch 055/98 | Loss: 0.7573 | Batch Acc: 70.69%\n",
      "    Val Batch 060/98 | Loss: 0.9223 | Batch Acc: 67.24%\n",
      "    Val Batch 065/98 | Loss: 1.0500 | Batch Acc: 70.69%\n",
      "    Val Batch 070/98 | Loss: 0.1900 | Batch Acc: 94.83%\n",
      "    Val Batch 075/98 | Loss: 0.1722 | Batch Acc: 94.83%\n",
      "    Val Batch 080/98 | Loss: 0.4724 | Batch Acc: 86.21%\n",
      "    Val Batch 085/98 | Loss: 0.0395 | Batch Acc: 98.28%\n",
      "    Val Batch 090/98 | Loss: 0.2962 | Batch Acc: 89.66%\n",
      "    Val Batch 095/98 | Loss: 0.0271 | Batch Acc: 98.28%\n",
      "    Val Batch 098/98 | Loss: 0.1726 | Batch Acc: 96.97%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.7785 | Accuracy: 76.30%\n",
      "  Current Best Acc: 79.36%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/902 | Loss: 0.4640 | CLoss: 0.3433 | FLoss: 0.2413 | LR: 1.04e-04\n",
      "  Batch 020/902 | Loss: 0.7517 | CLoss: 0.6338 | FLoss: 0.2357 | LR: 1.04e-04\n",
      "  Batch 030/902 | Loss: 0.4345 | CLoss: 0.3034 | FLoss: 0.2621 | LR: 1.04e-04\n",
      "  Batch 040/902 | Loss: 0.9263 | CLoss: 0.6573 | FLoss: 0.5380 | LR: 1.04e-04\n",
      "  Batch 050/902 | Loss: 0.6390 | CLoss: 0.5001 | FLoss: 0.2778 | LR: 1.04e-04\n",
      "  Batch 060/902 | Loss: 0.5902 | CLoss: 0.4623 | FLoss: 0.2559 | LR: 1.04e-04\n",
      "  Batch 070/902 | Loss: 0.9256 | CLoss: 0.7170 | FLoss: 0.4171 | LR: 1.04e-04\n",
      "  Batch 080/902 | Loss: 0.2744 | CLoss: 0.2091 | FLoss: 0.1305 | LR: 1.04e-04\n",
      "  Batch 090/902 | Loss: 0.7010 | CLoss: 0.6030 | FLoss: 0.1960 | LR: 1.04e-04\n",
      "  Batch 100/902 | Loss: 0.7072 | CLoss: 0.5656 | FLoss: 0.2831 | LR: 1.04e-04\n",
      "  Batch 110/902 | Loss: 0.5440 | CLoss: 0.3719 | FLoss: 0.3441 | LR: 1.04e-04\n",
      "  Batch 120/902 | Loss: 0.7813 | CLoss: 0.5783 | FLoss: 0.4061 | LR: 1.04e-04\n",
      "  Batch 130/902 | Loss: 0.7148 | CLoss: 0.5405 | FLoss: 0.3487 | LR: 1.04e-04\n",
      "  Batch 140/902 | Loss: 0.4281 | CLoss: 0.3316 | FLoss: 0.1929 | LR: 1.04e-04\n",
      "  Batch 150/902 | Loss: 0.5364 | CLoss: 0.3830 | FLoss: 0.3067 | LR: 1.04e-04\n",
      "  Batch 160/902 | Loss: 0.5301 | CLoss: 0.4379 | FLoss: 0.1844 | LR: 1.04e-04\n",
      "  Batch 170/902 | Loss: 0.4670 | CLoss: 0.3608 | FLoss: 0.2124 | LR: 1.04e-04\n",
      "  Batch 180/902 | Loss: 0.3596 | CLoss: 0.2621 | FLoss: 0.1950 | LR: 1.04e-04\n",
      "  Batch 190/902 | Loss: 0.7900 | CLoss: 0.6490 | FLoss: 0.2820 | LR: 1.04e-04\n",
      "  Batch 200/902 | Loss: 0.8623 | CLoss: 0.6518 | FLoss: 0.4211 | LR: 1.04e-04\n",
      "  Batch 210/902 | Loss: 0.9463 | CLoss: 0.7895 | FLoss: 0.3137 | LR: 1.04e-04\n",
      "  Batch 220/902 | Loss: 0.8677 | CLoss: 0.7364 | FLoss: 0.2626 | LR: 1.04e-04\n",
      "  Batch 230/902 | Loss: 0.5767 | CLoss: 0.4596 | FLoss: 0.2343 | LR: 1.04e-04\n",
      "  Batch 240/902 | Loss: 0.5047 | CLoss: 0.3929 | FLoss: 0.2237 | LR: 1.04e-04\n",
      "  Batch 250/902 | Loss: 0.6307 | CLoss: 0.5026 | FLoss: 0.2561 | LR: 1.04e-04\n",
      "  Batch 260/902 | Loss: 0.4107 | CLoss: 0.3200 | FLoss: 0.1814 | LR: 1.04e-04\n",
      "  Batch 270/902 | Loss: 0.7130 | CLoss: 0.5903 | FLoss: 0.2453 | LR: 1.04e-04\n",
      "  Batch 280/902 | Loss: 0.9877 | CLoss: 0.7953 | FLoss: 0.3848 | LR: 1.04e-04\n",
      "  Batch 290/902 | Loss: 0.4941 | CLoss: 0.3680 | FLoss: 0.2523 | LR: 1.04e-04\n",
      "  Batch 300/902 | Loss: 0.3741 | CLoss: 0.2492 | FLoss: 0.2499 | LR: 1.04e-04\n",
      "  Batch 310/902 | Loss: 0.6390 | CLoss: 0.5193 | FLoss: 0.2394 | LR: 1.04e-04\n",
      "  Batch 320/902 | Loss: 0.7096 | CLoss: 0.5626 | FLoss: 0.2939 | LR: 1.04e-04\n",
      "  Batch 330/902 | Loss: 0.7773 | CLoss: 0.5329 | FLoss: 0.4889 | LR: 1.04e-04\n",
      "  Batch 340/902 | Loss: 0.6626 | CLoss: 0.4032 | FLoss: 0.5189 | LR: 1.04e-04\n",
      "  Batch 350/902 | Loss: 0.6663 | CLoss: 0.5388 | FLoss: 0.2550 | LR: 1.04e-04\n",
      "  Batch 360/902 | Loss: 0.6631 | CLoss: 0.5001 | FLoss: 0.3259 | LR: 1.04e-04\n",
      "  Batch 370/902 | Loss: 0.5758 | CLoss: 0.4890 | FLoss: 0.1735 | LR: 1.04e-04\n",
      "  Batch 380/902 | Loss: 0.3306 | CLoss: 0.2564 | FLoss: 0.1484 | LR: 1.04e-04\n",
      "  Batch 390/902 | Loss: 0.5463 | CLoss: 0.4345 | FLoss: 0.2235 | LR: 1.04e-04\n",
      "  Batch 400/902 | Loss: 0.8309 | CLoss: 0.7017 | FLoss: 0.2584 | LR: 1.04e-04\n",
      "  Batch 410/902 | Loss: 0.4748 | CLoss: 0.3676 | FLoss: 0.2145 | LR: 1.04e-04\n",
      "  Batch 420/902 | Loss: 0.9140 | CLoss: 0.8276 | FLoss: 0.1728 | LR: 1.04e-04\n",
      "  Batch 430/902 | Loss: 0.6284 | CLoss: 0.4451 | FLoss: 0.3666 | LR: 1.04e-04\n",
      "  Batch 440/902 | Loss: 0.7914 | CLoss: 0.5956 | FLoss: 0.3916 | LR: 1.04e-04\n",
      "  Batch 450/902 | Loss: 0.8293 | CLoss: 0.6619 | FLoss: 0.3348 | LR: 1.04e-04\n",
      "  Batch 460/902 | Loss: 0.9957 | CLoss: 0.8419 | FLoss: 0.3075 | LR: 1.04e-04\n",
      "  Batch 470/902 | Loss: 0.7690 | CLoss: 0.5712 | FLoss: 0.3955 | LR: 1.04e-04\n",
      "  Batch 480/902 | Loss: 0.9368 | CLoss: 0.7815 | FLoss: 0.3105 | LR: 1.04e-04\n",
      "  Batch 490/902 | Loss: 0.6479 | CLoss: 0.5477 | FLoss: 0.2005 | LR: 1.04e-04\n",
      "  Batch 500/902 | Loss: 0.4580 | CLoss: 0.3469 | FLoss: 0.2222 | LR: 1.04e-04\n",
      "  Batch 510/902 | Loss: 0.5955 | CLoss: 0.4432 | FLoss: 0.3046 | LR: 1.04e-04\n",
      "  Batch 520/902 | Loss: 0.3597 | CLoss: 0.2400 | FLoss: 0.2394 | LR: 1.04e-04\n",
      "  Batch 530/902 | Loss: 0.7621 | CLoss: 0.5741 | FLoss: 0.3761 | LR: 1.04e-04\n",
      "  Batch 540/902 | Loss: 0.5415 | CLoss: 0.4335 | FLoss: 0.2159 | LR: 1.04e-04\n",
      "  Batch 550/902 | Loss: 0.3742 | CLoss: 0.3015 | FLoss: 0.1453 | LR: 1.04e-04\n",
      "  Batch 560/902 | Loss: 0.8046 | CLoss: 0.6351 | FLoss: 0.3391 | LR: 1.04e-04\n",
      "  Batch 570/902 | Loss: 0.3963 | CLoss: 0.2890 | FLoss: 0.2147 | LR: 1.04e-04\n",
      "  Batch 580/902 | Loss: 0.8461 | CLoss: 0.6358 | FLoss: 0.4206 | LR: 1.04e-04\n",
      "  Batch 590/902 | Loss: 0.3588 | CLoss: 0.3038 | FLoss: 0.1100 | LR: 1.04e-04\n",
      "  Batch 600/902 | Loss: 0.6232 | CLoss: 0.4623 | FLoss: 0.3218 | LR: 1.04e-04\n",
      "  Batch 610/902 | Loss: 0.6461 | CLoss: 0.4752 | FLoss: 0.3417 | LR: 1.04e-04\n",
      "  Batch 620/902 | Loss: 0.7739 | CLoss: 0.6066 | FLoss: 0.3346 | LR: 1.04e-04\n",
      "  Batch 630/902 | Loss: 0.6372 | CLoss: 0.5188 | FLoss: 0.2368 | LR: 1.04e-04\n",
      "  Batch 640/902 | Loss: 0.4173 | CLoss: 0.3009 | FLoss: 0.2327 | LR: 1.04e-04\n",
      "  Batch 650/902 | Loss: 0.5765 | CLoss: 0.4272 | FLoss: 0.2986 | LR: 1.04e-04\n",
      "  Batch 660/902 | Loss: 0.3575 | CLoss: 0.2435 | FLoss: 0.2279 | LR: 1.04e-04\n",
      "  Batch 670/902 | Loss: 0.7257 | CLoss: 0.5552 | FLoss: 0.3411 | LR: 1.04e-04\n",
      "  Batch 680/902 | Loss: 0.6724 | CLoss: 0.4731 | FLoss: 0.3988 | LR: 1.04e-04\n",
      "  Batch 690/902 | Loss: 1.0577 | CLoss: 0.8965 | FLoss: 0.3225 | LR: 1.04e-04\n",
      "  Batch 700/902 | Loss: 0.8361 | CLoss: 0.7063 | FLoss: 0.2596 | LR: 1.04e-04\n",
      "  Batch 710/902 | Loss: 0.6549 | CLoss: 0.4906 | FLoss: 0.3287 | LR: 1.04e-04\n",
      "  Batch 720/902 | Loss: 0.8234 | CLoss: 0.6684 | FLoss: 0.3102 | LR: 1.04e-04\n",
      "  Batch 730/902 | Loss: 0.4465 | CLoss: 0.3596 | FLoss: 0.1739 | LR: 1.04e-04\n",
      "  Batch 740/902 | Loss: 0.8695 | CLoss: 0.6069 | FLoss: 0.5252 | LR: 1.04e-04\n",
      "  Batch 750/902 | Loss: 0.7856 | CLoss: 0.6156 | FLoss: 0.3399 | LR: 1.04e-04\n",
      "  Batch 760/902 | Loss: 0.6386 | CLoss: 0.4819 | FLoss: 0.3134 | LR: 1.04e-04\n",
      "  Batch 770/902 | Loss: 0.8144 | CLoss: 0.5724 | FLoss: 0.4839 | LR: 1.04e-04\n",
      "  Batch 780/902 | Loss: 1.0171 | CLoss: 0.8394 | FLoss: 0.3554 | LR: 1.04e-04\n",
      "  Batch 790/902 | Loss: 0.5628 | CLoss: 0.4423 | FLoss: 0.2409 | LR: 1.04e-04\n",
      "  Batch 800/902 | Loss: 0.8682 | CLoss: 0.6525 | FLoss: 0.4314 | LR: 1.04e-04\n",
      "  Batch 810/902 | Loss: 0.4514 | CLoss: 0.3431 | FLoss: 0.2166 | LR: 1.04e-04\n",
      "  Batch 820/902 | Loss: 0.4616 | CLoss: 0.3479 | FLoss: 0.2275 | LR: 1.04e-04\n",
      "  Batch 830/902 | Loss: 0.8029 | CLoss: 0.6415 | FLoss: 0.3228 | LR: 1.04e-04\n",
      "  Batch 840/902 | Loss: 0.6531 | CLoss: 0.4741 | FLoss: 0.3580 | LR: 1.04e-04\n",
      "  Batch 850/902 | Loss: 0.6720 | CLoss: 0.5372 | FLoss: 0.2695 | LR: 1.04e-04\n",
      "  Batch 860/902 | Loss: 0.9832 | CLoss: 0.7339 | FLoss: 0.4986 | LR: 1.04e-04\n",
      "  Batch 870/902 | Loss: 0.7131 | CLoss: 0.5721 | FLoss: 0.2820 | LR: 1.04e-04\n",
      "  Batch 880/902 | Loss: 0.4258 | CLoss: 0.3488 | FLoss: 0.1539 | LR: 1.04e-04\n",
      "  Batch 890/902 | Loss: 0.3926 | CLoss: 0.2216 | FLoss: 0.3420 | LR: 1.04e-04\n",
      "  Batch 900/902 | Loss: 0.4180 | CLoss: 0.3184 | FLoss: 0.1992 | LR: 1.04e-04\n",
      "  Batch 902/902 | Loss: 1.2807 | CLoss: 0.0000 | FLoss: 2.5615 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.6449\n",
      "  Last Batch Loss: 1.2807\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/98 | Loss: 0.7908 | Batch Acc: 70.69%\n",
      "    Val Batch 010/98 | Loss: 0.8127 | Batch Acc: 79.31%\n",
      "    Val Batch 015/98 | Loss: 0.4295 | Batch Acc: 82.76%\n",
      "    Val Batch 020/98 | Loss: 0.9683 | Batch Acc: 44.83%\n",
      "    Val Batch 025/98 | Loss: 1.3314 | Batch Acc: 56.90%\n",
      "    Val Batch 030/98 | Loss: 0.7736 | Batch Acc: 81.03%\n",
      "    Val Batch 035/98 | Loss: 0.8039 | Batch Acc: 74.14%\n",
      "    Val Batch 040/98 | Loss: 0.7669 | Batch Acc: 82.76%\n",
      "    Val Batch 045/98 | Loss: 1.3082 | Batch Acc: 70.69%\n",
      "    Val Batch 050/98 | Loss: 0.9193 | Batch Acc: 77.59%\n",
      "    Val Batch 055/98 | Loss: 0.8045 | Batch Acc: 74.14%\n",
      "    Val Batch 060/98 | Loss: 0.3923 | Batch Acc: 87.93%\n",
      "    Val Batch 065/98 | Loss: 0.5043 | Batch Acc: 86.21%\n",
      "    Val Batch 070/98 | Loss: 0.1975 | Batch Acc: 94.83%\n",
      "    Val Batch 075/98 | Loss: 0.1158 | Batch Acc: 96.55%\n",
      "    Val Batch 080/98 | Loss: 0.0903 | Batch Acc: 94.83%\n",
      "    Val Batch 085/98 | Loss: 0.1122 | Batch Acc: 93.10%\n",
      "    Val Batch 090/98 | Loss: 0.2755 | Batch Acc: 89.66%\n",
      "    Val Batch 095/98 | Loss: 0.3059 | Batch Acc: 93.10%\n",
      "    Val Batch 098/98 | Loss: 0.2449 | Batch Acc: 93.94%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.6700 | Accuracy: 79.04%\n",
      "  Current Best Acc: 79.36%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/902 | Loss: 1.0599 | CLoss: 0.8207 | FLoss: 0.4784 | LR: 2.86e-05\n",
      "  Batch 020/902 | Loss: 0.5521 | CLoss: 0.4414 | FLoss: 0.2213 | LR: 2.86e-05\n",
      "  Batch 030/902 | Loss: 0.9037 | CLoss: 0.6947 | FLoss: 0.4179 | LR: 2.86e-05\n",
      "  Batch 040/902 | Loss: 0.3091 | CLoss: 0.2079 | FLoss: 0.2023 | LR: 2.86e-05\n",
      "  Batch 050/902 | Loss: 0.4289 | CLoss: 0.3027 | FLoss: 0.2523 | LR: 2.86e-05\n",
      "  Batch 060/902 | Loss: 0.6473 | CLoss: 0.5065 | FLoss: 0.2815 | LR: 2.86e-05\n",
      "  Batch 070/902 | Loss: 0.5757 | CLoss: 0.4941 | FLoss: 0.1632 | LR: 2.86e-05\n",
      "  Batch 080/902 | Loss: 1.2065 | CLoss: 0.9703 | FLoss: 0.4723 | LR: 2.86e-05\n",
      "  Batch 090/902 | Loss: 0.7339 | CLoss: 0.5530 | FLoss: 0.3617 | LR: 2.86e-05\n",
      "  Batch 100/902 | Loss: 0.7176 | CLoss: 0.5988 | FLoss: 0.2375 | LR: 2.86e-05\n",
      "  Batch 110/902 | Loss: 0.2919 | CLoss: 0.1781 | FLoss: 0.2276 | LR: 2.86e-05\n",
      "  Batch 120/902 | Loss: 1.1813 | CLoss: 1.0475 | FLoss: 0.2675 | LR: 2.86e-05\n",
      "  Batch 130/902 | Loss: 0.5118 | CLoss: 0.3587 | FLoss: 0.3061 | LR: 2.86e-05\n",
      "  Batch 140/902 | Loss: 0.4403 | CLoss: 0.3181 | FLoss: 0.2444 | LR: 2.86e-05\n",
      "  Batch 150/902 | Loss: 0.6372 | CLoss: 0.4885 | FLoss: 0.2974 | LR: 2.86e-05\n",
      "  Batch 160/902 | Loss: 0.3317 | CLoss: 0.2910 | FLoss: 0.0814 | LR: 2.86e-05\n",
      "  Batch 170/902 | Loss: 0.4843 | CLoss: 0.3956 | FLoss: 0.1776 | LR: 2.86e-05\n",
      "  Batch 180/902 | Loss: 0.9458 | CLoss: 0.7363 | FLoss: 0.4189 | LR: 2.86e-05\n",
      "  Batch 190/902 | Loss: 0.5182 | CLoss: 0.3713 | FLoss: 0.2938 | LR: 2.86e-05\n",
      "  Batch 200/902 | Loss: 0.4867 | CLoss: 0.4034 | FLoss: 0.1667 | LR: 2.86e-05\n",
      "  Batch 210/902 | Loss: 0.6069 | CLoss: 0.4669 | FLoss: 0.2800 | LR: 2.86e-05\n",
      "  Batch 220/902 | Loss: 0.7549 | CLoss: 0.5388 | FLoss: 0.4323 | LR: 2.86e-05\n",
      "  Batch 230/902 | Loss: 0.6203 | CLoss: 0.4919 | FLoss: 0.2567 | LR: 2.86e-05\n",
      "  Batch 240/902 | Loss: 0.3553 | CLoss: 0.2908 | FLoss: 0.1289 | LR: 2.86e-05\n",
      "  Batch 250/902 | Loss: 0.8087 | CLoss: 0.6655 | FLoss: 0.2864 | LR: 2.86e-05\n",
      "  Batch 260/902 | Loss: 0.9197 | CLoss: 0.8121 | FLoss: 0.2153 | LR: 2.86e-05\n",
      "  Batch 270/902 | Loss: 0.3900 | CLoss: 0.3087 | FLoss: 0.1625 | LR: 2.86e-05\n",
      "  Batch 280/902 | Loss: 0.1768 | CLoss: 0.1533 | FLoss: 0.0469 | LR: 2.86e-05\n",
      "  Batch 290/902 | Loss: 0.3786 | CLoss: 0.2547 | FLoss: 0.2478 | LR: 2.86e-05\n",
      "  Batch 300/902 | Loss: 0.9230 | CLoss: 0.7082 | FLoss: 0.4297 | LR: 2.86e-05\n",
      "  Batch 310/902 | Loss: 0.4505 | CLoss: 0.3326 | FLoss: 0.2357 | LR: 2.86e-05\n",
      "  Batch 320/902 | Loss: 0.4037 | CLoss: 0.3220 | FLoss: 0.1633 | LR: 2.86e-05\n",
      "  Batch 330/902 | Loss: 0.4384 | CLoss: 0.3368 | FLoss: 0.2034 | LR: 2.86e-05\n",
      "  Batch 340/902 | Loss: 0.4655 | CLoss: 0.3425 | FLoss: 0.2461 | LR: 2.86e-05\n",
      "  Batch 350/902 | Loss: 0.7088 | CLoss: 0.6014 | FLoss: 0.2149 | LR: 2.86e-05\n",
      "  Batch 360/902 | Loss: 0.6715 | CLoss: 0.5142 | FLoss: 0.3147 | LR: 2.86e-05\n",
      "  Batch 370/902 | Loss: 0.6549 | CLoss: 0.5299 | FLoss: 0.2500 | LR: 2.86e-05\n",
      "  Batch 380/902 | Loss: 0.7797 | CLoss: 0.6497 | FLoss: 0.2599 | LR: 2.86e-05\n",
      "  Batch 390/902 | Loss: 0.6798 | CLoss: 0.5535 | FLoss: 0.2527 | LR: 2.86e-05\n",
      "  Batch 400/902 | Loss: 0.9704 | CLoss: 0.7814 | FLoss: 0.3781 | LR: 2.86e-05\n",
      "  Batch 410/902 | Loss: 0.3723 | CLoss: 0.3126 | FLoss: 0.1195 | LR: 2.86e-05\n",
      "  Batch 420/902 | Loss: 0.7065 | CLoss: 0.5671 | FLoss: 0.2788 | LR: 2.86e-05\n",
      "  Batch 430/902 | Loss: 0.6377 | CLoss: 0.4417 | FLoss: 0.3921 | LR: 2.86e-05\n",
      "  Batch 440/902 | Loss: 1.0121 | CLoss: 0.7917 | FLoss: 0.4407 | LR: 2.86e-05\n",
      "  Batch 450/902 | Loss: 1.1853 | CLoss: 1.0270 | FLoss: 0.3165 | LR: 2.86e-05\n",
      "  Batch 460/902 | Loss: 0.4949 | CLoss: 0.4169 | FLoss: 0.1559 | LR: 2.86e-05\n",
      "  Batch 470/902 | Loss: 0.3816 | CLoss: 0.2700 | FLoss: 0.2232 | LR: 2.86e-05\n",
      "  Batch 480/902 | Loss: 0.5791 | CLoss: 0.3942 | FLoss: 0.3699 | LR: 2.86e-05\n",
      "  Batch 490/902 | Loss: 0.3613 | CLoss: 0.2912 | FLoss: 0.1403 | LR: 2.86e-05\n",
      "  Batch 500/902 | Loss: 0.4194 | CLoss: 0.3350 | FLoss: 0.1688 | LR: 2.86e-05\n",
      "  Batch 510/902 | Loss: 0.7634 | CLoss: 0.6657 | FLoss: 0.1952 | LR: 2.86e-05\n",
      "  Batch 520/902 | Loss: 0.2558 | CLoss: 0.2123 | FLoss: 0.0869 | LR: 2.86e-05\n",
      "  Batch 530/902 | Loss: 0.8270 | CLoss: 0.7071 | FLoss: 0.2399 | LR: 2.86e-05\n",
      "  Batch 540/902 | Loss: 0.4983 | CLoss: 0.3324 | FLoss: 0.3318 | LR: 2.86e-05\n",
      "  Batch 550/902 | Loss: 0.6540 | CLoss: 0.5560 | FLoss: 0.1959 | LR: 2.86e-05\n",
      "  Batch 560/902 | Loss: 0.3760 | CLoss: 0.2909 | FLoss: 0.1702 | LR: 2.86e-05\n",
      "  Batch 570/902 | Loss: 0.7585 | CLoss: 0.6074 | FLoss: 0.3021 | LR: 2.86e-05\n",
      "  Batch 580/902 | Loss: 0.4843 | CLoss: 0.4330 | FLoss: 0.1027 | LR: 2.86e-05\n",
      "  Batch 590/902 | Loss: 0.9443 | CLoss: 0.7827 | FLoss: 0.3232 | LR: 2.86e-05\n",
      "  Batch 600/902 | Loss: 0.6064 | CLoss: 0.4872 | FLoss: 0.2385 | LR: 2.86e-05\n",
      "  Batch 610/902 | Loss: 0.5638 | CLoss: 0.4755 | FLoss: 0.1766 | LR: 2.86e-05\n",
      "  Batch 620/902 | Loss: 0.8335 | CLoss: 0.5972 | FLoss: 0.4726 | LR: 2.86e-05\n",
      "  Batch 630/902 | Loss: 0.2681 | CLoss: 0.2226 | FLoss: 0.0909 | LR: 2.86e-05\n",
      "  Batch 640/902 | Loss: 0.6341 | CLoss: 0.5321 | FLoss: 0.2042 | LR: 2.86e-05\n",
      "  Batch 650/902 | Loss: 0.5164 | CLoss: 0.4414 | FLoss: 0.1499 | LR: 2.86e-05\n",
      "  Batch 660/902 | Loss: 0.4414 | CLoss: 0.3677 | FLoss: 0.1474 | LR: 2.86e-05\n",
      "  Batch 670/902 | Loss: 0.8728 | CLoss: 0.7247 | FLoss: 0.2964 | LR: 2.86e-05\n",
      "  Batch 680/902 | Loss: 0.7012 | CLoss: 0.5081 | FLoss: 0.3861 | LR: 2.86e-05\n",
      "  Batch 690/902 | Loss: 0.2040 | CLoss: 0.1510 | FLoss: 0.1060 | LR: 2.86e-05\n",
      "  Batch 700/902 | Loss: 0.6282 | CLoss: 0.4400 | FLoss: 0.3763 | LR: 2.86e-05\n",
      "  Batch 710/902 | Loss: 0.7190 | CLoss: 0.5741 | FLoss: 0.2899 | LR: 2.86e-05\n",
      "  Batch 720/902 | Loss: 0.6737 | CLoss: 0.5619 | FLoss: 0.2236 | LR: 2.86e-05\n",
      "  Batch 730/902 | Loss: 0.6298 | CLoss: 0.4635 | FLoss: 0.3327 | LR: 2.86e-05\n",
      "  Batch 740/902 | Loss: 0.5104 | CLoss: 0.4313 | FLoss: 0.1582 | LR: 2.86e-05\n",
      "  Batch 750/902 | Loss: 0.2846 | CLoss: 0.2355 | FLoss: 0.0981 | LR: 2.86e-05\n",
      "  Batch 760/902 | Loss: 0.4503 | CLoss: 0.2646 | FLoss: 0.3715 | LR: 2.86e-05\n",
      "  Batch 770/902 | Loss: 0.6765 | CLoss: 0.5589 | FLoss: 0.2353 | LR: 2.86e-05\n",
      "  Batch 780/902 | Loss: 0.4814 | CLoss: 0.4038 | FLoss: 0.1552 | LR: 2.86e-05\n",
      "  Batch 790/902 | Loss: 0.7344 | CLoss: 0.5917 | FLoss: 0.2853 | LR: 2.86e-05\n",
      "  Batch 800/902 | Loss: 0.8284 | CLoss: 0.6479 | FLoss: 0.3611 | LR: 2.86e-05\n",
      "  Batch 810/902 | Loss: 0.6154 | CLoss: 0.4886 | FLoss: 0.2535 | LR: 2.86e-05\n",
      "  Batch 820/902 | Loss: 0.9284 | CLoss: 0.7374 | FLoss: 0.3819 | LR: 2.86e-05\n",
      "  Batch 830/902 | Loss: 0.6218 | CLoss: 0.5100 | FLoss: 0.2235 | LR: 2.86e-05\n",
      "  Batch 840/902 | Loss: 0.5904 | CLoss: 0.4698 | FLoss: 0.2412 | LR: 2.86e-05\n",
      "  Batch 850/902 | Loss: 0.6135 | CLoss: 0.4970 | FLoss: 0.2330 | LR: 2.86e-05\n",
      "  Batch 860/902 | Loss: 0.6186 | CLoss: 0.5296 | FLoss: 0.1779 | LR: 2.86e-05\n",
      "  Batch 870/902 | Loss: 0.6916 | CLoss: 0.5218 | FLoss: 0.3396 | LR: 2.86e-05\n",
      "  Batch 880/902 | Loss: 0.3787 | CLoss: 0.3032 | FLoss: 0.1511 | LR: 2.86e-05\n",
      "  Batch 890/902 | Loss: 0.6899 | CLoss: 0.4580 | FLoss: 0.4636 | LR: 2.86e-05\n",
      "  Batch 900/902 | Loss: 0.5088 | CLoss: 0.3713 | FLoss: 0.2751 | LR: 2.86e-05\n",
      "  Batch 902/902 | Loss: 1.5433 | CLoss: 0.0000 | FLoss: 3.0866 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.5997\n",
      "  Last Batch Loss: 1.5433\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/98 | Loss: 0.8162 | Batch Acc: 65.52%\n",
      "    Val Batch 010/98 | Loss: 0.4041 | Batch Acc: 84.48%\n",
      "    Val Batch 015/98 | Loss: 0.4996 | Batch Acc: 84.48%\n",
      "    Val Batch 020/98 | Loss: 1.5773 | Batch Acc: 13.79%\n",
      "    Val Batch 025/98 | Loss: 1.3580 | Batch Acc: 70.69%\n",
      "    Val Batch 030/98 | Loss: 0.5509 | Batch Acc: 79.31%\n",
      "    Val Batch 035/98 | Loss: 0.7914 | Batch Acc: 79.31%\n",
      "    Val Batch 040/98 | Loss: 0.7474 | Batch Acc: 93.10%\n",
      "    Val Batch 045/98 | Loss: 1.1721 | Batch Acc: 74.14%\n",
      "    Val Batch 050/98 | Loss: 1.1420 | Batch Acc: 75.86%\n",
      "    Val Batch 055/98 | Loss: 1.0962 | Batch Acc: 63.79%\n",
      "    Val Batch 060/98 | Loss: 0.5478 | Batch Acc: 82.76%\n",
      "    Val Batch 065/98 | Loss: 0.6420 | Batch Acc: 84.48%\n",
      "    Val Batch 070/98 | Loss: 0.1193 | Batch Acc: 94.83%\n",
      "    Val Batch 075/98 | Loss: 0.1452 | Batch Acc: 91.38%\n",
      "    Val Batch 080/98 | Loss: 0.2725 | Batch Acc: 91.38%\n",
      "    Val Batch 085/98 | Loss: 0.2824 | Batch Acc: 93.10%\n",
      "    Val Batch 090/98 | Loss: 0.2516 | Batch Acc: 91.38%\n",
      "    Val Batch 095/98 | Loss: 0.1145 | Batch Acc: 94.83%\n",
      "    Val Batch 098/98 | Loss: 0.0496 | Batch Acc: 96.97%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.6399 | Accuracy: 79.86%\n",
      "  Current Best Acc: 79.86%\n",
      "\n",
      "========================================\n",
      "=== Fold 8 Completed ===\n",
      "Best Validation Accuracy: 79.86%\n",
      "\n",
      "========================================\n",
      "=== Fold 9/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/901 | Loss: 2.0699 | CLoss: 1.1185 | FLoss: 1.9028 | LR: 3.00e-04\n",
      "  Batch 020/901 | Loss: 1.8711 | CLoss: 1.1451 | FLoss: 1.4520 | LR: 3.00e-04\n",
      "  Batch 030/901 | Loss: 0.7812 | CLoss: 0.2945 | FLoss: 0.9735 | LR: 3.00e-04\n",
      "  Batch 040/901 | Loss: 1.0622 | CLoss: 0.6510 | FLoss: 0.8225 | LR: 3.00e-04\n",
      "  Batch 050/901 | Loss: 1.3451 | CLoss: 0.9575 | FLoss: 0.7751 | LR: 3.00e-04\n",
      "  Batch 060/901 | Loss: 1.0739 | CLoss: 0.6835 | FLoss: 0.7807 | LR: 3.00e-04\n",
      "  Batch 070/901 | Loss: 1.2381 | CLoss: 0.9230 | FLoss: 0.6302 | LR: 3.00e-04\n",
      "  Batch 080/901 | Loss: 0.7276 | CLoss: 0.4933 | FLoss: 0.4687 | LR: 3.00e-04\n",
      "  Batch 090/901 | Loss: 1.3513 | CLoss: 1.0428 | FLoss: 0.6171 | LR: 3.00e-04\n",
      "  Batch 100/901 | Loss: 1.0526 | CLoss: 0.8522 | FLoss: 0.4008 | LR: 3.00e-04\n",
      "  Batch 110/901 | Loss: 0.7394 | CLoss: 0.5300 | FLoss: 0.4188 | LR: 3.00e-04\n",
      "  Batch 120/901 | Loss: 1.6925 | CLoss: 1.2883 | FLoss: 0.8084 | LR: 3.00e-04\n",
      "  Batch 130/901 | Loss: 0.9144 | CLoss: 0.6663 | FLoss: 0.4961 | LR: 3.00e-04\n",
      "  Batch 140/901 | Loss: 0.6796 | CLoss: 0.5238 | FLoss: 0.3116 | LR: 3.00e-04\n",
      "  Batch 150/901 | Loss: 1.1532 | CLoss: 0.8901 | FLoss: 0.5264 | LR: 3.00e-04\n",
      "  Batch 160/901 | Loss: 0.7107 | CLoss: 0.4404 | FLoss: 0.5406 | LR: 3.00e-04\n",
      "  Batch 170/901 | Loss: 0.5876 | CLoss: 0.4422 | FLoss: 0.2909 | LR: 3.00e-04\n",
      "  Batch 180/901 | Loss: 0.8109 | CLoss: 0.5446 | FLoss: 0.5327 | LR: 3.00e-04\n",
      "  Batch 190/901 | Loss: 0.7513 | CLoss: 0.5171 | FLoss: 0.4684 | LR: 3.00e-04\n",
      "  Batch 200/901 | Loss: 0.8610 | CLoss: 0.6474 | FLoss: 0.4274 | LR: 3.00e-04\n",
      "  Batch 210/901 | Loss: 0.7580 | CLoss: 0.5367 | FLoss: 0.4427 | LR: 3.00e-04\n",
      "  Batch 220/901 | Loss: 1.2519 | CLoss: 1.0178 | FLoss: 0.4682 | LR: 3.00e-04\n",
      "  Batch 230/901 | Loss: 1.0218 | CLoss: 0.7339 | FLoss: 0.5757 | LR: 3.00e-04\n",
      "  Batch 240/901 | Loss: 1.0928 | CLoss: 0.8513 | FLoss: 0.4830 | LR: 3.00e-04\n",
      "  Batch 250/901 | Loss: 1.1901 | CLoss: 0.9603 | FLoss: 0.4597 | LR: 3.00e-04\n",
      "  Batch 260/901 | Loss: 0.6290 | CLoss: 0.4598 | FLoss: 0.3385 | LR: 3.00e-04\n",
      "  Batch 270/901 | Loss: 0.8864 | CLoss: 0.7359 | FLoss: 0.3009 | LR: 3.00e-04\n",
      "  Batch 280/901 | Loss: 0.9965 | CLoss: 0.7890 | FLoss: 0.4150 | LR: 3.00e-04\n",
      "  Batch 290/901 | Loss: 1.0448 | CLoss: 0.7987 | FLoss: 0.4922 | LR: 3.00e-04\n",
      "  Batch 300/901 | Loss: 1.3286 | CLoss: 1.1047 | FLoss: 0.4478 | LR: 3.00e-04\n",
      "  Batch 310/901 | Loss: 1.4365 | CLoss: 1.1922 | FLoss: 0.4886 | LR: 3.00e-04\n",
      "  Batch 320/901 | Loss: 0.7581 | CLoss: 0.5839 | FLoss: 0.3483 | LR: 3.00e-04\n",
      "  Batch 330/901 | Loss: 0.6942 | CLoss: 0.5361 | FLoss: 0.3164 | LR: 3.00e-04\n",
      "  Batch 340/901 | Loss: 1.0640 | CLoss: 0.9180 | FLoss: 0.2920 | LR: 3.00e-04\n",
      "  Batch 350/901 | Loss: 0.8760 | CLoss: 0.6861 | FLoss: 0.3796 | LR: 3.00e-04\n",
      "  Batch 360/901 | Loss: 0.8060 | CLoss: 0.6571 | FLoss: 0.2978 | LR: 3.00e-04\n",
      "  Batch 370/901 | Loss: 0.6752 | CLoss: 0.5167 | FLoss: 0.3170 | LR: 3.00e-04\n",
      "  Batch 380/901 | Loss: 0.8993 | CLoss: 0.7585 | FLoss: 0.2816 | LR: 3.00e-04\n",
      "  Batch 390/901 | Loss: 0.8740 | CLoss: 0.7423 | FLoss: 0.2633 | LR: 3.00e-04\n",
      "  Batch 400/901 | Loss: 0.9529 | CLoss: 0.7752 | FLoss: 0.3555 | LR: 3.00e-04\n",
      "  Batch 410/901 | Loss: 0.5754 | CLoss: 0.4521 | FLoss: 0.2465 | LR: 3.00e-04\n",
      "  Batch 420/901 | Loss: 0.7129 | CLoss: 0.5407 | FLoss: 0.3443 | LR: 3.00e-04\n",
      "  Batch 430/901 | Loss: 1.0079 | CLoss: 0.7927 | FLoss: 0.4303 | LR: 3.00e-04\n",
      "  Batch 440/901 | Loss: 0.9618 | CLoss: 0.6857 | FLoss: 0.5523 | LR: 3.00e-04\n",
      "  Batch 450/901 | Loss: 0.6300 | CLoss: 0.4905 | FLoss: 0.2791 | LR: 3.00e-04\n",
      "  Batch 460/901 | Loss: 0.7011 | CLoss: 0.5473 | FLoss: 0.3077 | LR: 3.00e-04\n",
      "  Batch 470/901 | Loss: 1.1022 | CLoss: 0.9010 | FLoss: 0.4024 | LR: 3.00e-04\n",
      "  Batch 480/901 | Loss: 0.9920 | CLoss: 0.8375 | FLoss: 0.3090 | LR: 3.00e-04\n",
      "  Batch 490/901 | Loss: 0.6882 | CLoss: 0.4646 | FLoss: 0.4473 | LR: 3.00e-04\n",
      "  Batch 500/901 | Loss: 0.9416 | CLoss: 0.7110 | FLoss: 0.4612 | LR: 3.00e-04\n",
      "  Batch 510/901 | Loss: 1.1288 | CLoss: 0.9234 | FLoss: 0.4107 | LR: 3.00e-04\n",
      "  Batch 520/901 | Loss: 0.9507 | CLoss: 0.7546 | FLoss: 0.3920 | LR: 3.00e-04\n",
      "  Batch 530/901 | Loss: 0.8461 | CLoss: 0.6363 | FLoss: 0.4198 | LR: 3.00e-04\n",
      "  Batch 540/901 | Loss: 0.6879 | CLoss: 0.5018 | FLoss: 0.3722 | LR: 3.00e-04\n",
      "  Batch 550/901 | Loss: 0.7015 | CLoss: 0.5873 | FLoss: 0.2285 | LR: 3.00e-04\n",
      "  Batch 560/901 | Loss: 1.1313 | CLoss: 0.8920 | FLoss: 0.4785 | LR: 3.00e-04\n",
      "  Batch 570/901 | Loss: 0.8377 | CLoss: 0.6560 | FLoss: 0.3634 | LR: 3.00e-04\n",
      "  Batch 580/901 | Loss: 0.5103 | CLoss: 0.3612 | FLoss: 0.2982 | LR: 3.00e-04\n",
      "  Batch 590/901 | Loss: 0.8429 | CLoss: 0.6097 | FLoss: 0.4665 | LR: 3.00e-04\n",
      "  Batch 600/901 | Loss: 0.7776 | CLoss: 0.6123 | FLoss: 0.3306 | LR: 3.00e-04\n",
      "  Batch 610/901 | Loss: 0.8627 | CLoss: 0.7004 | FLoss: 0.3246 | LR: 3.00e-04\n",
      "  Batch 620/901 | Loss: 1.2757 | CLoss: 1.0096 | FLoss: 0.5322 | LR: 3.00e-04\n",
      "  Batch 630/901 | Loss: 0.8779 | CLoss: 0.7176 | FLoss: 0.3207 | LR: 3.00e-04\n",
      "  Batch 640/901 | Loss: 0.6763 | CLoss: 0.4811 | FLoss: 0.3904 | LR: 3.00e-04\n",
      "  Batch 650/901 | Loss: 1.2963 | CLoss: 1.1406 | FLoss: 0.3115 | LR: 3.00e-04\n",
      "  Batch 660/901 | Loss: 0.8083 | CLoss: 0.6034 | FLoss: 0.4096 | LR: 3.00e-04\n",
      "  Batch 670/901 | Loss: 1.1355 | CLoss: 0.8416 | FLoss: 0.5879 | LR: 3.00e-04\n",
      "  Batch 680/901 | Loss: 0.6188 | CLoss: 0.4697 | FLoss: 0.2982 | LR: 3.00e-04\n",
      "  Batch 690/901 | Loss: 0.7120 | CLoss: 0.5703 | FLoss: 0.2833 | LR: 3.00e-04\n",
      "  Batch 700/901 | Loss: 0.5997 | CLoss: 0.4200 | FLoss: 0.3595 | LR: 3.00e-04\n",
      "  Batch 710/901 | Loss: 0.5447 | CLoss: 0.3607 | FLoss: 0.3679 | LR: 3.00e-04\n",
      "  Batch 720/901 | Loss: 0.9613 | CLoss: 0.8016 | FLoss: 0.3193 | LR: 3.00e-04\n",
      "  Batch 730/901 | Loss: 0.7813 | CLoss: 0.6581 | FLoss: 0.2464 | LR: 3.00e-04\n",
      "  Batch 740/901 | Loss: 0.8146 | CLoss: 0.5887 | FLoss: 0.4518 | LR: 3.00e-04\n",
      "  Batch 750/901 | Loss: 0.7202 | CLoss: 0.5603 | FLoss: 0.3199 | LR: 3.00e-04\n",
      "  Batch 760/901 | Loss: 1.2356 | CLoss: 1.0670 | FLoss: 0.3371 | LR: 3.00e-04\n",
      "  Batch 770/901 | Loss: 0.5800 | CLoss: 0.4458 | FLoss: 0.2684 | LR: 3.00e-04\n",
      "  Batch 780/901 | Loss: 0.8566 | CLoss: 0.5911 | FLoss: 0.5308 | LR: 3.00e-04\n",
      "  Batch 790/901 | Loss: 0.8551 | CLoss: 0.6654 | FLoss: 0.3793 | LR: 3.00e-04\n",
      "  Batch 800/901 | Loss: 0.7824 | CLoss: 0.6605 | FLoss: 0.2440 | LR: 3.00e-04\n",
      "  Batch 810/901 | Loss: 0.4512 | CLoss: 0.3221 | FLoss: 0.2580 | LR: 3.00e-04\n",
      "  Batch 820/901 | Loss: 0.7592 | CLoss: 0.5979 | FLoss: 0.3226 | LR: 3.00e-04\n",
      "  Batch 830/901 | Loss: 0.6621 | CLoss: 0.5449 | FLoss: 0.2343 | LR: 3.00e-04\n",
      "  Batch 840/901 | Loss: 1.2531 | CLoss: 0.9155 | FLoss: 0.6750 | LR: 3.00e-04\n",
      "  Batch 850/901 | Loss: 0.6403 | CLoss: 0.4915 | FLoss: 0.2975 | LR: 3.00e-04\n",
      "  Batch 860/901 | Loss: 0.8434 | CLoss: 0.6618 | FLoss: 0.3632 | LR: 3.00e-04\n",
      "  Batch 870/901 | Loss: 0.7654 | CLoss: 0.5772 | FLoss: 0.3764 | LR: 3.00e-04\n",
      "  Batch 880/901 | Loss: 0.6472 | CLoss: 0.4160 | FLoss: 0.4623 | LR: 3.00e-04\n",
      "  Batch 890/901 | Loss: 1.0296 | CLoss: 0.8086 | FLoss: 0.4420 | LR: 3.00e-04\n",
      "  Batch 900/901 | Loss: 0.5558 | CLoss: 0.3717 | FLoss: 0.3682 | LR: 3.00e-04\n",
      "  Batch 901/901 | Loss: 0.3577 | CLoss: 0.1855 | FLoss: 0.3445 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 0.9118\n",
      "  Last Batch Loss: 0.3577\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.7311 | Batch Acc: 70.69%\n",
      "    Val Batch 010/99 | Loss: 0.1949 | Batch Acc: 96.55%\n",
      "    Val Batch 015/99 | Loss: 0.1967 | Batch Acc: 94.83%\n",
      "    Val Batch 020/99 | Loss: 1.4443 | Batch Acc: 15.52%\n",
      "    Val Batch 025/99 | Loss: 0.3976 | Batch Acc: 87.93%\n",
      "    Val Batch 030/99 | Loss: 0.2799 | Batch Acc: 89.66%\n",
      "    Val Batch 035/99 | Loss: 0.1507 | Batch Acc: 98.28%\n",
      "    Val Batch 040/99 | Loss: 0.5935 | Batch Acc: 89.66%\n",
      "    Val Batch 045/99 | Loss: 0.3261 | Batch Acc: 89.66%\n",
      "    Val Batch 050/99 | Loss: 0.2455 | Batch Acc: 91.38%\n",
      "    Val Batch 055/99 | Loss: 0.3297 | Batch Acc: 86.21%\n",
      "    Val Batch 060/99 | Loss: 0.2302 | Batch Acc: 93.10%\n",
      "    Val Batch 065/99 | Loss: 0.7238 | Batch Acc: 75.86%\n",
      "    Val Batch 070/99 | Loss: 0.2748 | Batch Acc: 89.66%\n",
      "    Val Batch 075/99 | Loss: 0.2505 | Batch Acc: 89.66%\n",
      "    Val Batch 080/99 | Loss: 0.2604 | Batch Acc: 94.83%\n",
      "    Val Batch 085/99 | Loss: 0.1897 | Batch Acc: 96.55%\n",
      "    Val Batch 090/99 | Loss: 0.0910 | Batch Acc: 98.28%\n",
      "    Val Batch 095/99 | Loss: 0.1327 | Batch Acc: 96.55%\n",
      "    Val Batch 099/99 | Loss: 0.0470 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.4140 | Accuracy: 84.53%\n",
      "  Current Best Acc: 84.53%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/901 | Loss: 0.7027 | CLoss: 0.5028 | FLoss: 0.3998 | LR: 2.71e-04\n",
      "  Batch 020/901 | Loss: 0.7258 | CLoss: 0.5724 | FLoss: 0.3068 | LR: 2.71e-04\n",
      "  Batch 030/901 | Loss: 1.2129 | CLoss: 0.9753 | FLoss: 0.4753 | LR: 2.71e-04\n",
      "  Batch 040/901 | Loss: 0.4956 | CLoss: 0.3050 | FLoss: 0.3811 | LR: 2.71e-04\n",
      "  Batch 050/901 | Loss: 0.5086 | CLoss: 0.3564 | FLoss: 0.3043 | LR: 2.71e-04\n",
      "  Batch 060/901 | Loss: 1.1611 | CLoss: 0.8687 | FLoss: 0.5848 | LR: 2.71e-04\n",
      "  Batch 070/901 | Loss: 0.6739 | CLoss: 0.4950 | FLoss: 0.3577 | LR: 2.71e-04\n",
      "  Batch 080/901 | Loss: 0.8038 | CLoss: 0.5153 | FLoss: 0.5769 | LR: 2.71e-04\n",
      "  Batch 090/901 | Loss: 1.0566 | CLoss: 0.7836 | FLoss: 0.5461 | LR: 2.71e-04\n",
      "  Batch 100/901 | Loss: 0.6562 | CLoss: 0.4697 | FLoss: 0.3730 | LR: 2.71e-04\n",
      "  Batch 110/901 | Loss: 1.0450 | CLoss: 0.7778 | FLoss: 0.5345 | LR: 2.71e-04\n",
      "  Batch 120/901 | Loss: 1.0989 | CLoss: 0.8425 | FLoss: 0.5128 | LR: 2.71e-04\n",
      "  Batch 130/901 | Loss: 0.7780 | CLoss: 0.5572 | FLoss: 0.4416 | LR: 2.71e-04\n",
      "  Batch 140/901 | Loss: 0.7428 | CLoss: 0.5648 | FLoss: 0.3561 | LR: 2.71e-04\n",
      "  Batch 150/901 | Loss: 0.8253 | CLoss: 0.6158 | FLoss: 0.4190 | LR: 2.71e-04\n",
      "  Batch 160/901 | Loss: 1.0134 | CLoss: 0.8401 | FLoss: 0.3464 | LR: 2.71e-04\n",
      "  Batch 170/901 | Loss: 1.0975 | CLoss: 0.9425 | FLoss: 0.3101 | LR: 2.71e-04\n",
      "  Batch 180/901 | Loss: 0.8052 | CLoss: 0.6005 | FLoss: 0.4093 | LR: 2.71e-04\n",
      "  Batch 190/901 | Loss: 0.9405 | CLoss: 0.7171 | FLoss: 0.4468 | LR: 2.71e-04\n",
      "  Batch 200/901 | Loss: 0.4062 | CLoss: 0.3399 | FLoss: 0.1327 | LR: 2.71e-04\n",
      "  Batch 210/901 | Loss: 0.7823 | CLoss: 0.6510 | FLoss: 0.2627 | LR: 2.71e-04\n",
      "  Batch 220/901 | Loss: 0.5416 | CLoss: 0.4469 | FLoss: 0.1894 | LR: 2.71e-04\n",
      "  Batch 230/901 | Loss: 0.7601 | CLoss: 0.5967 | FLoss: 0.3267 | LR: 2.71e-04\n",
      "  Batch 240/901 | Loss: 0.9164 | CLoss: 0.6629 | FLoss: 0.5071 | LR: 2.71e-04\n",
      "  Batch 250/901 | Loss: 0.8202 | CLoss: 0.5879 | FLoss: 0.4646 | LR: 2.71e-04\n",
      "  Batch 260/901 | Loss: 0.5459 | CLoss: 0.4345 | FLoss: 0.2228 | LR: 2.71e-04\n",
      "  Batch 270/901 | Loss: 1.0677 | CLoss: 0.8672 | FLoss: 0.4010 | LR: 2.71e-04\n",
      "  Batch 280/901 | Loss: 0.4751 | CLoss: 0.3506 | FLoss: 0.2489 | LR: 2.71e-04\n",
      "  Batch 290/901 | Loss: 0.6489 | CLoss: 0.5178 | FLoss: 0.2622 | LR: 2.71e-04\n",
      "  Batch 300/901 | Loss: 0.8388 | CLoss: 0.6731 | FLoss: 0.3315 | LR: 2.71e-04\n",
      "  Batch 310/901 | Loss: 0.5788 | CLoss: 0.4959 | FLoss: 0.1657 | LR: 2.71e-04\n",
      "  Batch 320/901 | Loss: 1.2136 | CLoss: 0.9505 | FLoss: 0.5261 | LR: 2.71e-04\n",
      "  Batch 330/901 | Loss: 0.7649 | CLoss: 0.6179 | FLoss: 0.2939 | LR: 2.71e-04\n",
      "  Batch 340/901 | Loss: 0.8618 | CLoss: 0.6368 | FLoss: 0.4501 | LR: 2.71e-04\n",
      "  Batch 350/901 | Loss: 0.4663 | CLoss: 0.2974 | FLoss: 0.3379 | LR: 2.71e-04\n",
      "  Batch 360/901 | Loss: 0.8892 | CLoss: 0.6040 | FLoss: 0.5704 | LR: 2.71e-04\n",
      "  Batch 370/901 | Loss: 0.8521 | CLoss: 0.6715 | FLoss: 0.3612 | LR: 2.71e-04\n",
      "  Batch 380/901 | Loss: 0.4730 | CLoss: 0.3341 | FLoss: 0.2778 | LR: 2.71e-04\n",
      "  Batch 390/901 | Loss: 0.8814 | CLoss: 0.7042 | FLoss: 0.3545 | LR: 2.71e-04\n",
      "  Batch 400/901 | Loss: 0.2876 | CLoss: 0.1770 | FLoss: 0.2212 | LR: 2.71e-04\n",
      "  Batch 410/901 | Loss: 0.7553 | CLoss: 0.4943 | FLoss: 0.5219 | LR: 2.71e-04\n",
      "  Batch 420/901 | Loss: 1.1257 | CLoss: 0.9075 | FLoss: 0.4365 | LR: 2.71e-04\n",
      "  Batch 430/901 | Loss: 1.3407 | CLoss: 1.0880 | FLoss: 0.5054 | LR: 2.71e-04\n",
      "  Batch 440/901 | Loss: 0.5662 | CLoss: 0.4108 | FLoss: 0.3109 | LR: 2.71e-04\n",
      "  Batch 450/901 | Loss: 0.6823 | CLoss: 0.5523 | FLoss: 0.2600 | LR: 2.71e-04\n",
      "  Batch 460/901 | Loss: 0.8308 | CLoss: 0.7249 | FLoss: 0.2118 | LR: 2.71e-04\n",
      "  Batch 470/901 | Loss: 0.8060 | CLoss: 0.6367 | FLoss: 0.3387 | LR: 2.71e-04\n",
      "  Batch 480/901 | Loss: 0.6975 | CLoss: 0.5760 | FLoss: 0.2430 | LR: 2.71e-04\n",
      "  Batch 490/901 | Loss: 0.7919 | CLoss: 0.6740 | FLoss: 0.2358 | LR: 2.71e-04\n",
      "  Batch 500/901 | Loss: 0.8891 | CLoss: 0.7032 | FLoss: 0.3719 | LR: 2.71e-04\n",
      "  Batch 510/901 | Loss: 0.9519 | CLoss: 0.7647 | FLoss: 0.3744 | LR: 2.71e-04\n",
      "  Batch 520/901 | Loss: 0.7138 | CLoss: 0.5055 | FLoss: 0.4167 | LR: 2.71e-04\n",
      "  Batch 530/901 | Loss: 0.7649 | CLoss: 0.5809 | FLoss: 0.3680 | LR: 2.71e-04\n",
      "  Batch 540/901 | Loss: 0.8925 | CLoss: 0.6857 | FLoss: 0.4136 | LR: 2.71e-04\n",
      "  Batch 550/901 | Loss: 0.7547 | CLoss: 0.6228 | FLoss: 0.2638 | LR: 2.71e-04\n",
      "  Batch 560/901 | Loss: 0.6537 | CLoss: 0.5043 | FLoss: 0.2988 | LR: 2.71e-04\n",
      "  Batch 570/901 | Loss: 0.5645 | CLoss: 0.4204 | FLoss: 0.2881 | LR: 2.71e-04\n",
      "  Batch 580/901 | Loss: 0.9000 | CLoss: 0.6718 | FLoss: 0.4565 | LR: 2.71e-04\n",
      "  Batch 590/901 | Loss: 0.9105 | CLoss: 0.7559 | FLoss: 0.3092 | LR: 2.71e-04\n",
      "  Batch 600/901 | Loss: 0.7966 | CLoss: 0.6835 | FLoss: 0.2263 | LR: 2.71e-04\n",
      "  Batch 610/901 | Loss: 0.9287 | CLoss: 0.6511 | FLoss: 0.5553 | LR: 2.71e-04\n",
      "  Batch 620/901 | Loss: 0.6683 | CLoss: 0.4987 | FLoss: 0.3392 | LR: 2.71e-04\n",
      "  Batch 630/901 | Loss: 0.9089 | CLoss: 0.7159 | FLoss: 0.3859 | LR: 2.71e-04\n",
      "  Batch 640/901 | Loss: 0.8285 | CLoss: 0.6956 | FLoss: 0.2657 | LR: 2.71e-04\n",
      "  Batch 650/901 | Loss: 0.8398 | CLoss: 0.7320 | FLoss: 0.2156 | LR: 2.71e-04\n",
      "  Batch 660/901 | Loss: 0.6144 | CLoss: 0.5134 | FLoss: 0.2020 | LR: 2.71e-04\n",
      "  Batch 670/901 | Loss: 0.9237 | CLoss: 0.8085 | FLoss: 0.2302 | LR: 2.71e-04\n",
      "  Batch 680/901 | Loss: 0.7096 | CLoss: 0.5971 | FLoss: 0.2250 | LR: 2.71e-04\n",
      "  Batch 690/901 | Loss: 0.8810 | CLoss: 0.7214 | FLoss: 0.3192 | LR: 2.71e-04\n",
      "  Batch 700/901 | Loss: 0.8672 | CLoss: 0.6889 | FLoss: 0.3566 | LR: 2.71e-04\n",
      "  Batch 710/901 | Loss: 1.4027 | CLoss: 1.0398 | FLoss: 0.7258 | LR: 2.71e-04\n",
      "  Batch 720/901 | Loss: 0.8309 | CLoss: 0.6479 | FLoss: 0.3661 | LR: 2.71e-04\n",
      "  Batch 730/901 | Loss: 0.5983 | CLoss: 0.4633 | FLoss: 0.2700 | LR: 2.71e-04\n",
      "  Batch 740/901 | Loss: 0.3125 | CLoss: 0.2498 | FLoss: 0.1253 | LR: 2.71e-04\n",
      "  Batch 750/901 | Loss: 0.2234 | CLoss: 0.1503 | FLoss: 0.1463 | LR: 2.71e-04\n",
      "  Batch 760/901 | Loss: 0.7241 | CLoss: 0.5890 | FLoss: 0.2701 | LR: 2.71e-04\n",
      "  Batch 770/901 | Loss: 0.5968 | CLoss: 0.4598 | FLoss: 0.2740 | LR: 2.71e-04\n",
      "  Batch 780/901 | Loss: 0.8371 | CLoss: 0.6234 | FLoss: 0.4273 | LR: 2.71e-04\n",
      "  Batch 790/901 | Loss: 0.5356 | CLoss: 0.4293 | FLoss: 0.2125 | LR: 2.71e-04\n",
      "  Batch 800/901 | Loss: 0.8198 | CLoss: 0.6619 | FLoss: 0.3157 | LR: 2.71e-04\n",
      "  Batch 810/901 | Loss: 1.4730 | CLoss: 1.1752 | FLoss: 0.5956 | LR: 2.71e-04\n",
      "  Batch 820/901 | Loss: 1.1094 | CLoss: 0.9448 | FLoss: 0.3291 | LR: 2.71e-04\n",
      "  Batch 830/901 | Loss: 0.7687 | CLoss: 0.5677 | FLoss: 0.4019 | LR: 2.71e-04\n",
      "  Batch 840/901 | Loss: 0.5276 | CLoss: 0.4339 | FLoss: 0.1874 | LR: 2.71e-04\n",
      "  Batch 850/901 | Loss: 0.9710 | CLoss: 0.7775 | FLoss: 0.3870 | LR: 2.71e-04\n",
      "  Batch 860/901 | Loss: 0.6014 | CLoss: 0.5542 | FLoss: 0.0944 | LR: 2.71e-04\n",
      "  Batch 870/901 | Loss: 0.6915 | CLoss: 0.5414 | FLoss: 0.3001 | LR: 2.71e-04\n",
      "  Batch 880/901 | Loss: 0.4631 | CLoss: 0.3203 | FLoss: 0.2855 | LR: 2.71e-04\n",
      "  Batch 890/901 | Loss: 0.7209 | CLoss: 0.5433 | FLoss: 0.3554 | LR: 2.71e-04\n",
      "  Batch 900/901 | Loss: 0.4949 | CLoss: 0.3600 | FLoss: 0.2697 | LR: 2.71e-04\n",
      "  Batch 901/901 | Loss: 0.3862 | CLoss: 0.1202 | FLoss: 0.5321 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.7958\n",
      "  Last Batch Loss: 0.3862\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 1.0520 | Batch Acc: 68.97%\n",
      "    Val Batch 010/99 | Loss: 0.0818 | Batch Acc: 96.55%\n",
      "    Val Batch 015/99 | Loss: 0.2336 | Batch Acc: 91.38%\n",
      "    Val Batch 020/99 | Loss: 0.9757 | Batch Acc: 27.59%\n",
      "    Val Batch 025/99 | Loss: 0.3394 | Batch Acc: 89.66%\n",
      "    Val Batch 030/99 | Loss: 0.2393 | Batch Acc: 91.38%\n",
      "    Val Batch 035/99 | Loss: 0.1242 | Batch Acc: 94.83%\n",
      "    Val Batch 040/99 | Loss: 0.3691 | Batch Acc: 100.00%\n",
      "    Val Batch 045/99 | Loss: 0.1924 | Batch Acc: 93.10%\n",
      "    Val Batch 050/99 | Loss: 0.3174 | Batch Acc: 87.93%\n",
      "    Val Batch 055/99 | Loss: 0.2490 | Batch Acc: 87.93%\n",
      "    Val Batch 060/99 | Loss: 0.2434 | Batch Acc: 91.38%\n",
      "    Val Batch 065/99 | Loss: 0.4068 | Batch Acc: 86.21%\n",
      "    Val Batch 070/99 | Loss: 0.1371 | Batch Acc: 98.28%\n",
      "    Val Batch 075/99 | Loss: 0.3472 | Batch Acc: 89.66%\n",
      "    Val Batch 080/99 | Loss: 0.1321 | Batch Acc: 96.55%\n",
      "    Val Batch 085/99 | Loss: 0.0979 | Batch Acc: 98.28%\n",
      "    Val Batch 090/99 | Loss: 0.1803 | Batch Acc: 93.10%\n",
      "    Val Batch 095/99 | Loss: 0.0693 | Batch Acc: 98.28%\n",
      "    Val Batch 099/99 | Loss: 0.0975 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.3666 | Accuracy: 85.97%\n",
      "  Current Best Acc: 85.97%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/901 | Loss: 1.0282 | CLoss: 0.7669 | FLoss: 0.5226 | LR: 1.96e-04\n",
      "  Batch 020/901 | Loss: 0.6740 | CLoss: 0.5135 | FLoss: 0.3211 | LR: 1.96e-04\n",
      "  Batch 030/901 | Loss: 0.9024 | CLoss: 0.7422 | FLoss: 0.3203 | LR: 1.96e-04\n",
      "  Batch 040/901 | Loss: 0.9773 | CLoss: 0.8297 | FLoss: 0.2951 | LR: 1.96e-04\n",
      "  Batch 050/901 | Loss: 0.6068 | CLoss: 0.4727 | FLoss: 0.2682 | LR: 1.96e-04\n",
      "  Batch 060/901 | Loss: 0.8649 | CLoss: 0.6666 | FLoss: 0.3966 | LR: 1.96e-04\n",
      "  Batch 070/901 | Loss: 0.9065 | CLoss: 0.7187 | FLoss: 0.3756 | LR: 1.96e-04\n",
      "  Batch 080/901 | Loss: 0.6283 | CLoss: 0.5302 | FLoss: 0.1961 | LR: 1.96e-04\n",
      "  Batch 090/901 | Loss: 0.5961 | CLoss: 0.3686 | FLoss: 0.4550 | LR: 1.96e-04\n",
      "  Batch 100/901 | Loss: 0.6703 | CLoss: 0.5466 | FLoss: 0.2474 | LR: 1.96e-04\n",
      "  Batch 110/901 | Loss: 1.5141 | CLoss: 1.2812 | FLoss: 0.4658 | LR: 1.96e-04\n",
      "  Batch 120/901 | Loss: 0.2680 | CLoss: 0.1676 | FLoss: 0.2009 | LR: 1.96e-04\n",
      "  Batch 130/901 | Loss: 0.3311 | CLoss: 0.2771 | FLoss: 0.1079 | LR: 1.96e-04\n",
      "  Batch 140/901 | Loss: 0.6767 | CLoss: 0.5135 | FLoss: 0.3264 | LR: 1.96e-04\n",
      "  Batch 150/901 | Loss: 0.4530 | CLoss: 0.3146 | FLoss: 0.2769 | LR: 1.96e-04\n",
      "  Batch 160/901 | Loss: 0.5046 | CLoss: 0.4156 | FLoss: 0.1780 | LR: 1.96e-04\n",
      "  Batch 170/901 | Loss: 0.7364 | CLoss: 0.5931 | FLoss: 0.2868 | LR: 1.96e-04\n",
      "  Batch 180/901 | Loss: 0.7073 | CLoss: 0.5969 | FLoss: 0.2207 | LR: 1.96e-04\n",
      "  Batch 190/901 | Loss: 0.5626 | CLoss: 0.4304 | FLoss: 0.2644 | LR: 1.96e-04\n",
      "  Batch 200/901 | Loss: 0.7034 | CLoss: 0.5611 | FLoss: 0.2845 | LR: 1.96e-04\n",
      "  Batch 210/901 | Loss: 0.5545 | CLoss: 0.3954 | FLoss: 0.3182 | LR: 1.96e-04\n",
      "  Batch 220/901 | Loss: 0.7645 | CLoss: 0.6137 | FLoss: 0.3015 | LR: 1.96e-04\n",
      "  Batch 230/901 | Loss: 0.4833 | CLoss: 0.3678 | FLoss: 0.2310 | LR: 1.96e-04\n",
      "  Batch 240/901 | Loss: 1.0293 | CLoss: 0.8435 | FLoss: 0.3715 | LR: 1.96e-04\n",
      "  Batch 250/901 | Loss: 0.7508 | CLoss: 0.6105 | FLoss: 0.2808 | LR: 1.96e-04\n",
      "  Batch 260/901 | Loss: 0.3832 | CLoss: 0.3042 | FLoss: 0.1582 | LR: 1.96e-04\n",
      "  Batch 270/901 | Loss: 0.3631 | CLoss: 0.2438 | FLoss: 0.2386 | LR: 1.96e-04\n",
      "  Batch 280/901 | Loss: 0.9593 | CLoss: 0.7111 | FLoss: 0.4965 | LR: 1.96e-04\n",
      "  Batch 290/901 | Loss: 0.2509 | CLoss: 0.1551 | FLoss: 0.1915 | LR: 1.96e-04\n",
      "  Batch 300/901 | Loss: 0.7215 | CLoss: 0.5482 | FLoss: 0.3465 | LR: 1.96e-04\n",
      "  Batch 310/901 | Loss: 1.1703 | CLoss: 0.9110 | FLoss: 0.5186 | LR: 1.96e-04\n",
      "  Batch 320/901 | Loss: 0.7669 | CLoss: 0.5732 | FLoss: 0.3872 | LR: 1.96e-04\n",
      "  Batch 330/901 | Loss: 0.5911 | CLoss: 0.5125 | FLoss: 0.1572 | LR: 1.96e-04\n",
      "  Batch 340/901 | Loss: 0.6226 | CLoss: 0.4411 | FLoss: 0.3628 | LR: 1.96e-04\n",
      "  Batch 350/901 | Loss: 0.3280 | CLoss: 0.2405 | FLoss: 0.1751 | LR: 1.96e-04\n",
      "  Batch 360/901 | Loss: 0.9084 | CLoss: 0.7567 | FLoss: 0.3033 | LR: 1.96e-04\n",
      "  Batch 370/901 | Loss: 0.6140 | CLoss: 0.4816 | FLoss: 0.2648 | LR: 1.96e-04\n",
      "  Batch 380/901 | Loss: 0.5422 | CLoss: 0.3735 | FLoss: 0.3374 | LR: 1.96e-04\n",
      "  Batch 390/901 | Loss: 1.4681 | CLoss: 1.1785 | FLoss: 0.5793 | LR: 1.96e-04\n",
      "  Batch 400/901 | Loss: 1.0667 | CLoss: 0.8362 | FLoss: 0.4609 | LR: 1.96e-04\n",
      "  Batch 410/901 | Loss: 0.5640 | CLoss: 0.4742 | FLoss: 0.1796 | LR: 1.96e-04\n",
      "  Batch 420/901 | Loss: 1.0561 | CLoss: 0.8511 | FLoss: 0.4101 | LR: 1.96e-04\n",
      "  Batch 430/901 | Loss: 0.5088 | CLoss: 0.3562 | FLoss: 0.3053 | LR: 1.96e-04\n",
      "  Batch 440/901 | Loss: 0.7868 | CLoss: 0.6087 | FLoss: 0.3561 | LR: 1.96e-04\n",
      "  Batch 450/901 | Loss: 0.9346 | CLoss: 0.7327 | FLoss: 0.4037 | LR: 1.96e-04\n",
      "  Batch 460/901 | Loss: 0.3659 | CLoss: 0.2576 | FLoss: 0.2166 | LR: 1.96e-04\n",
      "  Batch 470/901 | Loss: 0.7906 | CLoss: 0.6337 | FLoss: 0.3138 | LR: 1.96e-04\n",
      "  Batch 480/901 | Loss: 0.6491 | CLoss: 0.5173 | FLoss: 0.2636 | LR: 1.96e-04\n",
      "  Batch 490/901 | Loss: 0.8539 | CLoss: 0.7176 | FLoss: 0.2726 | LR: 1.96e-04\n",
      "  Batch 500/901 | Loss: 0.5868 | CLoss: 0.4407 | FLoss: 0.2922 | LR: 1.96e-04\n",
      "  Batch 510/901 | Loss: 0.6484 | CLoss: 0.4992 | FLoss: 0.2984 | LR: 1.96e-04\n",
      "  Batch 520/901 | Loss: 0.5551 | CLoss: 0.4512 | FLoss: 0.2078 | LR: 1.96e-04\n",
      "  Batch 530/901 | Loss: 0.6474 | CLoss: 0.5555 | FLoss: 0.1836 | LR: 1.96e-04\n",
      "  Batch 540/901 | Loss: 1.0557 | CLoss: 0.8488 | FLoss: 0.4138 | LR: 1.96e-04\n",
      "  Batch 550/901 | Loss: 0.7169 | CLoss: 0.5956 | FLoss: 0.2426 | LR: 1.96e-04\n",
      "  Batch 560/901 | Loss: 0.5812 | CLoss: 0.4685 | FLoss: 0.2256 | LR: 1.96e-04\n",
      "  Batch 570/901 | Loss: 0.8184 | CLoss: 0.6187 | FLoss: 0.3995 | LR: 1.96e-04\n",
      "  Batch 580/901 | Loss: 0.5672 | CLoss: 0.4557 | FLoss: 0.2230 | LR: 1.96e-04\n",
      "  Batch 590/901 | Loss: 0.7125 | CLoss: 0.5509 | FLoss: 0.3232 | LR: 1.96e-04\n",
      "  Batch 600/901 | Loss: 0.6148 | CLoss: 0.5108 | FLoss: 0.2080 | LR: 1.96e-04\n",
      "  Batch 610/901 | Loss: 0.6643 | CLoss: 0.5412 | FLoss: 0.2461 | LR: 1.96e-04\n",
      "  Batch 620/901 | Loss: 0.6216 | CLoss: 0.4859 | FLoss: 0.2714 | LR: 1.96e-04\n",
      "  Batch 630/901 | Loss: 0.4586 | CLoss: 0.3835 | FLoss: 0.1501 | LR: 1.96e-04\n",
      "  Batch 640/901 | Loss: 0.1935 | CLoss: 0.1191 | FLoss: 0.1488 | LR: 1.96e-04\n",
      "  Batch 650/901 | Loss: 0.9138 | CLoss: 0.6785 | FLoss: 0.4707 | LR: 1.96e-04\n",
      "  Batch 660/901 | Loss: 0.4076 | CLoss: 0.2852 | FLoss: 0.2448 | LR: 1.96e-04\n",
      "  Batch 670/901 | Loss: 0.6651 | CLoss: 0.4526 | FLoss: 0.4250 | LR: 1.96e-04\n",
      "  Batch 680/901 | Loss: 0.8546 | CLoss: 0.6510 | FLoss: 0.4072 | LR: 1.96e-04\n",
      "  Batch 690/901 | Loss: 0.8139 | CLoss: 0.6967 | FLoss: 0.2344 | LR: 1.96e-04\n",
      "  Batch 700/901 | Loss: 0.7559 | CLoss: 0.6091 | FLoss: 0.2935 | LR: 1.96e-04\n",
      "  Batch 710/901 | Loss: 0.5285 | CLoss: 0.3438 | FLoss: 0.3693 | LR: 1.96e-04\n",
      "  Batch 720/901 | Loss: 0.4966 | CLoss: 0.3075 | FLoss: 0.3781 | LR: 1.96e-04\n",
      "  Batch 730/901 | Loss: 0.4927 | CLoss: 0.3841 | FLoss: 0.2174 | LR: 1.96e-04\n",
      "  Batch 740/901 | Loss: 0.5703 | CLoss: 0.4294 | FLoss: 0.2819 | LR: 1.96e-04\n",
      "  Batch 750/901 | Loss: 0.7769 | CLoss: 0.6550 | FLoss: 0.2438 | LR: 1.96e-04\n",
      "  Batch 760/901 | Loss: 0.5929 | CLoss: 0.4260 | FLoss: 0.3337 | LR: 1.96e-04\n",
      "  Batch 770/901 | Loss: 0.6825 | CLoss: 0.5616 | FLoss: 0.2417 | LR: 1.96e-04\n",
      "  Batch 780/901 | Loss: 0.6770 | CLoss: 0.4900 | FLoss: 0.3739 | LR: 1.96e-04\n",
      "  Batch 790/901 | Loss: 0.6928 | CLoss: 0.5472 | FLoss: 0.2913 | LR: 1.96e-04\n",
      "  Batch 800/901 | Loss: 0.5095 | CLoss: 0.4128 | FLoss: 0.1934 | LR: 1.96e-04\n",
      "  Batch 810/901 | Loss: 0.7210 | CLoss: 0.6182 | FLoss: 0.2056 | LR: 1.96e-04\n",
      "  Batch 820/901 | Loss: 0.6830 | CLoss: 0.5722 | FLoss: 0.2215 | LR: 1.96e-04\n",
      "  Batch 830/901 | Loss: 0.8055 | CLoss: 0.6310 | FLoss: 0.3489 | LR: 1.96e-04\n",
      "  Batch 840/901 | Loss: 0.7994 | CLoss: 0.6241 | FLoss: 0.3507 | LR: 1.96e-04\n",
      "  Batch 850/901 | Loss: 0.7227 | CLoss: 0.5349 | FLoss: 0.3755 | LR: 1.96e-04\n",
      "  Batch 860/901 | Loss: 0.6712 | CLoss: 0.5350 | FLoss: 0.2724 | LR: 1.96e-04\n",
      "  Batch 870/901 | Loss: 0.7898 | CLoss: 0.6504 | FLoss: 0.2788 | LR: 1.96e-04\n",
      "  Batch 880/901 | Loss: 0.5449 | CLoss: 0.3806 | FLoss: 0.3286 | LR: 1.96e-04\n",
      "  Batch 890/901 | Loss: 0.6578 | CLoss: 0.5324 | FLoss: 0.2509 | LR: 1.96e-04\n",
      "  Batch 900/901 | Loss: 0.5866 | CLoss: 0.4707 | FLoss: 0.2318 | LR: 1.96e-04\n",
      "  Batch 901/901 | Loss: 0.7938 | CLoss: 0.4888 | FLoss: 0.6099 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.7156\n",
      "  Last Batch Loss: 0.7938\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.8061 | Batch Acc: 67.24%\n",
      "    Val Batch 010/99 | Loss: 0.2067 | Batch Acc: 91.38%\n",
      "    Val Batch 015/99 | Loss: 0.4552 | Batch Acc: 84.48%\n",
      "    Val Batch 020/99 | Loss: 1.1373 | Batch Acc: 25.86%\n",
      "    Val Batch 025/99 | Loss: 0.4580 | Batch Acc: 89.66%\n",
      "    Val Batch 030/99 | Loss: 0.2845 | Batch Acc: 87.93%\n",
      "    Val Batch 035/99 | Loss: 0.2263 | Batch Acc: 89.66%\n",
      "    Val Batch 040/99 | Loss: 0.5218 | Batch Acc: 96.55%\n",
      "    Val Batch 045/99 | Loss: 0.1012 | Batch Acc: 98.28%\n",
      "    Val Batch 050/99 | Loss: 0.1256 | Batch Acc: 98.28%\n",
      "    Val Batch 055/99 | Loss: 0.2608 | Batch Acc: 87.93%\n",
      "    Val Batch 060/99 | Loss: 0.2014 | Batch Acc: 93.10%\n",
      "    Val Batch 065/99 | Loss: 0.3726 | Batch Acc: 86.21%\n",
      "    Val Batch 070/99 | Loss: 0.1565 | Batch Acc: 94.83%\n",
      "    Val Batch 075/99 | Loss: 0.0804 | Batch Acc: 96.55%\n",
      "    Val Batch 080/99 | Loss: 0.2840 | Batch Acc: 91.38%\n",
      "    Val Batch 085/99 | Loss: 0.0226 | Batch Acc: 100.00%\n",
      "    Val Batch 090/99 | Loss: 0.1851 | Batch Acc: 94.83%\n",
      "    Val Batch 095/99 | Loss: 0.2292 | Batch Acc: 93.10%\n",
      "    Val Batch 099/99 | Loss: 0.4692 | Batch Acc: 90.91%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.3444 | Accuracy: 86.41%\n",
      "  Current Best Acc: 86.41%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/901 | Loss: 0.6373 | CLoss: 0.4893 | FLoss: 0.2960 | LR: 1.04e-04\n",
      "  Batch 020/901 | Loss: 1.1946 | CLoss: 1.0074 | FLoss: 0.3744 | LR: 1.04e-04\n",
      "  Batch 030/901 | Loss: 0.8964 | CLoss: 0.6874 | FLoss: 0.4179 | LR: 1.04e-04\n",
      "  Batch 040/901 | Loss: 0.6698 | CLoss: 0.5951 | FLoss: 0.1493 | LR: 1.04e-04\n",
      "  Batch 050/901 | Loss: 0.7929 | CLoss: 0.5763 | FLoss: 0.4330 | LR: 1.04e-04\n",
      "  Batch 060/901 | Loss: 0.4798 | CLoss: 0.3861 | FLoss: 0.1873 | LR: 1.04e-04\n",
      "  Batch 070/901 | Loss: 0.6887 | CLoss: 0.5704 | FLoss: 0.2364 | LR: 1.04e-04\n",
      "  Batch 080/901 | Loss: 0.6469 | CLoss: 0.4964 | FLoss: 0.3009 | LR: 1.04e-04\n",
      "  Batch 090/901 | Loss: 0.4850 | CLoss: 0.2934 | FLoss: 0.3832 | LR: 1.04e-04\n",
      "  Batch 100/901 | Loss: 0.8200 | CLoss: 0.6601 | FLoss: 0.3198 | LR: 1.04e-04\n",
      "  Batch 110/901 | Loss: 1.2035 | CLoss: 0.9982 | FLoss: 0.4106 | LR: 1.04e-04\n",
      "  Batch 120/901 | Loss: 0.8376 | CLoss: 0.6390 | FLoss: 0.3972 | LR: 1.04e-04\n",
      "  Batch 130/901 | Loss: 0.5139 | CLoss: 0.4039 | FLoss: 0.2200 | LR: 1.04e-04\n",
      "  Batch 140/901 | Loss: 0.4841 | CLoss: 0.3659 | FLoss: 0.2364 | LR: 1.04e-04\n",
      "  Batch 150/901 | Loss: 0.5320 | CLoss: 0.3931 | FLoss: 0.2777 | LR: 1.04e-04\n",
      "  Batch 160/901 | Loss: 0.5998 | CLoss: 0.4233 | FLoss: 0.3530 | LR: 1.04e-04\n",
      "  Batch 170/901 | Loss: 0.6528 | CLoss: 0.5232 | FLoss: 0.2591 | LR: 1.04e-04\n",
      "  Batch 180/901 | Loss: 0.4246 | CLoss: 0.2819 | FLoss: 0.2853 | LR: 1.04e-04\n",
      "  Batch 190/901 | Loss: 0.5479 | CLoss: 0.3826 | FLoss: 0.3305 | LR: 1.04e-04\n",
      "  Batch 200/901 | Loss: 0.3662 | CLoss: 0.2853 | FLoss: 0.1616 | LR: 1.04e-04\n",
      "  Batch 210/901 | Loss: 0.8183 | CLoss: 0.6705 | FLoss: 0.2957 | LR: 1.04e-04\n",
      "  Batch 220/901 | Loss: 0.4867 | CLoss: 0.3357 | FLoss: 0.3019 | LR: 1.04e-04\n",
      "  Batch 230/901 | Loss: 0.3931 | CLoss: 0.2964 | FLoss: 0.1934 | LR: 1.04e-04\n",
      "  Batch 240/901 | Loss: 0.7749 | CLoss: 0.6078 | FLoss: 0.3343 | LR: 1.04e-04\n",
      "  Batch 250/901 | Loss: 0.7006 | CLoss: 0.5945 | FLoss: 0.2122 | LR: 1.04e-04\n",
      "  Batch 260/901 | Loss: 0.6995 | CLoss: 0.5059 | FLoss: 0.3872 | LR: 1.04e-04\n",
      "  Batch 270/901 | Loss: 0.7375 | CLoss: 0.6054 | FLoss: 0.2643 | LR: 1.04e-04\n",
      "  Batch 280/901 | Loss: 0.9614 | CLoss: 0.7849 | FLoss: 0.3529 | LR: 1.04e-04\n",
      "  Batch 290/901 | Loss: 0.5160 | CLoss: 0.3687 | FLoss: 0.2946 | LR: 1.04e-04\n",
      "  Batch 300/901 | Loss: 0.7782 | CLoss: 0.6109 | FLoss: 0.3347 | LR: 1.04e-04\n",
      "  Batch 310/901 | Loss: 0.4312 | CLoss: 0.2744 | FLoss: 0.3136 | LR: 1.04e-04\n",
      "  Batch 320/901 | Loss: 1.0038 | CLoss: 0.8191 | FLoss: 0.3694 | LR: 1.04e-04\n",
      "  Batch 330/901 | Loss: 0.6313 | CLoss: 0.4686 | FLoss: 0.3254 | LR: 1.04e-04\n",
      "  Batch 340/901 | Loss: 0.6370 | CLoss: 0.5094 | FLoss: 0.2551 | LR: 1.04e-04\n",
      "  Batch 350/901 | Loss: 0.5176 | CLoss: 0.4158 | FLoss: 0.2035 | LR: 1.04e-04\n",
      "  Batch 360/901 | Loss: 0.9218 | CLoss: 0.7591 | FLoss: 0.3254 | LR: 1.04e-04\n",
      "  Batch 370/901 | Loss: 0.5325 | CLoss: 0.4320 | FLoss: 0.2010 | LR: 1.04e-04\n",
      "  Batch 380/901 | Loss: 0.7415 | CLoss: 0.5572 | FLoss: 0.3686 | LR: 1.04e-04\n",
      "  Batch 390/901 | Loss: 0.4911 | CLoss: 0.3626 | FLoss: 0.2569 | LR: 1.04e-04\n",
      "  Batch 400/901 | Loss: 0.7773 | CLoss: 0.5669 | FLoss: 0.4208 | LR: 1.04e-04\n",
      "  Batch 410/901 | Loss: 0.8235 | CLoss: 0.6831 | FLoss: 0.2808 | LR: 1.04e-04\n",
      "  Batch 420/901 | Loss: 0.4497 | CLoss: 0.2975 | FLoss: 0.3043 | LR: 1.04e-04\n",
      "  Batch 430/901 | Loss: 0.5263 | CLoss: 0.3926 | FLoss: 0.2675 | LR: 1.04e-04\n",
      "  Batch 440/901 | Loss: 0.6475 | CLoss: 0.4748 | FLoss: 0.3454 | LR: 1.04e-04\n",
      "  Batch 450/901 | Loss: 0.5500 | CLoss: 0.4254 | FLoss: 0.2492 | LR: 1.04e-04\n",
      "  Batch 460/901 | Loss: 1.0625 | CLoss: 0.8782 | FLoss: 0.3686 | LR: 1.04e-04\n",
      "  Batch 470/901 | Loss: 1.0452 | CLoss: 0.7950 | FLoss: 0.5004 | LR: 1.04e-04\n",
      "  Batch 480/901 | Loss: 0.8694 | CLoss: 0.7396 | FLoss: 0.2597 | LR: 1.04e-04\n",
      "  Batch 490/901 | Loss: 0.9857 | CLoss: 0.7409 | FLoss: 0.4896 | LR: 1.04e-04\n",
      "  Batch 500/901 | Loss: 0.6736 | CLoss: 0.5691 | FLoss: 0.2089 | LR: 1.04e-04\n",
      "  Batch 510/901 | Loss: 0.6354 | CLoss: 0.5008 | FLoss: 0.2692 | LR: 1.04e-04\n",
      "  Batch 520/901 | Loss: 0.3389 | CLoss: 0.2330 | FLoss: 0.2118 | LR: 1.04e-04\n",
      "  Batch 530/901 | Loss: 0.7596 | CLoss: 0.6269 | FLoss: 0.2655 | LR: 1.04e-04\n",
      "  Batch 540/901 | Loss: 0.8125 | CLoss: 0.6203 | FLoss: 0.3844 | LR: 1.04e-04\n",
      "  Batch 550/901 | Loss: 0.5766 | CLoss: 0.4700 | FLoss: 0.2131 | LR: 1.04e-04\n",
      "  Batch 560/901 | Loss: 0.8554 | CLoss: 0.7406 | FLoss: 0.2296 | LR: 1.04e-04\n",
      "  Batch 570/901 | Loss: 0.7421 | CLoss: 0.6337 | FLoss: 0.2169 | LR: 1.04e-04\n",
      "  Batch 580/901 | Loss: 0.6838 | CLoss: 0.5496 | FLoss: 0.2684 | LR: 1.04e-04\n",
      "  Batch 590/901 | Loss: 0.2197 | CLoss: 0.1224 | FLoss: 0.1946 | LR: 1.04e-04\n",
      "  Batch 600/901 | Loss: 0.7282 | CLoss: 0.6252 | FLoss: 0.2059 | LR: 1.04e-04\n",
      "  Batch 610/901 | Loss: 0.6502 | CLoss: 0.5131 | FLoss: 0.2743 | LR: 1.04e-04\n",
      "  Batch 620/901 | Loss: 0.9044 | CLoss: 0.6922 | FLoss: 0.4243 | LR: 1.04e-04\n",
      "  Batch 630/901 | Loss: 0.7738 | CLoss: 0.5855 | FLoss: 0.3766 | LR: 1.04e-04\n",
      "  Batch 640/901 | Loss: 0.2943 | CLoss: 0.2413 | FLoss: 0.1061 | LR: 1.04e-04\n",
      "  Batch 650/901 | Loss: 0.6930 | CLoss: 0.5620 | FLoss: 0.2619 | LR: 1.04e-04\n",
      "  Batch 660/901 | Loss: 1.0869 | CLoss: 0.8179 | FLoss: 0.5380 | LR: 1.04e-04\n",
      "  Batch 670/901 | Loss: 0.7114 | CLoss: 0.6061 | FLoss: 0.2106 | LR: 1.04e-04\n",
      "  Batch 680/901 | Loss: 0.5110 | CLoss: 0.3715 | FLoss: 0.2791 | LR: 1.04e-04\n",
      "  Batch 690/901 | Loss: 0.8318 | CLoss: 0.5983 | FLoss: 0.4670 | LR: 1.04e-04\n",
      "  Batch 700/901 | Loss: 0.7717 | CLoss: 0.6014 | FLoss: 0.3407 | LR: 1.04e-04\n",
      "  Batch 710/901 | Loss: 0.4633 | CLoss: 0.2968 | FLoss: 0.3330 | LR: 1.04e-04\n",
      "  Batch 720/901 | Loss: 0.6571 | CLoss: 0.5342 | FLoss: 0.2458 | LR: 1.04e-04\n",
      "  Batch 730/901 | Loss: 1.0556 | CLoss: 0.8048 | FLoss: 0.5016 | LR: 1.04e-04\n",
      "  Batch 740/901 | Loss: 0.7112 | CLoss: 0.5537 | FLoss: 0.3150 | LR: 1.04e-04\n",
      "  Batch 750/901 | Loss: 0.3829 | CLoss: 0.2923 | FLoss: 0.1811 | LR: 1.04e-04\n",
      "  Batch 760/901 | Loss: 0.4403 | CLoss: 0.3006 | FLoss: 0.2793 | LR: 1.04e-04\n",
      "  Batch 770/901 | Loss: 0.8628 | CLoss: 0.7021 | FLoss: 0.3214 | LR: 1.04e-04\n",
      "  Batch 780/901 | Loss: 0.3755 | CLoss: 0.2989 | FLoss: 0.1533 | LR: 1.04e-04\n",
      "  Batch 790/901 | Loss: 0.4457 | CLoss: 0.2516 | FLoss: 0.3881 | LR: 1.04e-04\n",
      "  Batch 800/901 | Loss: 0.6914 | CLoss: 0.5224 | FLoss: 0.3380 | LR: 1.04e-04\n",
      "  Batch 810/901 | Loss: 0.4642 | CLoss: 0.2449 | FLoss: 0.4386 | LR: 1.04e-04\n",
      "  Batch 820/901 | Loss: 0.3767 | CLoss: 0.2894 | FLoss: 0.1747 | LR: 1.04e-04\n",
      "  Batch 830/901 | Loss: 0.8093 | CLoss: 0.6951 | FLoss: 0.2284 | LR: 1.04e-04\n",
      "  Batch 840/901 | Loss: 0.6155 | CLoss: 0.5238 | FLoss: 0.1833 | LR: 1.04e-04\n",
      "  Batch 850/901 | Loss: 0.5922 | CLoss: 0.4335 | FLoss: 0.3174 | LR: 1.04e-04\n",
      "  Batch 860/901 | Loss: 0.7727 | CLoss: 0.6160 | FLoss: 0.3135 | LR: 1.04e-04\n",
      "  Batch 870/901 | Loss: 0.9734 | CLoss: 0.7066 | FLoss: 0.5336 | LR: 1.04e-04\n",
      "  Batch 880/901 | Loss: 0.3916 | CLoss: 0.2981 | FLoss: 0.1871 | LR: 1.04e-04\n",
      "  Batch 890/901 | Loss: 0.7383 | CLoss: 0.5615 | FLoss: 0.3535 | LR: 1.04e-04\n",
      "  Batch 900/901 | Loss: 0.7038 | CLoss: 0.5781 | FLoss: 0.2515 | LR: 1.04e-04\n",
      "  Batch 901/901 | Loss: 0.5405 | CLoss: 0.3953 | FLoss: 0.2904 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.6419\n",
      "  Last Batch Loss: 0.5405\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.8067 | Batch Acc: 67.24%\n",
      "    Val Batch 010/99 | Loss: 0.0982 | Batch Acc: 98.28%\n",
      "    Val Batch 015/99 | Loss: 0.1872 | Batch Acc: 94.83%\n",
      "    Val Batch 020/99 | Loss: 0.7965 | Batch Acc: 86.21%\n",
      "    Val Batch 025/99 | Loss: 0.3004 | Batch Acc: 89.66%\n",
      "    Val Batch 030/99 | Loss: 0.3436 | Batch Acc: 84.48%\n",
      "    Val Batch 035/99 | Loss: 0.1398 | Batch Acc: 93.10%\n",
      "    Val Batch 040/99 | Loss: 0.5566 | Batch Acc: 67.24%\n",
      "    Val Batch 045/99 | Loss: 0.1563 | Batch Acc: 93.10%\n",
      "    Val Batch 050/99 | Loss: 0.2567 | Batch Acc: 91.38%\n",
      "    Val Batch 055/99 | Loss: 0.1973 | Batch Acc: 93.10%\n",
      "    Val Batch 060/99 | Loss: 0.1495 | Batch Acc: 94.83%\n",
      "    Val Batch 065/99 | Loss: 0.4364 | Batch Acc: 84.48%\n",
      "    Val Batch 070/99 | Loss: 0.2230 | Batch Acc: 94.83%\n",
      "    Val Batch 075/99 | Loss: 0.1510 | Batch Acc: 93.10%\n",
      "    Val Batch 080/99 | Loss: 0.2247 | Batch Acc: 91.38%\n",
      "    Val Batch 085/99 | Loss: 0.2185 | Batch Acc: 94.83%\n",
      "    Val Batch 090/99 | Loss: 0.1970 | Batch Acc: 91.38%\n",
      "    Val Batch 095/99 | Loss: 0.1023 | Batch Acc: 98.28%\n",
      "    Val Batch 099/99 | Loss: 0.0627 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.3046 | Accuracy: 89.34%\n",
      "  Current Best Acc: 89.34%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/901 | Loss: 0.4611 | CLoss: 0.2907 | FLoss: 0.3409 | LR: 2.86e-05\n",
      "  Batch 020/901 | Loss: 0.7669 | CLoss: 0.6552 | FLoss: 0.2233 | LR: 2.86e-05\n",
      "  Batch 030/901 | Loss: 0.8978 | CLoss: 0.7402 | FLoss: 0.3151 | LR: 2.86e-05\n",
      "  Batch 040/901 | Loss: 0.4686 | CLoss: 0.3586 | FLoss: 0.2200 | LR: 2.86e-05\n",
      "  Batch 050/901 | Loss: 0.7183 | CLoss: 0.6140 | FLoss: 0.2087 | LR: 2.86e-05\n",
      "  Batch 060/901 | Loss: 0.8011 | CLoss: 0.6676 | FLoss: 0.2670 | LR: 2.86e-05\n",
      "  Batch 070/901 | Loss: 0.5532 | CLoss: 0.4236 | FLoss: 0.2591 | LR: 2.86e-05\n",
      "  Batch 080/901 | Loss: 0.4723 | CLoss: 0.3803 | FLoss: 0.1840 | LR: 2.86e-05\n",
      "  Batch 090/901 | Loss: 0.5365 | CLoss: 0.4104 | FLoss: 0.2523 | LR: 2.86e-05\n",
      "  Batch 100/901 | Loss: 0.4023 | CLoss: 0.3223 | FLoss: 0.1600 | LR: 2.86e-05\n",
      "  Batch 110/901 | Loss: 0.8571 | CLoss: 0.6955 | FLoss: 0.3233 | LR: 2.86e-05\n",
      "  Batch 120/901 | Loss: 0.5663 | CLoss: 0.4788 | FLoss: 0.1750 | LR: 2.86e-05\n",
      "  Batch 130/901 | Loss: 0.3565 | CLoss: 0.2998 | FLoss: 0.1134 | LR: 2.86e-05\n",
      "  Batch 140/901 | Loss: 1.3346 | CLoss: 1.0818 | FLoss: 0.5056 | LR: 2.86e-05\n",
      "  Batch 150/901 | Loss: 0.4875 | CLoss: 0.4222 | FLoss: 0.1306 | LR: 2.86e-05\n",
      "  Batch 160/901 | Loss: 0.6975 | CLoss: 0.5969 | FLoss: 0.2012 | LR: 2.86e-05\n",
      "  Batch 170/901 | Loss: 0.7791 | CLoss: 0.6066 | FLoss: 0.3450 | LR: 2.86e-05\n",
      "  Batch 180/901 | Loss: 0.4355 | CLoss: 0.3305 | FLoss: 0.2100 | LR: 2.86e-05\n",
      "  Batch 190/901 | Loss: 0.6315 | CLoss: 0.5405 | FLoss: 0.1820 | LR: 2.86e-05\n",
      "  Batch 200/901 | Loss: 1.0295 | CLoss: 0.8567 | FLoss: 0.3456 | LR: 2.86e-05\n",
      "  Batch 210/901 | Loss: 0.7729 | CLoss: 0.6060 | FLoss: 0.3337 | LR: 2.86e-05\n",
      "  Batch 220/901 | Loss: 0.2748 | CLoss: 0.2013 | FLoss: 0.1470 | LR: 2.86e-05\n",
      "  Batch 230/901 | Loss: 0.6590 | CLoss: 0.4632 | FLoss: 0.3917 | LR: 2.86e-05\n",
      "  Batch 240/901 | Loss: 0.8914 | CLoss: 0.6874 | FLoss: 0.4079 | LR: 2.86e-05\n",
      "  Batch 250/901 | Loss: 0.8198 | CLoss: 0.7072 | FLoss: 0.2251 | LR: 2.86e-05\n",
      "  Batch 260/901 | Loss: 0.2191 | CLoss: 0.1681 | FLoss: 0.1019 | LR: 2.86e-05\n",
      "  Batch 270/901 | Loss: 0.9436 | CLoss: 0.6601 | FLoss: 0.5670 | LR: 2.86e-05\n",
      "  Batch 280/901 | Loss: 0.3733 | CLoss: 0.2534 | FLoss: 0.2399 | LR: 2.86e-05\n",
      "  Batch 290/901 | Loss: 0.5934 | CLoss: 0.4496 | FLoss: 0.2876 | LR: 2.86e-05\n",
      "  Batch 300/901 | Loss: 0.6311 | CLoss: 0.5045 | FLoss: 0.2531 | LR: 2.86e-05\n",
      "  Batch 310/901 | Loss: 0.9439 | CLoss: 0.6961 | FLoss: 0.4956 | LR: 2.86e-05\n",
      "  Batch 320/901 | Loss: 1.1694 | CLoss: 0.9094 | FLoss: 0.5199 | LR: 2.86e-05\n",
      "  Batch 330/901 | Loss: 0.3694 | CLoss: 0.3133 | FLoss: 0.1123 | LR: 2.86e-05\n",
      "  Batch 340/901 | Loss: 0.5959 | CLoss: 0.4303 | FLoss: 0.3313 | LR: 2.86e-05\n",
      "  Batch 350/901 | Loss: 0.9684 | CLoss: 0.7552 | FLoss: 0.4264 | LR: 2.86e-05\n",
      "  Batch 360/901 | Loss: 0.7210 | CLoss: 0.5691 | FLoss: 0.3039 | LR: 2.86e-05\n",
      "  Batch 370/901 | Loss: 0.3191 | CLoss: 0.2359 | FLoss: 0.1664 | LR: 2.86e-05\n",
      "  Batch 380/901 | Loss: 0.7691 | CLoss: 0.5615 | FLoss: 0.4152 | LR: 2.86e-05\n",
      "  Batch 390/901 | Loss: 0.6217 | CLoss: 0.4659 | FLoss: 0.3117 | LR: 2.86e-05\n",
      "  Batch 400/901 | Loss: 0.6912 | CLoss: 0.5649 | FLoss: 0.2526 | LR: 2.86e-05\n",
      "  Batch 410/901 | Loss: 0.2834 | CLoss: 0.2402 | FLoss: 0.0863 | LR: 2.86e-05\n",
      "  Batch 420/901 | Loss: 0.3672 | CLoss: 0.2882 | FLoss: 0.1581 | LR: 2.86e-05\n",
      "  Batch 430/901 | Loss: 0.4822 | CLoss: 0.4035 | FLoss: 0.1574 | LR: 2.86e-05\n",
      "  Batch 440/901 | Loss: 0.4817 | CLoss: 0.3958 | FLoss: 0.1719 | LR: 2.86e-05\n",
      "  Batch 450/901 | Loss: 0.5738 | CLoss: 0.4724 | FLoss: 0.2028 | LR: 2.86e-05\n",
      "  Batch 460/901 | Loss: 0.5377 | CLoss: 0.4392 | FLoss: 0.1970 | LR: 2.86e-05\n",
      "  Batch 470/901 | Loss: 0.9225 | CLoss: 0.7275 | FLoss: 0.3901 | LR: 2.86e-05\n",
      "  Batch 480/901 | Loss: 0.7694 | CLoss: 0.6149 | FLoss: 0.3089 | LR: 2.86e-05\n",
      "  Batch 490/901 | Loss: 0.9730 | CLoss: 0.7276 | FLoss: 0.4908 | LR: 2.86e-05\n",
      "  Batch 500/901 | Loss: 0.5903 | CLoss: 0.4722 | FLoss: 0.2363 | LR: 2.86e-05\n",
      "  Batch 510/901 | Loss: 0.4048 | CLoss: 0.3488 | FLoss: 0.1119 | LR: 2.86e-05\n",
      "  Batch 520/901 | Loss: 0.6253 | CLoss: 0.4441 | FLoss: 0.3625 | LR: 2.86e-05\n",
      "  Batch 530/901 | Loss: 0.5526 | CLoss: 0.4255 | FLoss: 0.2542 | LR: 2.86e-05\n",
      "  Batch 540/901 | Loss: 0.5850 | CLoss: 0.4907 | FLoss: 0.1885 | LR: 2.86e-05\n",
      "  Batch 550/901 | Loss: 0.7983 | CLoss: 0.7004 | FLoss: 0.1958 | LR: 2.86e-05\n",
      "  Batch 560/901 | Loss: 0.5825 | CLoss: 0.4525 | FLoss: 0.2598 | LR: 2.86e-05\n",
      "  Batch 570/901 | Loss: 0.2768 | CLoss: 0.2193 | FLoss: 0.1149 | LR: 2.86e-05\n",
      "  Batch 580/901 | Loss: 0.7652 | CLoss: 0.6277 | FLoss: 0.2750 | LR: 2.86e-05\n",
      "  Batch 590/901 | Loss: 0.6877 | CLoss: 0.5394 | FLoss: 0.2967 | LR: 2.86e-05\n",
      "  Batch 600/901 | Loss: 0.6773 | CLoss: 0.5398 | FLoss: 0.2748 | LR: 2.86e-05\n",
      "  Batch 610/901 | Loss: 0.4137 | CLoss: 0.3048 | FLoss: 0.2178 | LR: 2.86e-05\n",
      "  Batch 620/901 | Loss: 0.6564 | CLoss: 0.4679 | FLoss: 0.3770 | LR: 2.86e-05\n",
      "  Batch 630/901 | Loss: 0.3820 | CLoss: 0.2899 | FLoss: 0.1841 | LR: 2.86e-05\n",
      "  Batch 640/901 | Loss: 0.6268 | CLoss: 0.4823 | FLoss: 0.2890 | LR: 2.86e-05\n",
      "  Batch 650/901 | Loss: 0.3420 | CLoss: 0.2863 | FLoss: 0.1114 | LR: 2.86e-05\n",
      "  Batch 660/901 | Loss: 0.6784 | CLoss: 0.5857 | FLoss: 0.1853 | LR: 2.86e-05\n",
      "  Batch 670/901 | Loss: 0.5175 | CLoss: 0.4208 | FLoss: 0.1935 | LR: 2.86e-05\n",
      "  Batch 680/901 | Loss: 0.9691 | CLoss: 0.7030 | FLoss: 0.5322 | LR: 2.86e-05\n",
      "  Batch 690/901 | Loss: 0.6265 | CLoss: 0.5150 | FLoss: 0.2231 | LR: 2.86e-05\n",
      "  Batch 700/901 | Loss: 0.6563 | CLoss: 0.5342 | FLoss: 0.2442 | LR: 2.86e-05\n",
      "  Batch 710/901 | Loss: 0.7479 | CLoss: 0.6083 | FLoss: 0.2793 | LR: 2.86e-05\n",
      "  Batch 720/901 | Loss: 0.8150 | CLoss: 0.6403 | FLoss: 0.3493 | LR: 2.86e-05\n",
      "  Batch 730/901 | Loss: 0.4696 | CLoss: 0.3760 | FLoss: 0.1871 | LR: 2.86e-05\n",
      "  Batch 740/901 | Loss: 0.4841 | CLoss: 0.2876 | FLoss: 0.3931 | LR: 2.86e-05\n",
      "  Batch 750/901 | Loss: 0.4261 | CLoss: 0.3328 | FLoss: 0.1866 | LR: 2.86e-05\n",
      "  Batch 760/901 | Loss: 0.3242 | CLoss: 0.2404 | FLoss: 0.1677 | LR: 2.86e-05\n",
      "  Batch 770/901 | Loss: 0.7554 | CLoss: 0.6343 | FLoss: 0.2421 | LR: 2.86e-05\n",
      "  Batch 780/901 | Loss: 0.4169 | CLoss: 0.3416 | FLoss: 0.1505 | LR: 2.86e-05\n",
      "  Batch 790/901 | Loss: 0.3792 | CLoss: 0.2622 | FLoss: 0.2341 | LR: 2.86e-05\n",
      "  Batch 800/901 | Loss: 0.5960 | CLoss: 0.4419 | FLoss: 0.3082 | LR: 2.86e-05\n",
      "  Batch 810/901 | Loss: 0.4576 | CLoss: 0.3359 | FLoss: 0.2435 | LR: 2.86e-05\n",
      "  Batch 820/901 | Loss: 0.6159 | CLoss: 0.4839 | FLoss: 0.2641 | LR: 2.86e-05\n",
      "  Batch 830/901 | Loss: 0.8988 | CLoss: 0.7442 | FLoss: 0.3092 | LR: 2.86e-05\n",
      "  Batch 840/901 | Loss: 0.3441 | CLoss: 0.2283 | FLoss: 0.2316 | LR: 2.86e-05\n",
      "  Batch 850/901 | Loss: 0.4127 | CLoss: 0.3352 | FLoss: 0.1549 | LR: 2.86e-05\n",
      "  Batch 860/901 | Loss: 0.5643 | CLoss: 0.4526 | FLoss: 0.2234 | LR: 2.86e-05\n",
      "  Batch 870/901 | Loss: 1.0577 | CLoss: 0.8205 | FLoss: 0.4743 | LR: 2.86e-05\n",
      "  Batch 880/901 | Loss: 0.5891 | CLoss: 0.4776 | FLoss: 0.2229 | LR: 2.86e-05\n",
      "  Batch 890/901 | Loss: 0.7721 | CLoss: 0.6454 | FLoss: 0.2534 | LR: 2.86e-05\n",
      "  Batch 900/901 | Loss: 0.6277 | CLoss: 0.4830 | FLoss: 0.2893 | LR: 2.86e-05\n",
      "  Batch 901/901 | Loss: 0.8387 | CLoss: 0.6971 | FLoss: 0.2831 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.6065\n",
      "  Last Batch Loss: 0.8387\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.7864 | Batch Acc: 72.41%\n",
      "    Val Batch 010/99 | Loss: 0.0953 | Batch Acc: 96.55%\n",
      "    Val Batch 015/99 | Loss: 0.1943 | Batch Acc: 93.10%\n",
      "    Val Batch 020/99 | Loss: 1.0541 | Batch Acc: 25.86%\n",
      "    Val Batch 025/99 | Loss: 0.2561 | Batch Acc: 89.66%\n",
      "    Val Batch 030/99 | Loss: 0.2517 | Batch Acc: 93.10%\n",
      "    Val Batch 035/99 | Loss: 0.0725 | Batch Acc: 98.28%\n",
      "    Val Batch 040/99 | Loss: 0.5586 | Batch Acc: 94.83%\n",
      "    Val Batch 045/99 | Loss: 0.0607 | Batch Acc: 96.55%\n",
      "    Val Batch 050/99 | Loss: 0.2367 | Batch Acc: 91.38%\n",
      "    Val Batch 055/99 | Loss: 0.3836 | Batch Acc: 89.66%\n",
      "    Val Batch 060/99 | Loss: 0.3196 | Batch Acc: 86.21%\n",
      "    Val Batch 065/99 | Loss: 0.4273 | Batch Acc: 82.76%\n",
      "    Val Batch 070/99 | Loss: 0.1828 | Batch Acc: 93.10%\n",
      "    Val Batch 075/99 | Loss: 0.4674 | Batch Acc: 86.21%\n",
      "    Val Batch 080/99 | Loss: 0.3246 | Batch Acc: 91.38%\n",
      "    Val Batch 085/99 | Loss: 0.0967 | Batch Acc: 96.55%\n",
      "    Val Batch 090/99 | Loss: 0.1131 | Batch Acc: 96.55%\n",
      "    Val Batch 095/99 | Loss: 0.0444 | Batch Acc: 98.28%\n",
      "    Val Batch 099/99 | Loss: 0.0146 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.3179 | Accuracy: 87.50%\n",
      "  Current Best Acc: 89.34%\n",
      "\n",
      "========================================\n",
      "=== Fold 9 Completed ===\n",
      "Best Validation Accuracy: 89.34%\n",
      "\n",
      "========================================\n",
      "=== Fold 10/10 ====================\n",
      "========================================\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "  Batch 010/900 | Loss: 1.8163 | CLoss: 0.8499 | FLoss: 1.9327 | LR: 3.00e-04\n",
      "  Batch 020/900 | Loss: 1.5174 | CLoss: 0.7594 | FLoss: 1.5160 | LR: 3.00e-04\n",
      "  Batch 030/900 | Loss: 1.4989 | CLoss: 0.9456 | FLoss: 1.1065 | LR: 3.00e-04\n",
      "  Batch 040/900 | Loss: 1.0413 | CLoss: 0.6000 | FLoss: 0.8827 | LR: 3.00e-04\n",
      "  Batch 050/900 | Loss: 0.9559 | CLoss: 0.5868 | FLoss: 0.7383 | LR: 3.00e-04\n",
      "  Batch 060/900 | Loss: 0.7339 | CLoss: 0.4821 | FLoss: 0.5036 | LR: 3.00e-04\n",
      "  Batch 070/900 | Loss: 0.8009 | CLoss: 0.5540 | FLoss: 0.4938 | LR: 3.00e-04\n",
      "  Batch 080/900 | Loss: 0.8842 | CLoss: 0.6127 | FLoss: 0.5431 | LR: 3.00e-04\n",
      "  Batch 090/900 | Loss: 0.7359 | CLoss: 0.5189 | FLoss: 0.4340 | LR: 3.00e-04\n",
      "  Batch 100/900 | Loss: 0.9963 | CLoss: 0.7090 | FLoss: 0.5746 | LR: 3.00e-04\n",
      "  Batch 110/900 | Loss: 0.8416 | CLoss: 0.5618 | FLoss: 0.5596 | LR: 3.00e-04\n",
      "  Batch 120/900 | Loss: 0.6577 | CLoss: 0.4738 | FLoss: 0.3677 | LR: 3.00e-04\n",
      "  Batch 130/900 | Loss: 1.0240 | CLoss: 0.7807 | FLoss: 0.4866 | LR: 3.00e-04\n",
      "  Batch 140/900 | Loss: 0.7223 | CLoss: 0.5117 | FLoss: 0.4212 | LR: 3.00e-04\n",
      "  Batch 150/900 | Loss: 0.7627 | CLoss: 0.5899 | FLoss: 0.3458 | LR: 3.00e-04\n",
      "  Batch 160/900 | Loss: 1.0203 | CLoss: 0.7650 | FLoss: 0.5106 | LR: 3.00e-04\n",
      "  Batch 170/900 | Loss: 0.9446 | CLoss: 0.7217 | FLoss: 0.4459 | LR: 3.00e-04\n",
      "  Batch 180/900 | Loss: 0.9789 | CLoss: 0.7154 | FLoss: 0.5270 | LR: 3.00e-04\n",
      "  Batch 190/900 | Loss: 1.0304 | CLoss: 0.8308 | FLoss: 0.3991 | LR: 3.00e-04\n",
      "  Batch 200/900 | Loss: 0.8095 | CLoss: 0.6097 | FLoss: 0.3995 | LR: 3.00e-04\n",
      "  Batch 210/900 | Loss: 0.6884 | CLoss: 0.5306 | FLoss: 0.3157 | LR: 3.00e-04\n",
      "  Batch 220/900 | Loss: 0.7853 | CLoss: 0.6051 | FLoss: 0.3604 | LR: 3.00e-04\n",
      "  Batch 230/900 | Loss: 0.7871 | CLoss: 0.5853 | FLoss: 0.4036 | LR: 3.00e-04\n",
      "  Batch 240/900 | Loss: 0.9156 | CLoss: 0.6488 | FLoss: 0.5335 | LR: 3.00e-04\n",
      "  Batch 250/900 | Loss: 0.7261 | CLoss: 0.5088 | FLoss: 0.4346 | LR: 3.00e-04\n",
      "  Batch 260/900 | Loss: 0.7678 | CLoss: 0.5492 | FLoss: 0.4374 | LR: 3.00e-04\n",
      "  Batch 270/900 | Loss: 0.9678 | CLoss: 0.7777 | FLoss: 0.3803 | LR: 3.00e-04\n",
      "  Batch 280/900 | Loss: 0.2409 | CLoss: 0.1600 | FLoss: 0.1618 | LR: 3.00e-04\n",
      "  Batch 290/900 | Loss: 0.8516 | CLoss: 0.6206 | FLoss: 0.4620 | LR: 3.00e-04\n",
      "  Batch 300/900 | Loss: 0.6018 | CLoss: 0.4591 | FLoss: 0.2854 | LR: 3.00e-04\n",
      "  Batch 310/900 | Loss: 0.9161 | CLoss: 0.6988 | FLoss: 0.4346 | LR: 3.00e-04\n",
      "  Batch 320/900 | Loss: 0.9287 | CLoss: 0.6950 | FLoss: 0.4675 | LR: 3.00e-04\n",
      "  Batch 330/900 | Loss: 0.9888 | CLoss: 0.8373 | FLoss: 0.3030 | LR: 3.00e-04\n",
      "  Batch 340/900 | Loss: 0.7238 | CLoss: 0.5278 | FLoss: 0.3919 | LR: 3.00e-04\n",
      "  Batch 350/900 | Loss: 0.7347 | CLoss: 0.5187 | FLoss: 0.4320 | LR: 3.00e-04\n",
      "  Batch 360/900 | Loss: 1.1705 | CLoss: 0.9095 | FLoss: 0.5220 | LR: 3.00e-04\n",
      "  Batch 370/900 | Loss: 1.2586 | CLoss: 0.9290 | FLoss: 0.6593 | LR: 3.00e-04\n",
      "  Batch 380/900 | Loss: 0.6798 | CLoss: 0.4749 | FLoss: 0.4097 | LR: 3.00e-04\n",
      "  Batch 390/900 | Loss: 1.5545 | CLoss: 1.2439 | FLoss: 0.6212 | LR: 3.00e-04\n",
      "  Batch 400/900 | Loss: 0.6963 | CLoss: 0.5171 | FLoss: 0.3583 | LR: 3.00e-04\n",
      "  Batch 410/900 | Loss: 0.8552 | CLoss: 0.6477 | FLoss: 0.4149 | LR: 3.00e-04\n",
      "  Batch 420/900 | Loss: 1.1528 | CLoss: 0.8721 | FLoss: 0.5614 | LR: 3.00e-04\n",
      "  Batch 430/900 | Loss: 1.0151 | CLoss: 0.7562 | FLoss: 0.5176 | LR: 3.00e-04\n",
      "  Batch 440/900 | Loss: 0.8646 | CLoss: 0.5758 | FLoss: 0.5776 | LR: 3.00e-04\n",
      "  Batch 450/900 | Loss: 0.6695 | CLoss: 0.5475 | FLoss: 0.2439 | LR: 3.00e-04\n",
      "  Batch 460/900 | Loss: 0.6763 | CLoss: 0.5658 | FLoss: 0.2208 | LR: 3.00e-04\n",
      "  Batch 470/900 | Loss: 0.8077 | CLoss: 0.6602 | FLoss: 0.2950 | LR: 3.00e-04\n",
      "  Batch 480/900 | Loss: 1.1996 | CLoss: 0.9625 | FLoss: 0.4743 | LR: 3.00e-04\n",
      "  Batch 490/900 | Loss: 0.9285 | CLoss: 0.7454 | FLoss: 0.3661 | LR: 3.00e-04\n",
      "  Batch 500/900 | Loss: 0.4288 | CLoss: 0.3685 | FLoss: 0.1206 | LR: 3.00e-04\n",
      "  Batch 510/900 | Loss: 0.9659 | CLoss: 0.7823 | FLoss: 0.3671 | LR: 3.00e-04\n",
      "  Batch 520/900 | Loss: 0.8540 | CLoss: 0.6467 | FLoss: 0.4145 | LR: 3.00e-04\n",
      "  Batch 530/900 | Loss: 1.1183 | CLoss: 0.8673 | FLoss: 0.5020 | LR: 3.00e-04\n",
      "  Batch 540/900 | Loss: 1.1182 | CLoss: 0.8897 | FLoss: 0.4570 | LR: 3.00e-04\n",
      "  Batch 550/900 | Loss: 1.1421 | CLoss: 0.8784 | FLoss: 0.5273 | LR: 3.00e-04\n",
      "  Batch 560/900 | Loss: 0.9410 | CLoss: 0.7603 | FLoss: 0.3614 | LR: 3.00e-04\n",
      "  Batch 570/900 | Loss: 0.5504 | CLoss: 0.3736 | FLoss: 0.3536 | LR: 3.00e-04\n",
      "  Batch 580/900 | Loss: 1.1945 | CLoss: 0.9239 | FLoss: 0.5412 | LR: 3.00e-04\n",
      "  Batch 590/900 | Loss: 1.0139 | CLoss: 0.8675 | FLoss: 0.2929 | LR: 3.00e-04\n",
      "  Batch 600/900 | Loss: 0.6386 | CLoss: 0.5571 | FLoss: 0.1630 | LR: 3.00e-04\n",
      "  Batch 610/900 | Loss: 0.7476 | CLoss: 0.6247 | FLoss: 0.2457 | LR: 3.00e-04\n",
      "  Batch 620/900 | Loss: 1.1320 | CLoss: 0.9912 | FLoss: 0.2815 | LR: 3.00e-04\n",
      "  Batch 630/900 | Loss: 0.8252 | CLoss: 0.6294 | FLoss: 0.3915 | LR: 3.00e-04\n",
      "  Batch 640/900 | Loss: 0.9806 | CLoss: 0.7269 | FLoss: 0.5073 | LR: 3.00e-04\n",
      "  Batch 650/900 | Loss: 0.9179 | CLoss: 0.7289 | FLoss: 0.3779 | LR: 3.00e-04\n",
      "  Batch 660/900 | Loss: 0.9642 | CLoss: 0.7608 | FLoss: 0.4068 | LR: 3.00e-04\n",
      "  Batch 670/900 | Loss: 0.4961 | CLoss: 0.3564 | FLoss: 0.2793 | LR: 3.00e-04\n",
      "  Batch 680/900 | Loss: 0.8180 | CLoss: 0.6026 | FLoss: 0.4308 | LR: 3.00e-04\n",
      "  Batch 690/900 | Loss: 0.7101 | CLoss: 0.5762 | FLoss: 0.2678 | LR: 3.00e-04\n",
      "  Batch 700/900 | Loss: 0.9372 | CLoss: 0.7764 | FLoss: 0.3216 | LR: 3.00e-04\n",
      "  Batch 710/900 | Loss: 0.9106 | CLoss: 0.7491 | FLoss: 0.3229 | LR: 3.00e-04\n",
      "  Batch 720/900 | Loss: 0.9753 | CLoss: 0.8038 | FLoss: 0.3429 | LR: 3.00e-04\n",
      "  Batch 730/900 | Loss: 0.6528 | CLoss: 0.4197 | FLoss: 0.4661 | LR: 3.00e-04\n",
      "  Batch 740/900 | Loss: 0.9239 | CLoss: 0.7503 | FLoss: 0.3472 | LR: 3.00e-04\n",
      "  Batch 750/900 | Loss: 0.9606 | CLoss: 0.7920 | FLoss: 0.3371 | LR: 3.00e-04\n",
      "  Batch 760/900 | Loss: 1.1236 | CLoss: 0.9108 | FLoss: 0.4258 | LR: 3.00e-04\n",
      "  Batch 770/900 | Loss: 1.4637 | CLoss: 1.1639 | FLoss: 0.5997 | LR: 3.00e-04\n",
      "  Batch 780/900 | Loss: 0.9188 | CLoss: 0.7324 | FLoss: 0.3730 | LR: 3.00e-04\n",
      "  Batch 790/900 | Loss: 1.0405 | CLoss: 0.8425 | FLoss: 0.3960 | LR: 3.00e-04\n",
      "  Batch 800/900 | Loss: 0.7604 | CLoss: 0.6596 | FLoss: 0.2017 | LR: 3.00e-04\n",
      "  Batch 810/900 | Loss: 0.7319 | CLoss: 0.5598 | FLoss: 0.3442 | LR: 3.00e-04\n",
      "  Batch 820/900 | Loss: 0.5033 | CLoss: 0.3536 | FLoss: 0.2993 | LR: 3.00e-04\n",
      "  Batch 830/900 | Loss: 0.9295 | CLoss: 0.7729 | FLoss: 0.3132 | LR: 3.00e-04\n",
      "  Batch 840/900 | Loss: 0.6886 | CLoss: 0.4787 | FLoss: 0.4198 | LR: 3.00e-04\n",
      "  Batch 850/900 | Loss: 1.1136 | CLoss: 0.8903 | FLoss: 0.4466 | LR: 3.00e-04\n",
      "  Batch 860/900 | Loss: 0.6578 | CLoss: 0.5122 | FLoss: 0.2912 | LR: 3.00e-04\n",
      "  Batch 870/900 | Loss: 0.8778 | CLoss: 0.6890 | FLoss: 0.3776 | LR: 3.00e-04\n",
      "  Batch 880/900 | Loss: 0.9389 | CLoss: 0.7755 | FLoss: 0.3269 | LR: 3.00e-04\n",
      "  Batch 890/900 | Loss: 0.8322 | CLoss: 0.6982 | FLoss: 0.2680 | LR: 3.00e-04\n",
      "  Batch 900/900 | Loss: 0.3915 | CLoss: 0.2313 | FLoss: 0.3206 | LR: 3.00e-04\n",
      "\n",
      "  Training Summary | Epoch 1\n",
      "  Avg Loss: 0.9117\n",
      "  Last Batch Loss: 0.3915\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.6445 | Batch Acc: 72.41%\n",
      "    Val Batch 010/99 | Loss: 0.4134 | Batch Acc: 87.93%\n",
      "    Val Batch 015/99 | Loss: 0.6394 | Batch Acc: 79.31%\n",
      "    Val Batch 020/99 | Loss: 0.4486 | Batch Acc: 72.41%\n",
      "    Val Batch 025/99 | Loss: 0.4282 | Batch Acc: 86.21%\n",
      "    Val Batch 030/99 | Loss: 0.3413 | Batch Acc: 91.38%\n",
      "    Val Batch 035/99 | Loss: 0.2395 | Batch Acc: 93.10%\n",
      "    Val Batch 040/99 | Loss: 0.4777 | Batch Acc: 86.21%\n",
      "    Val Batch 045/99 | Loss: 0.3288 | Batch Acc: 91.38%\n",
      "    Val Batch 050/99 | Loss: 0.4080 | Batch Acc: 89.66%\n",
      "    Val Batch 055/99 | Loss: 0.2137 | Batch Acc: 93.10%\n",
      "    Val Batch 060/99 | Loss: 0.1755 | Batch Acc: 93.10%\n",
      "    Val Batch 065/99 | Loss: 0.3920 | Batch Acc: 87.93%\n",
      "    Val Batch 070/99 | Loss: 0.1138 | Batch Acc: 96.55%\n",
      "    Val Batch 075/99 | Loss: 0.1985 | Batch Acc: 93.10%\n",
      "    Val Batch 080/99 | Loss: 0.2587 | Batch Acc: 93.10%\n",
      "    Val Batch 085/99 | Loss: 0.2022 | Batch Acc: 91.38%\n",
      "    Val Batch 090/99 | Loss: 0.2551 | Batch Acc: 89.66%\n",
      "    Val Batch 095/99 | Loss: 0.1344 | Batch Acc: 96.55%\n",
      "    Val Batch 099/99 | Loss: 0.2208 | Batch Acc: 97.14%\n",
      "\n",
      "  Validation Summary | Epoch 1\n",
      "  Avg Loss: 0.2961 | Accuracy: 89.84%\n",
      "  Current Best Acc: 89.84%\n",
      "\n",
      "Epoch 2/5\n",
      "  Batch 010/900 | Loss: 0.6932 | CLoss: 0.5940 | FLoss: 0.1982 | LR: 2.71e-04\n",
      "  Batch 020/900 | Loss: 0.7837 | CLoss: 0.6542 | FLoss: 0.2589 | LR: 2.71e-04\n",
      "  Batch 030/900 | Loss: 0.9963 | CLoss: 0.8017 | FLoss: 0.3892 | LR: 2.71e-04\n",
      "  Batch 040/900 | Loss: 1.6454 | CLoss: 1.3592 | FLoss: 0.5725 | LR: 2.71e-04\n",
      "  Batch 050/900 | Loss: 1.0518 | CLoss: 0.8175 | FLoss: 0.4687 | LR: 2.71e-04\n",
      "  Batch 060/900 | Loss: 0.8847 | CLoss: 0.7476 | FLoss: 0.2743 | LR: 2.71e-04\n",
      "  Batch 070/900 | Loss: 0.7891 | CLoss: 0.5999 | FLoss: 0.3784 | LR: 2.71e-04\n",
      "  Batch 080/900 | Loss: 1.2675 | CLoss: 1.1223 | FLoss: 0.2903 | LR: 2.71e-04\n",
      "  Batch 090/900 | Loss: 0.5103 | CLoss: 0.3589 | FLoss: 0.3028 | LR: 2.71e-04\n",
      "  Batch 100/900 | Loss: 1.1303 | CLoss: 0.9760 | FLoss: 0.3086 | LR: 2.71e-04\n",
      "  Batch 110/900 | Loss: 0.8567 | CLoss: 0.6163 | FLoss: 0.4809 | LR: 2.71e-04\n",
      "  Batch 120/900 | Loss: 0.8206 | CLoss: 0.6467 | FLoss: 0.3478 | LR: 2.71e-04\n",
      "  Batch 130/900 | Loss: 0.8328 | CLoss: 0.6982 | FLoss: 0.2692 | LR: 2.71e-04\n",
      "  Batch 140/900 | Loss: 1.0951 | CLoss: 0.9186 | FLoss: 0.3530 | LR: 2.71e-04\n",
      "  Batch 150/900 | Loss: 0.6687 | CLoss: 0.4929 | FLoss: 0.3514 | LR: 2.71e-04\n",
      "  Batch 160/900 | Loss: 1.1983 | CLoss: 0.9829 | FLoss: 0.4306 | LR: 2.71e-04\n",
      "  Batch 170/900 | Loss: 0.8501 | CLoss: 0.6788 | FLoss: 0.3427 | LR: 2.71e-04\n",
      "  Batch 180/900 | Loss: 0.6799 | CLoss: 0.5533 | FLoss: 0.2531 | LR: 2.71e-04\n",
      "  Batch 190/900 | Loss: 0.3169 | CLoss: 0.2426 | FLoss: 0.1486 | LR: 2.71e-04\n",
      "  Batch 200/900 | Loss: 0.8543 | CLoss: 0.6780 | FLoss: 0.3527 | LR: 2.71e-04\n",
      "  Batch 210/900 | Loss: 1.0359 | CLoss: 0.7855 | FLoss: 0.5008 | LR: 2.71e-04\n",
      "  Batch 220/900 | Loss: 0.5804 | CLoss: 0.4282 | FLoss: 0.3046 | LR: 2.71e-04\n",
      "  Batch 230/900 | Loss: 0.7699 | CLoss: 0.6610 | FLoss: 0.2177 | LR: 2.71e-04\n",
      "  Batch 240/900 | Loss: 0.7112 | CLoss: 0.5297 | FLoss: 0.3631 | LR: 2.71e-04\n",
      "  Batch 250/900 | Loss: 0.8519 | CLoss: 0.6506 | FLoss: 0.4025 | LR: 2.71e-04\n",
      "  Batch 260/900 | Loss: 1.0221 | CLoss: 0.7484 | FLoss: 0.5475 | LR: 2.71e-04\n",
      "  Batch 270/900 | Loss: 1.0116 | CLoss: 0.8681 | FLoss: 0.2869 | LR: 2.71e-04\n",
      "  Batch 280/900 | Loss: 0.8684 | CLoss: 0.6489 | FLoss: 0.4391 | LR: 2.71e-04\n",
      "  Batch 290/900 | Loss: 1.0471 | CLoss: 0.8123 | FLoss: 0.4695 | LR: 2.71e-04\n",
      "  Batch 300/900 | Loss: 0.9353 | CLoss: 0.6536 | FLoss: 0.5634 | LR: 2.71e-04\n",
      "  Batch 310/900 | Loss: 0.6243 | CLoss: 0.4294 | FLoss: 0.3898 | LR: 2.71e-04\n",
      "  Batch 320/900 | Loss: 0.5856 | CLoss: 0.4561 | FLoss: 0.2591 | LR: 2.71e-04\n",
      "  Batch 330/900 | Loss: 0.5324 | CLoss: 0.3910 | FLoss: 0.2827 | LR: 2.71e-04\n",
      "  Batch 340/900 | Loss: 1.0296 | CLoss: 0.7405 | FLoss: 0.5783 | LR: 2.71e-04\n",
      "  Batch 350/900 | Loss: 0.9149 | CLoss: 0.7582 | FLoss: 0.3133 | LR: 2.71e-04\n",
      "  Batch 360/900 | Loss: 0.9424 | CLoss: 0.7275 | FLoss: 0.4298 | LR: 2.71e-04\n",
      "  Batch 370/900 | Loss: 0.6389 | CLoss: 0.4978 | FLoss: 0.2822 | LR: 2.71e-04\n",
      "  Batch 380/900 | Loss: 0.6138 | CLoss: 0.4012 | FLoss: 0.4252 | LR: 2.71e-04\n",
      "  Batch 390/900 | Loss: 0.7536 | CLoss: 0.6412 | FLoss: 0.2248 | LR: 2.71e-04\n",
      "  Batch 400/900 | Loss: 0.7938 | CLoss: 0.6586 | FLoss: 0.2703 | LR: 2.71e-04\n",
      "  Batch 410/900 | Loss: 0.8456 | CLoss: 0.7050 | FLoss: 0.2812 | LR: 2.71e-04\n",
      "  Batch 420/900 | Loss: 0.4578 | CLoss: 0.3736 | FLoss: 0.1684 | LR: 2.71e-04\n",
      "  Batch 430/900 | Loss: 1.3640 | CLoss: 1.0212 | FLoss: 0.6857 | LR: 2.71e-04\n",
      "  Batch 440/900 | Loss: 0.8330 | CLoss: 0.6821 | FLoss: 0.3017 | LR: 2.71e-04\n",
      "  Batch 450/900 | Loss: 0.5385 | CLoss: 0.3924 | FLoss: 0.2920 | LR: 2.71e-04\n",
      "  Batch 460/900 | Loss: 0.7507 | CLoss: 0.6135 | FLoss: 0.2743 | LR: 2.71e-04\n",
      "  Batch 470/900 | Loss: 1.2955 | CLoss: 1.0402 | FLoss: 0.5106 | LR: 2.71e-04\n",
      "  Batch 480/900 | Loss: 0.9825 | CLoss: 0.6826 | FLoss: 0.5999 | LR: 2.71e-04\n",
      "  Batch 490/900 | Loss: 0.5341 | CLoss: 0.3422 | FLoss: 0.3837 | LR: 2.71e-04\n",
      "  Batch 500/900 | Loss: 1.0941 | CLoss: 0.8873 | FLoss: 0.4135 | LR: 2.71e-04\n",
      "  Batch 510/900 | Loss: 0.9155 | CLoss: 0.7874 | FLoss: 0.2561 | LR: 2.71e-04\n",
      "  Batch 520/900 | Loss: 0.7103 | CLoss: 0.5749 | FLoss: 0.2707 | LR: 2.71e-04\n",
      "  Batch 530/900 | Loss: 0.4628 | CLoss: 0.3818 | FLoss: 0.1619 | LR: 2.71e-04\n",
      "  Batch 540/900 | Loss: 0.8061 | CLoss: 0.5902 | FLoss: 0.4318 | LR: 2.71e-04\n",
      "  Batch 550/900 | Loss: 0.5747 | CLoss: 0.4329 | FLoss: 0.2836 | LR: 2.71e-04\n",
      "  Batch 560/900 | Loss: 0.6232 | CLoss: 0.4752 | FLoss: 0.2961 | LR: 2.71e-04\n",
      "  Batch 570/900 | Loss: 0.9122 | CLoss: 0.7351 | FLoss: 0.3541 | LR: 2.71e-04\n",
      "  Batch 580/900 | Loss: 1.0393 | CLoss: 0.7778 | FLoss: 0.5231 | LR: 2.71e-04\n",
      "  Batch 590/900 | Loss: 0.6625 | CLoss: 0.5016 | FLoss: 0.3216 | LR: 2.71e-04\n",
      "  Batch 600/900 | Loss: 1.2609 | CLoss: 1.0924 | FLoss: 0.3370 | LR: 2.71e-04\n",
      "  Batch 610/900 | Loss: 0.5944 | CLoss: 0.4907 | FLoss: 0.2073 | LR: 2.71e-04\n",
      "  Batch 620/900 | Loss: 0.6756 | CLoss: 0.5296 | FLoss: 0.2919 | LR: 2.71e-04\n",
      "  Batch 630/900 | Loss: 1.2190 | CLoss: 1.0104 | FLoss: 0.4172 | LR: 2.71e-04\n",
      "  Batch 640/900 | Loss: 0.7265 | CLoss: 0.6288 | FLoss: 0.1956 | LR: 2.71e-04\n",
      "  Batch 650/900 | Loss: 0.6732 | CLoss: 0.4268 | FLoss: 0.4929 | LR: 2.71e-04\n",
      "  Batch 660/900 | Loss: 0.5207 | CLoss: 0.3758 | FLoss: 0.2896 | LR: 2.71e-04\n",
      "  Batch 670/900 | Loss: 0.5901 | CLoss: 0.4433 | FLoss: 0.2937 | LR: 2.71e-04\n",
      "  Batch 680/900 | Loss: 0.7040 | CLoss: 0.5290 | FLoss: 0.3499 | LR: 2.71e-04\n",
      "  Batch 690/900 | Loss: 1.1157 | CLoss: 0.8334 | FLoss: 0.5646 | LR: 2.71e-04\n",
      "  Batch 700/900 | Loss: 0.9861 | CLoss: 0.7834 | FLoss: 0.4056 | LR: 2.71e-04\n",
      "  Batch 710/900 | Loss: 0.7220 | CLoss: 0.5540 | FLoss: 0.3360 | LR: 2.71e-04\n",
      "  Batch 720/900 | Loss: 0.9144 | CLoss: 0.7490 | FLoss: 0.3307 | LR: 2.71e-04\n",
      "  Batch 730/900 | Loss: 0.4501 | CLoss: 0.3687 | FLoss: 0.1629 | LR: 2.71e-04\n",
      "  Batch 740/900 | Loss: 0.7026 | CLoss: 0.5454 | FLoss: 0.3144 | LR: 2.71e-04\n",
      "  Batch 750/900 | Loss: 0.7622 | CLoss: 0.5875 | FLoss: 0.3493 | LR: 2.71e-04\n",
      "  Batch 760/900 | Loss: 0.8214 | CLoss: 0.6242 | FLoss: 0.3945 | LR: 2.71e-04\n",
      "  Batch 770/900 | Loss: 0.4288 | CLoss: 0.3129 | FLoss: 0.2319 | LR: 2.71e-04\n",
      "  Batch 780/900 | Loss: 0.7471 | CLoss: 0.5463 | FLoss: 0.4017 | LR: 2.71e-04\n",
      "  Batch 790/900 | Loss: 1.1753 | CLoss: 1.0317 | FLoss: 0.2872 | LR: 2.71e-04\n",
      "  Batch 800/900 | Loss: 0.5574 | CLoss: 0.4450 | FLoss: 0.2248 | LR: 2.71e-04\n",
      "  Batch 810/900 | Loss: 0.8637 | CLoss: 0.6633 | FLoss: 0.4009 | LR: 2.71e-04\n",
      "  Batch 820/900 | Loss: 0.8111 | CLoss: 0.6252 | FLoss: 0.3717 | LR: 2.71e-04\n",
      "  Batch 830/900 | Loss: 0.9163 | CLoss: 0.7311 | FLoss: 0.3704 | LR: 2.71e-04\n",
      "  Batch 840/900 | Loss: 0.5525 | CLoss: 0.4493 | FLoss: 0.2063 | LR: 2.71e-04\n",
      "  Batch 850/900 | Loss: 0.7150 | CLoss: 0.4959 | FLoss: 0.4382 | LR: 2.71e-04\n",
      "  Batch 860/900 | Loss: 0.4121 | CLoss: 0.3094 | FLoss: 0.2054 | LR: 2.71e-04\n",
      "  Batch 870/900 | Loss: 0.8924 | CLoss: 0.7154 | FLoss: 0.3542 | LR: 2.71e-04\n",
      "  Batch 880/900 | Loss: 0.7423 | CLoss: 0.5853 | FLoss: 0.3142 | LR: 2.71e-04\n",
      "  Batch 890/900 | Loss: 0.7025 | CLoss: 0.5442 | FLoss: 0.3166 | LR: 2.71e-04\n",
      "  Batch 900/900 | Loss: 0.5946 | CLoss: 0.4907 | FLoss: 0.2076 | LR: 2.71e-04\n",
      "\n",
      "  Training Summary | Epoch 2\n",
      "  Avg Loss: 0.7910\n",
      "  Last Batch Loss: 0.5946\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.8281 | Batch Acc: 65.52%\n",
      "    Val Batch 010/99 | Loss: 0.4040 | Batch Acc: 87.93%\n",
      "    Val Batch 015/99 | Loss: 0.4477 | Batch Acc: 87.93%\n",
      "    Val Batch 020/99 | Loss: 0.7862 | Batch Acc: 68.97%\n",
      "    Val Batch 025/99 | Loss: 0.3085 | Batch Acc: 89.66%\n",
      "    Val Batch 030/99 | Loss: 0.2493 | Batch Acc: 87.93%\n",
      "    Val Batch 035/99 | Loss: 0.1450 | Batch Acc: 93.10%\n",
      "    Val Batch 040/99 | Loss: 0.2653 | Batch Acc: 100.00%\n",
      "    Val Batch 045/99 | Loss: 0.2678 | Batch Acc: 96.55%\n",
      "    Val Batch 050/99 | Loss: 0.1977 | Batch Acc: 94.83%\n",
      "    Val Batch 055/99 | Loss: 0.1141 | Batch Acc: 93.10%\n",
      "    Val Batch 060/99 | Loss: 0.2995 | Batch Acc: 87.93%\n",
      "    Val Batch 065/99 | Loss: 0.5114 | Batch Acc: 79.31%\n",
      "    Val Batch 070/99 | Loss: 0.3196 | Batch Acc: 91.38%\n",
      "    Val Batch 075/99 | Loss: 0.1296 | Batch Acc: 96.55%\n",
      "    Val Batch 080/99 | Loss: 0.3254 | Batch Acc: 86.21%\n",
      "    Val Batch 085/99 | Loss: 0.2328 | Batch Acc: 91.38%\n",
      "    Val Batch 090/99 | Loss: 0.2407 | Batch Acc: 91.38%\n",
      "    Val Batch 095/99 | Loss: 0.4204 | Batch Acc: 89.66%\n",
      "    Val Batch 099/99 | Loss: 0.3480 | Batch Acc: 94.29%\n",
      "\n",
      "  Validation Summary | Epoch 2\n",
      "  Avg Loss: 0.3059 | Accuracy: 89.30%\n",
      "  Current Best Acc: 89.84%\n",
      "\n",
      "Epoch 3/5\n",
      "  Batch 010/900 | Loss: 0.6966 | CLoss: 0.5644 | FLoss: 0.2645 | LR: 1.96e-04\n",
      "  Batch 020/900 | Loss: 0.5868 | CLoss: 0.4633 | FLoss: 0.2470 | LR: 1.96e-04\n",
      "  Batch 030/900 | Loss: 0.7209 | CLoss: 0.5455 | FLoss: 0.3508 | LR: 1.96e-04\n",
      "  Batch 040/900 | Loss: 0.4095 | CLoss: 0.2933 | FLoss: 0.2323 | LR: 1.96e-04\n",
      "  Batch 050/900 | Loss: 0.6442 | CLoss: 0.4836 | FLoss: 0.3211 | LR: 1.96e-04\n",
      "  Batch 060/900 | Loss: 0.7115 | CLoss: 0.5564 | FLoss: 0.3101 | LR: 1.96e-04\n",
      "  Batch 070/900 | Loss: 0.8833 | CLoss: 0.7124 | FLoss: 0.3419 | LR: 1.96e-04\n",
      "  Batch 080/900 | Loss: 0.4935 | CLoss: 0.4046 | FLoss: 0.1777 | LR: 1.96e-04\n",
      "  Batch 090/900 | Loss: 1.0005 | CLoss: 0.7155 | FLoss: 0.5700 | LR: 1.96e-04\n",
      "  Batch 100/900 | Loss: 0.3674 | CLoss: 0.3035 | FLoss: 0.1278 | LR: 1.96e-04\n",
      "  Batch 110/900 | Loss: 0.9904 | CLoss: 0.7977 | FLoss: 0.3854 | LR: 1.96e-04\n",
      "  Batch 120/900 | Loss: 0.4060 | CLoss: 0.3117 | FLoss: 0.1886 | LR: 1.96e-04\n",
      "  Batch 130/900 | Loss: 0.9303 | CLoss: 0.7776 | FLoss: 0.3054 | LR: 1.96e-04\n",
      "  Batch 140/900 | Loss: 0.7749 | CLoss: 0.5740 | FLoss: 0.4018 | LR: 1.96e-04\n",
      "  Batch 150/900 | Loss: 1.0272 | CLoss: 0.8267 | FLoss: 0.4011 | LR: 1.96e-04\n",
      "  Batch 160/900 | Loss: 0.5814 | CLoss: 0.4456 | FLoss: 0.2715 | LR: 1.96e-04\n",
      "  Batch 170/900 | Loss: 0.3088 | CLoss: 0.1999 | FLoss: 0.2178 | LR: 1.96e-04\n",
      "  Batch 180/900 | Loss: 0.7104 | CLoss: 0.5388 | FLoss: 0.3432 | LR: 1.96e-04\n",
      "  Batch 190/900 | Loss: 0.8982 | CLoss: 0.6619 | FLoss: 0.4725 | LR: 1.96e-04\n",
      "  Batch 200/900 | Loss: 0.7483 | CLoss: 0.6397 | FLoss: 0.2173 | LR: 1.96e-04\n",
      "  Batch 210/900 | Loss: 0.6478 | CLoss: 0.5352 | FLoss: 0.2252 | LR: 1.96e-04\n",
      "  Batch 220/900 | Loss: 0.9127 | CLoss: 0.7896 | FLoss: 0.2462 | LR: 1.96e-04\n",
      "  Batch 230/900 | Loss: 1.1929 | CLoss: 0.9629 | FLoss: 0.4600 | LR: 1.96e-04\n",
      "  Batch 240/900 | Loss: 0.9176 | CLoss: 0.6786 | FLoss: 0.4780 | LR: 1.96e-04\n",
      "  Batch 250/900 | Loss: 0.8052 | CLoss: 0.5768 | FLoss: 0.4567 | LR: 1.96e-04\n",
      "  Batch 260/900 | Loss: 0.6816 | CLoss: 0.4893 | FLoss: 0.3847 | LR: 1.96e-04\n",
      "  Batch 270/900 | Loss: 0.6072 | CLoss: 0.4868 | FLoss: 0.2408 | LR: 1.96e-04\n",
      "  Batch 280/900 | Loss: 0.9483 | CLoss: 0.6987 | FLoss: 0.4993 | LR: 1.96e-04\n",
      "  Batch 290/900 | Loss: 0.7711 | CLoss: 0.5848 | FLoss: 0.3726 | LR: 1.96e-04\n",
      "  Batch 300/900 | Loss: 0.4324 | CLoss: 0.3077 | FLoss: 0.2495 | LR: 1.96e-04\n",
      "  Batch 310/900 | Loss: 0.6781 | CLoss: 0.5077 | FLoss: 0.3408 | LR: 1.96e-04\n",
      "  Batch 320/900 | Loss: 0.7746 | CLoss: 0.5908 | FLoss: 0.3675 | LR: 1.96e-04\n",
      "  Batch 330/900 | Loss: 0.5353 | CLoss: 0.4641 | FLoss: 0.1425 | LR: 1.96e-04\n",
      "  Batch 340/900 | Loss: 0.4231 | CLoss: 0.2853 | FLoss: 0.2757 | LR: 1.96e-04\n",
      "  Batch 350/900 | Loss: 0.6637 | CLoss: 0.4581 | FLoss: 0.4112 | LR: 1.96e-04\n",
      "  Batch 360/900 | Loss: 1.2997 | CLoss: 1.0164 | FLoss: 0.5666 | LR: 1.96e-04\n",
      "  Batch 370/900 | Loss: 0.5520 | CLoss: 0.4140 | FLoss: 0.2760 | LR: 1.96e-04\n",
      "  Batch 380/900 | Loss: 0.7143 | CLoss: 0.5766 | FLoss: 0.2754 | LR: 1.96e-04\n",
      "  Batch 390/900 | Loss: 0.5894 | CLoss: 0.4866 | FLoss: 0.2057 | LR: 1.96e-04\n",
      "  Batch 400/900 | Loss: 1.2242 | CLoss: 0.9396 | FLoss: 0.5691 | LR: 1.96e-04\n",
      "  Batch 410/900 | Loss: 0.6787 | CLoss: 0.5042 | FLoss: 0.3490 | LR: 1.96e-04\n",
      "  Batch 420/900 | Loss: 0.8529 | CLoss: 0.6526 | FLoss: 0.4006 | LR: 1.96e-04\n",
      "  Batch 430/900 | Loss: 0.6081 | CLoss: 0.4467 | FLoss: 0.3228 | LR: 1.96e-04\n",
      "  Batch 440/900 | Loss: 0.9503 | CLoss: 0.7007 | FLoss: 0.4993 | LR: 1.96e-04\n",
      "  Batch 450/900 | Loss: 0.8734 | CLoss: 0.7384 | FLoss: 0.2699 | LR: 1.96e-04\n",
      "  Batch 460/900 | Loss: 0.8047 | CLoss: 0.5917 | FLoss: 0.4261 | LR: 1.96e-04\n",
      "  Batch 470/900 | Loss: 0.8499 | CLoss: 0.6133 | FLoss: 0.4732 | LR: 1.96e-04\n",
      "  Batch 480/900 | Loss: 0.8979 | CLoss: 0.7761 | FLoss: 0.2437 | LR: 1.96e-04\n",
      "  Batch 490/900 | Loss: 1.0896 | CLoss: 0.8730 | FLoss: 0.4331 | LR: 1.96e-04\n",
      "  Batch 500/900 | Loss: 0.4517 | CLoss: 0.3613 | FLoss: 0.1808 | LR: 1.96e-04\n",
      "  Batch 510/900 | Loss: 0.6202 | CLoss: 0.5299 | FLoss: 0.1806 | LR: 1.96e-04\n",
      "  Batch 520/900 | Loss: 0.7310 | CLoss: 0.5454 | FLoss: 0.3711 | LR: 1.96e-04\n",
      "  Batch 530/900 | Loss: 0.5849 | CLoss: 0.4696 | FLoss: 0.2305 | LR: 1.96e-04\n",
      "  Batch 540/900 | Loss: 0.6354 | CLoss: 0.4500 | FLoss: 0.3708 | LR: 1.96e-04\n",
      "  Batch 550/900 | Loss: 0.5958 | CLoss: 0.4758 | FLoss: 0.2400 | LR: 1.96e-04\n",
      "  Batch 560/900 | Loss: 0.6456 | CLoss: 0.5567 | FLoss: 0.1778 | LR: 1.96e-04\n",
      "  Batch 570/900 | Loss: 0.6528 | CLoss: 0.5318 | FLoss: 0.2419 | LR: 1.96e-04\n",
      "  Batch 580/900 | Loss: 1.3633 | CLoss: 1.1998 | FLoss: 0.3269 | LR: 1.96e-04\n",
      "  Batch 590/900 | Loss: 0.7235 | CLoss: 0.5589 | FLoss: 0.3292 | LR: 1.96e-04\n",
      "  Batch 600/900 | Loss: 0.6594 | CLoss: 0.5262 | FLoss: 0.2663 | LR: 1.96e-04\n",
      "  Batch 610/900 | Loss: 0.6868 | CLoss: 0.5139 | FLoss: 0.3459 | LR: 1.96e-04\n",
      "  Batch 620/900 | Loss: 0.6326 | CLoss: 0.4796 | FLoss: 0.3060 | LR: 1.96e-04\n",
      "  Batch 630/900 | Loss: 0.5325 | CLoss: 0.3852 | FLoss: 0.2947 | LR: 1.96e-04\n",
      "  Batch 640/900 | Loss: 0.7281 | CLoss: 0.4655 | FLoss: 0.5252 | LR: 1.96e-04\n",
      "  Batch 650/900 | Loss: 0.9354 | CLoss: 0.7502 | FLoss: 0.3704 | LR: 1.96e-04\n",
      "  Batch 660/900 | Loss: 1.0846 | CLoss: 0.9623 | FLoss: 0.2447 | LR: 1.96e-04\n",
      "  Batch 670/900 | Loss: 0.5986 | CLoss: 0.4640 | FLoss: 0.2693 | LR: 1.96e-04\n",
      "  Batch 680/900 | Loss: 0.7336 | CLoss: 0.5770 | FLoss: 0.3132 | LR: 1.96e-04\n",
      "  Batch 690/900 | Loss: 0.6763 | CLoss: 0.5209 | FLoss: 0.3108 | LR: 1.96e-04\n",
      "  Batch 700/900 | Loss: 0.5158 | CLoss: 0.4105 | FLoss: 0.2106 | LR: 1.96e-04\n",
      "  Batch 710/900 | Loss: 1.0995 | CLoss: 0.9384 | FLoss: 0.3222 | LR: 1.96e-04\n",
      "  Batch 720/900 | Loss: 0.7839 | CLoss: 0.5949 | FLoss: 0.3779 | LR: 1.96e-04\n",
      "  Batch 730/900 | Loss: 0.3662 | CLoss: 0.2742 | FLoss: 0.1840 | LR: 1.96e-04\n",
      "  Batch 740/900 | Loss: 0.9441 | CLoss: 0.6695 | FLoss: 0.5492 | LR: 1.96e-04\n",
      "  Batch 750/900 | Loss: 0.9687 | CLoss: 0.7837 | FLoss: 0.3700 | LR: 1.96e-04\n",
      "  Batch 760/900 | Loss: 0.7507 | CLoss: 0.6401 | FLoss: 0.2212 | LR: 1.96e-04\n",
      "  Batch 770/900 | Loss: 1.3300 | CLoss: 1.0630 | FLoss: 0.5339 | LR: 1.96e-04\n",
      "  Batch 780/900 | Loss: 0.6796 | CLoss: 0.5120 | FLoss: 0.3353 | LR: 1.96e-04\n",
      "  Batch 790/900 | Loss: 0.5294 | CLoss: 0.4000 | FLoss: 0.2589 | LR: 1.96e-04\n",
      "  Batch 800/900 | Loss: 0.7512 | CLoss: 0.5804 | FLoss: 0.3415 | LR: 1.96e-04\n",
      "  Batch 810/900 | Loss: 0.7702 | CLoss: 0.6079 | FLoss: 0.3246 | LR: 1.96e-04\n",
      "  Batch 820/900 | Loss: 0.6534 | CLoss: 0.4906 | FLoss: 0.3257 | LR: 1.96e-04\n",
      "  Batch 830/900 | Loss: 0.8353 | CLoss: 0.6580 | FLoss: 0.3545 | LR: 1.96e-04\n",
      "  Batch 840/900 | Loss: 0.6443 | CLoss: 0.4716 | FLoss: 0.3455 | LR: 1.96e-04\n",
      "  Batch 850/900 | Loss: 0.7899 | CLoss: 0.5898 | FLoss: 0.4000 | LR: 1.96e-04\n",
      "  Batch 860/900 | Loss: 0.6716 | CLoss: 0.5180 | FLoss: 0.3073 | LR: 1.96e-04\n",
      "  Batch 870/900 | Loss: 0.5781 | CLoss: 0.3682 | FLoss: 0.4198 | LR: 1.96e-04\n",
      "  Batch 880/900 | Loss: 0.6715 | CLoss: 0.5166 | FLoss: 0.3100 | LR: 1.96e-04\n",
      "  Batch 890/900 | Loss: 0.4684 | CLoss: 0.3009 | FLoss: 0.3349 | LR: 1.96e-04\n",
      "  Batch 900/900 | Loss: 1.0786 | CLoss: 0.8748 | FLoss: 0.4076 | LR: 1.96e-04\n",
      "\n",
      "  Training Summary | Epoch 3\n",
      "  Avg Loss: 0.7227\n",
      "  Last Batch Loss: 1.0786\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.8975 | Batch Acc: 63.79%\n",
      "    Val Batch 010/99 | Loss: 0.3371 | Batch Acc: 89.66%\n",
      "    Val Batch 015/99 | Loss: 0.4546 | Batch Acc: 84.48%\n",
      "    Val Batch 020/99 | Loss: 0.8280 | Batch Acc: 82.76%\n",
      "    Val Batch 025/99 | Loss: 0.2071 | Batch Acc: 94.83%\n",
      "    Val Batch 030/99 | Loss: 0.0755 | Batch Acc: 96.55%\n",
      "    Val Batch 035/99 | Loss: 0.2887 | Batch Acc: 87.93%\n",
      "    Val Batch 040/99 | Loss: 0.3197 | Batch Acc: 79.31%\n",
      "    Val Batch 045/99 | Loss: 0.3216 | Batch Acc: 79.31%\n",
      "    Val Batch 050/99 | Loss: 0.2056 | Batch Acc: 93.10%\n",
      "    Val Batch 055/99 | Loss: 0.1912 | Batch Acc: 93.10%\n",
      "    Val Batch 060/99 | Loss: 0.2448 | Batch Acc: 91.38%\n",
      "    Val Batch 065/99 | Loss: 0.2489 | Batch Acc: 93.10%\n",
      "    Val Batch 070/99 | Loss: 0.2532 | Batch Acc: 94.83%\n",
      "    Val Batch 075/99 | Loss: 0.3137 | Batch Acc: 93.10%\n",
      "    Val Batch 080/99 | Loss: 0.1074 | Batch Acc: 96.55%\n",
      "    Val Batch 085/99 | Loss: 0.1556 | Batch Acc: 94.83%\n",
      "    Val Batch 090/99 | Loss: 0.1884 | Batch Acc: 94.83%\n",
      "    Val Batch 095/99 | Loss: 0.1721 | Batch Acc: 96.55%\n",
      "    Val Batch 099/99 | Loss: 0.0158 | Batch Acc: 100.00%\n",
      "\n",
      "  Validation Summary | Epoch 3\n",
      "  Avg Loss: 0.3117 | Accuracy: 89.19%\n",
      "  Current Best Acc: 89.84%\n",
      "\n",
      "Epoch 4/5\n",
      "  Batch 010/900 | Loss: 0.9997 | CLoss: 0.8511 | FLoss: 0.2972 | LR: 1.04e-04\n",
      "  Batch 020/900 | Loss: 0.5017 | CLoss: 0.3985 | FLoss: 0.2064 | LR: 1.04e-04\n",
      "  Batch 030/900 | Loss: 0.3154 | CLoss: 0.2423 | FLoss: 0.1461 | LR: 1.04e-04\n",
      "  Batch 040/900 | Loss: 0.4774 | CLoss: 0.3608 | FLoss: 0.2333 | LR: 1.04e-04\n",
      "  Batch 050/900 | Loss: 0.8971 | CLoss: 0.7145 | FLoss: 0.3653 | LR: 1.04e-04\n",
      "  Batch 060/900 | Loss: 0.6049 | CLoss: 0.4997 | FLoss: 0.2106 | LR: 1.04e-04\n",
      "  Batch 070/900 | Loss: 0.7249 | CLoss: 0.6083 | FLoss: 0.2333 | LR: 1.04e-04\n",
      "  Batch 080/900 | Loss: 0.4932 | CLoss: 0.3503 | FLoss: 0.2858 | LR: 1.04e-04\n",
      "  Batch 090/900 | Loss: 0.5864 | CLoss: 0.4927 | FLoss: 0.1875 | LR: 1.04e-04\n",
      "  Batch 100/900 | Loss: 1.0118 | CLoss: 0.8130 | FLoss: 0.3977 | LR: 1.04e-04\n",
      "  Batch 110/900 | Loss: 0.5804 | CLoss: 0.4728 | FLoss: 0.2152 | LR: 1.04e-04\n",
      "  Batch 120/900 | Loss: 0.7876 | CLoss: 0.6333 | FLoss: 0.3087 | LR: 1.04e-04\n",
      "  Batch 130/900 | Loss: 0.7097 | CLoss: 0.5107 | FLoss: 0.3980 | LR: 1.04e-04\n",
      "  Batch 140/900 | Loss: 1.2149 | CLoss: 1.0119 | FLoss: 0.4061 | LR: 1.04e-04\n",
      "  Batch 150/900 | Loss: 0.7435 | CLoss: 0.5915 | FLoss: 0.3039 | LR: 1.04e-04\n",
      "  Batch 160/900 | Loss: 0.7114 | CLoss: 0.5369 | FLoss: 0.3491 | LR: 1.04e-04\n",
      "  Batch 170/900 | Loss: 0.4672 | CLoss: 0.3253 | FLoss: 0.2839 | LR: 1.04e-04\n",
      "  Batch 180/900 | Loss: 0.5860 | CLoss: 0.4527 | FLoss: 0.2666 | LR: 1.04e-04\n",
      "  Batch 190/900 | Loss: 0.5635 | CLoss: 0.4164 | FLoss: 0.2943 | LR: 1.04e-04\n",
      "  Batch 200/900 | Loss: 0.4575 | CLoss: 0.3002 | FLoss: 0.3145 | LR: 1.04e-04\n",
      "  Batch 210/900 | Loss: 0.6232 | CLoss: 0.4451 | FLoss: 0.3561 | LR: 1.04e-04\n",
      "  Batch 220/900 | Loss: 0.3868 | CLoss: 0.2906 | FLoss: 0.1924 | LR: 1.04e-04\n",
      "  Batch 230/900 | Loss: 0.5996 | CLoss: 0.4976 | FLoss: 0.2039 | LR: 1.04e-04\n",
      "  Batch 240/900 | Loss: 0.4092 | CLoss: 0.3015 | FLoss: 0.2155 | LR: 1.04e-04\n",
      "  Batch 250/900 | Loss: 0.5787 | CLoss: 0.4366 | FLoss: 0.2842 | LR: 1.04e-04\n",
      "  Batch 260/900 | Loss: 0.7244 | CLoss: 0.5936 | FLoss: 0.2616 | LR: 1.04e-04\n",
      "  Batch 270/900 | Loss: 0.7045 | CLoss: 0.5662 | FLoss: 0.2765 | LR: 1.04e-04\n",
      "  Batch 280/900 | Loss: 0.9165 | CLoss: 0.7446 | FLoss: 0.3438 | LR: 1.04e-04\n",
      "  Batch 290/900 | Loss: 0.5305 | CLoss: 0.4399 | FLoss: 0.1812 | LR: 1.04e-04\n",
      "  Batch 300/900 | Loss: 0.6712 | CLoss: 0.5349 | FLoss: 0.2725 | LR: 1.04e-04\n",
      "  Batch 310/900 | Loss: 0.8246 | CLoss: 0.6306 | FLoss: 0.3880 | LR: 1.04e-04\n",
      "  Batch 320/900 | Loss: 0.4511 | CLoss: 0.3807 | FLoss: 0.1408 | LR: 1.04e-04\n",
      "  Batch 330/900 | Loss: 0.3546 | CLoss: 0.2317 | FLoss: 0.2457 | LR: 1.04e-04\n",
      "  Batch 340/900 | Loss: 0.6390 | CLoss: 0.4802 | FLoss: 0.3175 | LR: 1.04e-04\n",
      "  Batch 350/900 | Loss: 0.7642 | CLoss: 0.6518 | FLoss: 0.2249 | LR: 1.04e-04\n",
      "  Batch 360/900 | Loss: 0.4921 | CLoss: 0.4191 | FLoss: 0.1459 | LR: 1.04e-04\n",
      "  Batch 370/900 | Loss: 0.5761 | CLoss: 0.4147 | FLoss: 0.3229 | LR: 1.04e-04\n",
      "  Batch 380/900 | Loss: 0.4535 | CLoss: 0.2963 | FLoss: 0.3144 | LR: 1.04e-04\n",
      "  Batch 390/900 | Loss: 0.7633 | CLoss: 0.6194 | FLoss: 0.2879 | LR: 1.04e-04\n",
      "  Batch 400/900 | Loss: 0.7206 | CLoss: 0.6253 | FLoss: 0.1907 | LR: 1.04e-04\n",
      "  Batch 410/900 | Loss: 0.7994 | CLoss: 0.6849 | FLoss: 0.2290 | LR: 1.04e-04\n",
      "  Batch 420/900 | Loss: 0.2094 | CLoss: 0.1084 | FLoss: 0.2020 | LR: 1.04e-04\n",
      "  Batch 430/900 | Loss: 0.9509 | CLoss: 0.7135 | FLoss: 0.4748 | LR: 1.04e-04\n",
      "  Batch 440/900 | Loss: 0.5797 | CLoss: 0.4709 | FLoss: 0.2176 | LR: 1.04e-04\n",
      "  Batch 450/900 | Loss: 0.6938 | CLoss: 0.5458 | FLoss: 0.2960 | LR: 1.04e-04\n",
      "  Batch 460/900 | Loss: 0.7048 | CLoss: 0.5526 | FLoss: 0.3044 | LR: 1.04e-04\n",
      "  Batch 470/900 | Loss: 0.6981 | CLoss: 0.5020 | FLoss: 0.3922 | LR: 1.04e-04\n",
      "  Batch 480/900 | Loss: 0.6571 | CLoss: 0.4849 | FLoss: 0.3444 | LR: 1.04e-04\n",
      "  Batch 490/900 | Loss: 0.6328 | CLoss: 0.5317 | FLoss: 0.2021 | LR: 1.04e-04\n",
      "  Batch 500/900 | Loss: 0.4901 | CLoss: 0.3143 | FLoss: 0.3517 | LR: 1.04e-04\n",
      "  Batch 510/900 | Loss: 0.8338 | CLoss: 0.6604 | FLoss: 0.3467 | LR: 1.04e-04\n",
      "  Batch 520/900 | Loss: 0.4028 | CLoss: 0.2871 | FLoss: 0.2314 | LR: 1.04e-04\n",
      "  Batch 530/900 | Loss: 0.5767 | CLoss: 0.4799 | FLoss: 0.1937 | LR: 1.04e-04\n",
      "  Batch 540/900 | Loss: 0.6774 | CLoss: 0.5039 | FLoss: 0.3471 | LR: 1.04e-04\n",
      "  Batch 550/900 | Loss: 0.6896 | CLoss: 0.5483 | FLoss: 0.2825 | LR: 1.04e-04\n",
      "  Batch 560/900 | Loss: 1.0447 | CLoss: 0.8024 | FLoss: 0.4846 | LR: 1.04e-04\n",
      "  Batch 570/900 | Loss: 0.3520 | CLoss: 0.2844 | FLoss: 0.1353 | LR: 1.04e-04\n",
      "  Batch 580/900 | Loss: 0.5576 | CLoss: 0.4076 | FLoss: 0.2999 | LR: 1.04e-04\n",
      "  Batch 590/900 | Loss: 0.7715 | CLoss: 0.6465 | FLoss: 0.2500 | LR: 1.04e-04\n",
      "  Batch 600/900 | Loss: 0.7865 | CLoss: 0.6155 | FLoss: 0.3419 | LR: 1.04e-04\n",
      "  Batch 610/900 | Loss: 0.8458 | CLoss: 0.6582 | FLoss: 0.3753 | LR: 1.04e-04\n",
      "  Batch 620/900 | Loss: 0.5476 | CLoss: 0.4146 | FLoss: 0.2660 | LR: 1.04e-04\n",
      "  Batch 630/900 | Loss: 0.8065 | CLoss: 0.6875 | FLoss: 0.2381 | LR: 1.04e-04\n",
      "  Batch 640/900 | Loss: 0.6933 | CLoss: 0.5813 | FLoss: 0.2240 | LR: 1.04e-04\n",
      "  Batch 650/900 | Loss: 0.6268 | CLoss: 0.5073 | FLoss: 0.2390 | LR: 1.04e-04\n",
      "  Batch 660/900 | Loss: 0.5878 | CLoss: 0.4793 | FLoss: 0.2169 | LR: 1.04e-04\n",
      "  Batch 670/900 | Loss: 0.7407 | CLoss: 0.6330 | FLoss: 0.2155 | LR: 1.04e-04\n",
      "  Batch 680/900 | Loss: 0.7241 | CLoss: 0.4685 | FLoss: 0.5111 | LR: 1.04e-04\n",
      "  Batch 690/900 | Loss: 0.5258 | CLoss: 0.4089 | FLoss: 0.2338 | LR: 1.04e-04\n",
      "  Batch 700/900 | Loss: 0.7115 | CLoss: 0.5720 | FLoss: 0.2789 | LR: 1.04e-04\n",
      "  Batch 710/900 | Loss: 0.9332 | CLoss: 0.7393 | FLoss: 0.3878 | LR: 1.04e-04\n",
      "  Batch 720/900 | Loss: 0.2797 | CLoss: 0.2270 | FLoss: 0.1053 | LR: 1.04e-04\n",
      "  Batch 730/900 | Loss: 1.0039 | CLoss: 0.8148 | FLoss: 0.3783 | LR: 1.04e-04\n",
      "  Batch 740/900 | Loss: 0.5798 | CLoss: 0.4754 | FLoss: 0.2089 | LR: 1.04e-04\n",
      "  Batch 750/900 | Loss: 0.6725 | CLoss: 0.5158 | FLoss: 0.3134 | LR: 1.04e-04\n",
      "  Batch 760/900 | Loss: 0.7225 | CLoss: 0.5206 | FLoss: 0.4039 | LR: 1.04e-04\n",
      "  Batch 770/900 | Loss: 0.5126 | CLoss: 0.3396 | FLoss: 0.3459 | LR: 1.04e-04\n",
      "  Batch 780/900 | Loss: 0.3767 | CLoss: 0.2964 | FLoss: 0.1605 | LR: 1.04e-04\n",
      "  Batch 790/900 | Loss: 0.9382 | CLoss: 0.6691 | FLoss: 0.5382 | LR: 1.04e-04\n",
      "  Batch 800/900 | Loss: 0.4469 | CLoss: 0.3262 | FLoss: 0.2413 | LR: 1.04e-04\n",
      "  Batch 810/900 | Loss: 0.5964 | CLoss: 0.4411 | FLoss: 0.3106 | LR: 1.04e-04\n",
      "  Batch 820/900 | Loss: 0.6340 | CLoss: 0.5326 | FLoss: 0.2028 | LR: 1.04e-04\n",
      "  Batch 830/900 | Loss: 0.5966 | CLoss: 0.4032 | FLoss: 0.3869 | LR: 1.04e-04\n",
      "  Batch 840/900 | Loss: 0.4377 | CLoss: 0.3228 | FLoss: 0.2298 | LR: 1.04e-04\n",
      "  Batch 850/900 | Loss: 0.6401 | CLoss: 0.5633 | FLoss: 0.1536 | LR: 1.04e-04\n",
      "  Batch 860/900 | Loss: 0.3314 | CLoss: 0.2717 | FLoss: 0.1195 | LR: 1.04e-04\n",
      "  Batch 870/900 | Loss: 0.4981 | CLoss: 0.4280 | FLoss: 0.1402 | LR: 1.04e-04\n",
      "  Batch 880/900 | Loss: 0.6067 | CLoss: 0.4997 | FLoss: 0.2140 | LR: 1.04e-04\n",
      "  Batch 890/900 | Loss: 0.4889 | CLoss: 0.3980 | FLoss: 0.1818 | LR: 1.04e-04\n",
      "  Batch 900/900 | Loss: 0.4937 | CLoss: 0.4113 | FLoss: 0.1648 | LR: 1.04e-04\n",
      "\n",
      "  Training Summary | Epoch 4\n",
      "  Avg Loss: 0.6558\n",
      "  Last Batch Loss: 0.4937\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.8963 | Batch Acc: 62.07%\n",
      "    Val Batch 010/99 | Loss: 0.4666 | Batch Acc: 86.21%\n",
      "    Val Batch 015/99 | Loss: 0.5200 | Batch Acc: 81.03%\n",
      "    Val Batch 020/99 | Loss: 0.7593 | Batch Acc: 82.76%\n",
      "    Val Batch 025/99 | Loss: 0.1599 | Batch Acc: 93.10%\n",
      "    Val Batch 030/99 | Loss: 0.1061 | Batch Acc: 96.55%\n",
      "    Val Batch 035/99 | Loss: 0.1677 | Batch Acc: 94.83%\n",
      "    Val Batch 040/99 | Loss: 0.3683 | Batch Acc: 74.14%\n",
      "    Val Batch 045/99 | Loss: 0.3800 | Batch Acc: 72.41%\n",
      "    Val Batch 050/99 | Loss: 0.1717 | Batch Acc: 93.10%\n",
      "    Val Batch 055/99 | Loss: 0.0297 | Batch Acc: 100.00%\n",
      "    Val Batch 060/99 | Loss: 0.0673 | Batch Acc: 98.28%\n",
      "    Val Batch 065/99 | Loss: 0.3526 | Batch Acc: 86.21%\n",
      "    Val Batch 070/99 | Loss: 0.1764 | Batch Acc: 93.10%\n",
      "    Val Batch 075/99 | Loss: 0.1761 | Batch Acc: 94.83%\n",
      "    Val Batch 080/99 | Loss: 0.3105 | Batch Acc: 91.38%\n",
      "    Val Batch 085/99 | Loss: 0.1108 | Batch Acc: 94.83%\n",
      "    Val Batch 090/99 | Loss: 0.0805 | Batch Acc: 96.55%\n",
      "    Val Batch 095/99 | Loss: 0.3116 | Batch Acc: 94.83%\n",
      "    Val Batch 099/99 | Loss: 0.2704 | Batch Acc: 94.29%\n",
      "\n",
      "  Validation Summary | Epoch 4\n",
      "  Avg Loss: 0.2685 | Accuracy: 90.09%\n",
      "  Current Best Acc: 90.09%\n",
      "\n",
      "Epoch 5/5\n",
      "  Batch 010/900 | Loss: 0.6455 | CLoss: 0.4911 | FLoss: 0.3088 | LR: 2.86e-05\n",
      "  Batch 020/900 | Loss: 0.7051 | CLoss: 0.5497 | FLoss: 0.3109 | LR: 2.86e-05\n",
      "  Batch 030/900 | Loss: 0.5215 | CLoss: 0.4122 | FLoss: 0.2186 | LR: 2.86e-05\n",
      "  Batch 040/900 | Loss: 0.7387 | CLoss: 0.5947 | FLoss: 0.2879 | LR: 2.86e-05\n",
      "  Batch 050/900 | Loss: 0.5955 | CLoss: 0.4671 | FLoss: 0.2568 | LR: 2.86e-05\n",
      "  Batch 060/900 | Loss: 0.6228 | CLoss: 0.4462 | FLoss: 0.3533 | LR: 2.86e-05\n",
      "  Batch 070/900 | Loss: 0.6158 | CLoss: 0.4936 | FLoss: 0.2442 | LR: 2.86e-05\n",
      "  Batch 080/900 | Loss: 0.7723 | CLoss: 0.5972 | FLoss: 0.3501 | LR: 2.86e-05\n",
      "  Batch 090/900 | Loss: 0.6181 | CLoss: 0.4205 | FLoss: 0.3952 | LR: 2.86e-05\n",
      "  Batch 100/900 | Loss: 0.9029 | CLoss: 0.7509 | FLoss: 0.3041 | LR: 2.86e-05\n",
      "  Batch 110/900 | Loss: 0.6553 | CLoss: 0.5180 | FLoss: 0.2746 | LR: 2.86e-05\n",
      "  Batch 120/900 | Loss: 0.3275 | CLoss: 0.2471 | FLoss: 0.1608 | LR: 2.86e-05\n",
      "  Batch 130/900 | Loss: 0.5901 | CLoss: 0.4392 | FLoss: 0.3017 | LR: 2.86e-05\n",
      "  Batch 140/900 | Loss: 0.3839 | CLoss: 0.3079 | FLoss: 0.1522 | LR: 2.86e-05\n",
      "  Batch 150/900 | Loss: 0.5491 | CLoss: 0.4623 | FLoss: 0.1736 | LR: 2.86e-05\n",
      "  Batch 160/900 | Loss: 0.3688 | CLoss: 0.2384 | FLoss: 0.2609 | LR: 2.86e-05\n",
      "  Batch 170/900 | Loss: 0.3005 | CLoss: 0.2310 | FLoss: 0.1389 | LR: 2.86e-05\n",
      "  Batch 180/900 | Loss: 0.4685 | CLoss: 0.3901 | FLoss: 0.1568 | LR: 2.86e-05\n",
      "  Batch 190/900 | Loss: 0.3804 | CLoss: 0.2661 | FLoss: 0.2286 | LR: 2.86e-05\n",
      "  Batch 200/900 | Loss: 0.6178 | CLoss: 0.4144 | FLoss: 0.4070 | LR: 2.86e-05\n",
      "  Batch 210/900 | Loss: 0.7131 | CLoss: 0.5347 | FLoss: 0.3568 | LR: 2.86e-05\n",
      "  Batch 220/900 | Loss: 0.4127 | CLoss: 0.3012 | FLoss: 0.2230 | LR: 2.86e-05\n",
      "  Batch 230/900 | Loss: 0.5418 | CLoss: 0.4071 | FLoss: 0.2695 | LR: 2.86e-05\n",
      "  Batch 240/900 | Loss: 0.7772 | CLoss: 0.5747 | FLoss: 0.4048 | LR: 2.86e-05\n",
      "  Batch 250/900 | Loss: 0.7209 | CLoss: 0.5678 | FLoss: 0.3062 | LR: 2.86e-05\n",
      "  Batch 260/900 | Loss: 0.8941 | CLoss: 0.6887 | FLoss: 0.4108 | LR: 2.86e-05\n",
      "  Batch 270/900 | Loss: 0.6441 | CLoss: 0.5344 | FLoss: 0.2196 | LR: 2.86e-05\n",
      "  Batch 280/900 | Loss: 0.9771 | CLoss: 0.8189 | FLoss: 0.3165 | LR: 2.86e-05\n",
      "  Batch 290/900 | Loss: 0.5572 | CLoss: 0.4277 | FLoss: 0.2590 | LR: 2.86e-05\n",
      "  Batch 300/900 | Loss: 0.5453 | CLoss: 0.4361 | FLoss: 0.2184 | LR: 2.86e-05\n",
      "  Batch 310/900 | Loss: 1.1291 | CLoss: 0.9455 | FLoss: 0.3672 | LR: 2.86e-05\n",
      "  Batch 320/900 | Loss: 0.5681 | CLoss: 0.4195 | FLoss: 0.2971 | LR: 2.86e-05\n",
      "  Batch 330/900 | Loss: 0.8318 | CLoss: 0.6486 | FLoss: 0.3663 | LR: 2.86e-05\n",
      "  Batch 340/900 | Loss: 0.3658 | CLoss: 0.2952 | FLoss: 0.1410 | LR: 2.86e-05\n",
      "  Batch 350/900 | Loss: 0.5772 | CLoss: 0.4486 | FLoss: 0.2572 | LR: 2.86e-05\n",
      "  Batch 360/900 | Loss: 0.9843 | CLoss: 0.8498 | FLoss: 0.2691 | LR: 2.86e-05\n",
      "  Batch 370/900 | Loss: 0.4776 | CLoss: 0.3555 | FLoss: 0.2442 | LR: 2.86e-05\n",
      "  Batch 380/900 | Loss: 0.4258 | CLoss: 0.3092 | FLoss: 0.2332 | LR: 2.86e-05\n",
      "  Batch 390/900 | Loss: 0.5927 | CLoss: 0.4599 | FLoss: 0.2656 | LR: 2.86e-05\n",
      "  Batch 400/900 | Loss: 0.9827 | CLoss: 0.7406 | FLoss: 0.4843 | LR: 2.86e-05\n",
      "  Batch 410/900 | Loss: 0.6579 | CLoss: 0.5129 | FLoss: 0.2901 | LR: 2.86e-05\n",
      "  Batch 420/900 | Loss: 0.4659 | CLoss: 0.3391 | FLoss: 0.2536 | LR: 2.86e-05\n",
      "  Batch 430/900 | Loss: 0.5549 | CLoss: 0.3511 | FLoss: 0.4077 | LR: 2.86e-05\n",
      "  Batch 440/900 | Loss: 0.5302 | CLoss: 0.3691 | FLoss: 0.3222 | LR: 2.86e-05\n",
      "  Batch 450/900 | Loss: 0.5832 | CLoss: 0.4554 | FLoss: 0.2558 | LR: 2.86e-05\n",
      "  Batch 460/900 | Loss: 0.3954 | CLoss: 0.3015 | FLoss: 0.1879 | LR: 2.86e-05\n",
      "  Batch 470/900 | Loss: 0.7485 | CLoss: 0.6186 | FLoss: 0.2597 | LR: 2.86e-05\n",
      "  Batch 480/900 | Loss: 0.7830 | CLoss: 0.5824 | FLoss: 0.4012 | LR: 2.86e-05\n",
      "  Batch 490/900 | Loss: 0.2969 | CLoss: 0.2059 | FLoss: 0.1819 | LR: 2.86e-05\n",
      "  Batch 500/900 | Loss: 0.7816 | CLoss: 0.6581 | FLoss: 0.2470 | LR: 2.86e-05\n",
      "  Batch 510/900 | Loss: 1.2806 | CLoss: 0.9637 | FLoss: 0.6338 | LR: 2.86e-05\n",
      "  Batch 520/900 | Loss: 0.5135 | CLoss: 0.4718 | FLoss: 0.0834 | LR: 2.86e-05\n",
      "  Batch 530/900 | Loss: 0.9133 | CLoss: 0.7738 | FLoss: 0.2791 | LR: 2.86e-05\n",
      "  Batch 540/900 | Loss: 0.7746 | CLoss: 0.6453 | FLoss: 0.2586 | LR: 2.86e-05\n",
      "  Batch 550/900 | Loss: 0.5220 | CLoss: 0.4286 | FLoss: 0.1867 | LR: 2.86e-05\n",
      "  Batch 560/900 | Loss: 0.4440 | CLoss: 0.3728 | FLoss: 0.1424 | LR: 2.86e-05\n",
      "  Batch 570/900 | Loss: 0.6288 | CLoss: 0.5365 | FLoss: 0.1846 | LR: 2.86e-05\n",
      "  Batch 580/900 | Loss: 0.7514 | CLoss: 0.5291 | FLoss: 0.4446 | LR: 2.86e-05\n",
      "  Batch 590/900 | Loss: 0.5083 | CLoss: 0.3569 | FLoss: 0.3028 | LR: 2.86e-05\n",
      "  Batch 600/900 | Loss: 0.5152 | CLoss: 0.4113 | FLoss: 0.2079 | LR: 2.86e-05\n",
      "  Batch 610/900 | Loss: 0.6107 | CLoss: 0.4208 | FLoss: 0.3798 | LR: 2.86e-05\n",
      "  Batch 620/900 | Loss: 0.3927 | CLoss: 0.3164 | FLoss: 0.1526 | LR: 2.86e-05\n",
      "  Batch 630/900 | Loss: 0.6502 | CLoss: 0.5046 | FLoss: 0.2912 | LR: 2.86e-05\n",
      "  Batch 640/900 | Loss: 0.7173 | CLoss: 0.5890 | FLoss: 0.2565 | LR: 2.86e-05\n",
      "  Batch 650/900 | Loss: 0.8065 | CLoss: 0.6144 | FLoss: 0.3843 | LR: 2.86e-05\n",
      "  Batch 660/900 | Loss: 0.6512 | CLoss: 0.5369 | FLoss: 0.2285 | LR: 2.86e-05\n",
      "  Batch 670/900 | Loss: 0.8330 | CLoss: 0.7082 | FLoss: 0.2496 | LR: 2.86e-05\n",
      "  Batch 680/900 | Loss: 0.5831 | CLoss: 0.4739 | FLoss: 0.2183 | LR: 2.86e-05\n",
      "  Batch 690/900 | Loss: 0.7498 | CLoss: 0.5509 | FLoss: 0.3979 | LR: 2.86e-05\n",
      "  Batch 700/900 | Loss: 0.4213 | CLoss: 0.3448 | FLoss: 0.1530 | LR: 2.86e-05\n",
      "  Batch 710/900 | Loss: 0.5057 | CLoss: 0.4389 | FLoss: 0.1336 | LR: 2.86e-05\n",
      "  Batch 720/900 | Loss: 0.5060 | CLoss: 0.4066 | FLoss: 0.1989 | LR: 2.86e-05\n",
      "  Batch 730/900 | Loss: 0.7300 | CLoss: 0.5897 | FLoss: 0.2805 | LR: 2.86e-05\n",
      "  Batch 740/900 | Loss: 0.5869 | CLoss: 0.4636 | FLoss: 0.2465 | LR: 2.86e-05\n",
      "  Batch 750/900 | Loss: 0.8238 | CLoss: 0.6914 | FLoss: 0.2649 | LR: 2.86e-05\n",
      "  Batch 760/900 | Loss: 0.3883 | CLoss: 0.2604 | FLoss: 0.2559 | LR: 2.86e-05\n",
      "  Batch 770/900 | Loss: 0.5402 | CLoss: 0.4070 | FLoss: 0.2664 | LR: 2.86e-05\n",
      "  Batch 780/900 | Loss: 0.4755 | CLoss: 0.3820 | FLoss: 0.1870 | LR: 2.86e-05\n",
      "  Batch 790/900 | Loss: 0.3104 | CLoss: 0.2522 | FLoss: 0.1165 | LR: 2.86e-05\n",
      "  Batch 800/900 | Loss: 0.7181 | CLoss: 0.6030 | FLoss: 0.2301 | LR: 2.86e-05\n",
      "  Batch 810/900 | Loss: 0.7966 | CLoss: 0.6279 | FLoss: 0.3375 | LR: 2.86e-05\n",
      "  Batch 820/900 | Loss: 0.7315 | CLoss: 0.6063 | FLoss: 0.2506 | LR: 2.86e-05\n",
      "  Batch 830/900 | Loss: 0.3277 | CLoss: 0.2457 | FLoss: 0.1640 | LR: 2.86e-05\n",
      "  Batch 840/900 | Loss: 0.2947 | CLoss: 0.2312 | FLoss: 0.1269 | LR: 2.86e-05\n",
      "  Batch 850/900 | Loss: 0.7243 | CLoss: 0.5810 | FLoss: 0.2866 | LR: 2.86e-05\n",
      "  Batch 860/900 | Loss: 0.5137 | CLoss: 0.4251 | FLoss: 0.1772 | LR: 2.86e-05\n",
      "  Batch 870/900 | Loss: 0.6227 | CLoss: 0.4908 | FLoss: 0.2639 | LR: 2.86e-05\n",
      "  Batch 880/900 | Loss: 0.2413 | CLoss: 0.1705 | FLoss: 0.1416 | LR: 2.86e-05\n",
      "  Batch 890/900 | Loss: 0.7309 | CLoss: 0.6279 | FLoss: 0.2059 | LR: 2.86e-05\n",
      "  Batch 900/900 | Loss: 0.6335 | CLoss: 0.5074 | FLoss: 0.2521 | LR: 2.86e-05\n",
      "\n",
      "  Training Summary | Epoch 5\n",
      "  Avg Loss: 0.6066\n",
      "  Last Batch Loss: 0.6335\n",
      "\n",
      "  Validating...\n",
      "    Val Batch 005/99 | Loss: 0.8590 | Batch Acc: 68.97%\n",
      "    Val Batch 010/99 | Loss: 0.2597 | Batch Acc: 93.10%\n",
      "    Val Batch 015/99 | Loss: 0.4039 | Batch Acc: 79.31%\n",
      "    Val Batch 020/99 | Loss: 0.5714 | Batch Acc: 72.41%\n",
      "    Val Batch 025/99 | Loss: 0.1768 | Batch Acc: 93.10%\n",
      "    Val Batch 030/99 | Loss: 0.0630 | Batch Acc: 98.28%\n",
      "    Val Batch 035/99 | Loss: 0.0505 | Batch Acc: 98.28%\n",
      "    Val Batch 040/99 | Loss: 0.2939 | Batch Acc: 94.83%\n",
      "    Val Batch 045/99 | Loss: 0.2918 | Batch Acc: 94.83%\n",
      "    Val Batch 050/99 | Loss: 0.2438 | Batch Acc: 94.83%\n",
      "    Val Batch 055/99 | Loss: 0.0740 | Batch Acc: 96.55%\n",
      "    Val Batch 060/99 | Loss: 0.0987 | Batch Acc: 98.28%\n",
      "    Val Batch 065/99 | Loss: 0.2235 | Batch Acc: 93.10%\n",
      "    Val Batch 070/99 | Loss: 0.0847 | Batch Acc: 98.28%\n",
      "    Val Batch 075/99 | Loss: 0.0390 | Batch Acc: 100.00%\n",
      "    Val Batch 080/99 | Loss: 0.2276 | Batch Acc: 91.38%\n",
      "    Val Batch 085/99 | Loss: 0.2795 | Batch Acc: 89.66%\n",
      "    Val Batch 090/99 | Loss: 0.2262 | Batch Acc: 91.38%\n",
      "    Val Batch 095/99 | Loss: 0.1011 | Batch Acc: 94.83%\n",
      "    Val Batch 099/99 | Loss: 0.0943 | Batch Acc: 97.14%\n",
      "\n",
      "  Validation Summary | Epoch 5\n",
      "  Avg Loss: 0.2519 | Accuracy: 90.98%\n",
      "  Current Best Acc: 90.98%\n",
      "\n",
      "========================================\n",
      "=== Fold 10 Completed ===\n",
      "Best Validation Accuracy: 90.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda:0\"\n",
    "num_epochs = 5\n",
    "num_folds = 10\n",
    "batch_size = 58\n",
    "feature_dim = 2048\n",
    "num_classes = 13\n",
    "root_dir = './spectrograms'\n",
    "csv_file = \"./spectrograms_balanced_no_sirens.csv\"\n",
    "\n",
    "# Get class names\n",
    "full_annotations = pd.read_csv(csv_file)\n",
    "class_names = sorted(full_annotations['classID'].unique())\n",
    "\n",
    "# Dataset Class\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, folds, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "\n",
    "        if isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        self.file_list = self.annotations[self.annotations['fold'].isin(folds)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.file_list.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, f'fold{row[\"fold\"]}', row['spec_file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = row['classID']\n",
    "\n",
    "        if self.transform:\n",
    "            xi = self.transform(image)\n",
    "            xj = self.transform(image)\n",
    "            return xi, xj, label\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Model Components\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2048, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Loss Function\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "\n",
    "        # Create labels: positives are the N off-diagonal elements\n",
    "        labels = torch.cat([\n",
    "            torch.arange(N, 2*N, device=z.device),\n",
    "            torch.arange(0, N, device=z.device)\n",
    "        ])\n",
    "\n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
    "        sim = sim.masked_fill(mask, -1e9)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "#tried but does work require more memeory with batch size 64\n",
    "# def mixup_data(x1, x2, y1, y2, alpha=0.2):\n",
    "#     '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "#     lam = np.random.beta(alpha, alpha)\n",
    "#     batch_size = x1.size(0)\n",
    "#     index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "#     mixed_x = lam * x1 + (1 - lam) * x2\n",
    "#     y_a, y_b = y1, y2\n",
    "#     return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# Training Function with Mixup and CosineAnnealingLR\n",
    "def train():\n",
    "    # Initialize backbone\n",
    "    backbone = models.resnet50(pretrained=True)\n",
    "    backbone.fc = torch.nn.Identity()\n",
    "\n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    for fold in range(1, num_folds+1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold}/{num_folds} {'='*20}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "\n",
    "        # Data loaders\n",
    "        train_ds = UrbanSoundDataset(root_dir, [f for f in range(1,11) if f != fold],\n",
    "                                   csv_file, transform)\n",
    "        val_ds = UrbanSoundDataset(root_dir, [fold], csv_file, transform)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        # Model components\n",
    "        simclr = SimCLR(backbone).to(device)\n",
    "        classifier = Classifier().to(device)\n",
    "        optimizer = torch.optim.Adam(list(simclr.parameters()) + list(classifier.parameters()), lr=3e-4)\n",
    "        criterion = NTXentLoss()\n",
    "\n",
    "        # Cosine Annealing learning rate scheduler\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "        # Metrics storage\n",
    "        train_losses, val_losses = [], []\n",
    "        val_accuracies, all_preds, all_labels = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            # Training Phase\n",
    "            simclr.train()\n",
    "            classifier.train()\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_idx, (xi, xj, labels) in enumerate(train_loader):\n",
    "                xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                zi, zj = simclr(xi), simclr(xj)\n",
    "                loss_contrastive = criterion(zi, zj)\n",
    "\n",
    "                # Classification\n",
    "                features = simclr.backbone(xi)\n",
    "                logits = classifier(features)\n",
    "                loss_classification = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "                # Total loss\n",
    "                loss = loss_contrastive + 0.5 * loss_classification\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Progress tracking\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print batch updates\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"  Batch {batch_idx + 1:03d}/{len(train_loader)} | \"\n",
    "                          f\"Loss: {loss.item():.4f} | \"\n",
    "                          f\"CLoss: {loss_contrastive.item():.4f} | \"\n",
    "                          f\"FLoss: {loss_classification.item():.4f} | \"\n",
    "                          f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "            # Epoch statistics\n",
    "            avg_train_loss = epoch_loss / batch_count\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"\\n  Training Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Last Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Validation Phase\n",
    "            simclr.eval()\n",
    "            classifier.eval()\n",
    "            val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "            print(\"\\n  Validating...\")\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xi, _, labels) in enumerate(val_loader):\n",
    "                    xi, labels = xi.to(device), labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    features = simclr.backbone(xi)\n",
    "                    logits = classifier(features)\n",
    "\n",
    "                    # Loss calculation\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Accuracy calculation\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Store predictions\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                    # Validation batch updates\n",
    "                    if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                        acc = 100 * (preds == labels).sum().item() / labels.size(0)\n",
    "                        print(f\"    Val Batch {batch_idx + 1:03d}/{len(val_loader)} | \"\n",
    "                              f\"Loss: {loss.item():.4f} | \"\n",
    "                              f\"Batch Acc: {acc:.2f}%\")\n",
    "\n",
    "            # Validation statistics\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_acc = 100 * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            print(f\"\\n  Validation Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
    "            print(f\"  Current Best Acc: {max(val_accuracies):.2f}%\")\n",
    "\n",
    "            # Step the scheduler\n",
    "\n",
    "        # Fold Completion\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold} Completed ===\")\n",
    "        print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
    "        checkpoint_dir = 'checkpoints'  # Or provide an absolute path like '/path/to/save'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "        torch.save({\n",
    "              'simclr': simclr.state_dict(),\n",
    "              'classifier': classifier.state_dict(),\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "          }, os.path.join(checkpoint_dir, f'fold_{fold}_checkpoint.pth'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "OVMcxLqn75pX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVMcxLqn75pX",
    "outputId": "ee904f8f-71f7-4e78-d3e9-1404ae758a01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-61890aece928>:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  spec.sort_values('classID').groupby('fold').apply(lambda x: x['classID'].unique())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['air_conditioner', 'ambulance', 'car_horn', 'children_playing',\n",
       "       'dog_bark', 'drilling', 'engine_idling', 'firetruck', 'gun_shot',\n",
       "       'jackhammer', 'police', 'street_music', 'traffic'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = pd.read_csv('/content/spectrograms_balanced_no_sirens.csv')\n",
    "spec.sort_values('classID').groupby('fold').apply(lambda x: x['classID'].unique())\n",
    "spec.sort_values('classID')['class'].unique()\n",
    "# spec[spec['classID'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c2c9d11-4ce6-41c4-8dee-5dded2647d4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c2c9d11-4ce6-41c4-8dee-5dded2647d4f",
    "outputId": "15ac17c7-f06b-4962-f74a-ea5495ac35f3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints/fold_10_checkpoint.pth\n",
      "Checkpoint keys: dict_keys(['simclr', 'classifier', 'optimizer'])\n",
      "Processing audio: 127873-0-0-0.wav\n",
      "spec_file_name    127873-0-orig.png\n",
      "orig_file_name     127873-0-0-0.wav\n",
      "fsID                         127873\n",
      "fold                              1\n",
      "classID                           0\n",
      "class               air_conditioner\n",
      "augmentation               original\n",
      "Name: 1330, dtype: object\n",
      "/content/spectrograms/fold1/127873-0-orig.png\n",
      "[0.73637927 0.00793837 0.04359502 0.00095662 0.07454624 0.01800478\n",
      " 0.03493095 0.01103815 0.03062015 0.02225268 0.00778793 0.00078703\n",
      " 0.0111628 ]\n",
      "[11  3 10  1  7 12  5  9  8  6  2  4  0]\n",
      "\n",
      "Top 3 predictions:\n",
      "1. street_music: 0.08%\n",
      "2. children_playing: 0.10%\n",
      "3. police: 0.78%\n",
      "4. ambulance: 0.79%\n",
      "5. firetruck: 1.10%\n",
      "6. traffic: 1.12%\n",
      "7. drilling: 1.80%\n",
      "8. jackhammer: 2.23%\n",
      "9. gun_shot: 3.06%\n",
      "10. engine_idling: 3.49%\n",
      "11. car_horn: 4.36%\n",
      "12. dog_bark: 7.45%\n",
      "13. air_conditioner: 73.64%\n",
      "\n",
      "Final prediction: air_conditioner with 73.64% confidence\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Model Components (same as in training script)\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2048, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def process_audio(audio_path, sr=22050, duration=None, n_mels=224):\n",
    "    \"\"\"Process audio file to mel spectrogram\"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "\n",
    "    # Create mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048,\n",
    "                                      hop_length=512, n_mels=n_mels)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Convert to image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    # Save temporarily and load as PIL\n",
    "    temp_path = 'temp_spec.png'\n",
    "    plt.savefig(temp_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    img = Image.open(temp_path).convert('RGB')\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "\n",
    "    return img, y, sr\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, checkpoint_path, class_names, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize model components\n",
    "        backbone = models.resnet50(pretrained=False)\n",
    "        backbone.fc = torch.nn.Identity()\n",
    "\n",
    "        self.simclr = SimCLR(backbone).to(self.device)\n",
    "        self.classifier = Classifier().to(self.device)\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Load checkpoint\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "\n",
    "        # Debug: print keys\n",
    "        print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "\n",
    "        self.simclr.load_state_dict(checkpoint['simclr'])\n",
    "        self.classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "        # Set to evaluation mode\n",
    "        self.simclr.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        # Define transforms (similar to validation transforms)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def predict(self, audio_path, verbose=True):\n",
    "        \"\"\"Predict class for audio file\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Processing audio: {audio_path}\")\n",
    "\n",
    "        # Process audio to spectrogram image\n",
    "        # img, audio, sr = process_audio(audio_path)\n",
    "        #extract the the img\n",
    "        r = pd.read_csv('./spectrograms_balanced_no_sirens.csv')\n",
    "        img = r[r['orig_file_name'] == audio_path].iloc[0]\n",
    "        print(img)\n",
    "        path= '/content/spectrograms/'+ 'fold'+str(img['fold']) + '/' + img['spec_file_name']\n",
    "        print(path)\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            # Extract features\n",
    "            features = self.simclr.backbone(img_tensor)\n",
    "\n",
    "            # Get logits from classifier\n",
    "            logits = self.classifier(features)\n",
    "\n",
    "            # Convert to probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            print(probs)\n",
    "\n",
    "        if verbose:\n",
    "            # Print top 3 predictions\n",
    "            top_indices = np.argsort(probs)\n",
    "            print(top_indices)\n",
    "            print(\"\\nTop 3 predictions:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1}. {self.class_names[idx]}: {probs[idx]*100:.2f}%\")\n",
    "\n",
    "        return probs\n",
    "\n",
    "def visualize_prediction(probs, class_names):\n",
    "    \"\"\"Visualize prediction with audio playback\"\"\"\n",
    "    # Play audio\n",
    "    # print(\"Audio sample:\")\n",
    "    # display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "    # Plot waveform and probabilities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Waveform\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio)\n",
    "    # plt.title(\"Waveform\")\n",
    "    # plt.xlabel(\"Time (s)\")\n",
    "    # plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    # Class probabilities\n",
    "    plt.subplot(1, 2, 2)\n",
    "    indices = np.argsort(probs)[::-1]\n",
    "    plt.barh(range(len(class_names)), [probs[i] for i in indices])\n",
    "    plt.yticks(range(len(class_names)), [class_names[i] for i in indices])\n",
    "    plt.title(\"Class Probabilities\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define class names (same as in training)\n",
    "    class_names = ['air_conditioner', 'ambulance', 'car_horn', 'children_playing',\n",
    "       'dog_bark', 'drilling', 'engine_idling', 'firetruck', 'gun_shot',\n",
    "       'jackhammer', 'police', 'street_music', 'traffic']\n",
    "\n",
    "    # Initialize classifier\n",
    "    classifier = AudioClassifier(\n",
    "        checkpoint_path='./checkpoints/fold_10_checkpoint.pth',\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    audio_path = \"127873-0-0-0.wav\"  # Replace with your audio file\n",
    "    probs = classifier.predict(audio_path)\n",
    "\n",
    "    # Visualize\n",
    "    # visualize_prediction(probs, class_names)\n",
    "\n",
    "    # Get top prediction\n",
    "    top_idx = np.argmax(probs)\n",
    "    print(f\"\\nFinal prediction: {class_names[top_idx]} with {probs[top_idx]*100:.2f}% confidence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "C1xS77Ogrf4k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1xS77Ogrf4k",
    "outputId": "c2677dbe-d83a-4436-ce15-09087f82f744"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints/fold_10_checkpoint.pth\n",
      "Checkpoint keys: dict_keys(['simclr', 'classifier', 'optimizer'])\n",
      "\n",
      "Top 1% Accuracy per class:\n",
      "+------------------------+-------------------+\n",
      "| Class                  | Top 1% Accuracy   |\n",
      "+========================+===================+\n",
      "| Class air_conditioner  | 95.10%            |\n",
      "+------------------------+-------------------+\n",
      "| Class ambulance        | 98.00%            |\n",
      "+------------------------+-------------------+\n",
      "| Class car_horn         | 73.23%            |\n",
      "+------------------------+-------------------+\n",
      "| Class children_playing | 89.53%            |\n",
      "+------------------------+-------------------+\n",
      "| Class dog_bark         | 69.93%            |\n",
      "+------------------------+-------------------+\n",
      "| Class drilling         | 90.65%            |\n",
      "+------------------------+-------------------+\n",
      "| Class engine_idling    | 93.54%            |\n",
      "+------------------------+-------------------+\n",
      "| Class firetruck        | 94.21%            |\n",
      "+------------------------+-------------------+\n",
      "| Class gun_shot         | 96.88%            |\n",
      "+------------------------+-------------------+\n",
      "| Class jackhammer       | 97.77%            |\n",
      "+------------------------+-------------------+\n",
      "| Class police           | 93.54%            |\n",
      "+------------------------+-------------------+\n",
      "| Class street_music     | 95.32%            |\n",
      "+------------------------+-------------------+\n",
      "| Class traffic          | 96.88%            |\n",
      "+------------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def top_1_percent_accuracy_per_class(logits, labels, num_classes, top_percent=1):\n",
    "    \"\"\"\n",
    "    Calculate the top 1% accuracy for each class.\n",
    "\n",
    "    Args:\n",
    "        logits: The model's raw output (before softmax), shape (batch_size, num_classes).\n",
    "        labels: The true labels, shape (batch_size,).\n",
    "        num_classes: The number of unique classes.\n",
    "        top_percent: The percentage of top predictions to consider for accuracy.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of top 1% accuracy per class.\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    num_samples = logits.size(0)\n",
    "\n",
    "    top_k = max(1, int(num_samples * top_percent / 100))  # Ensure at least 1\n",
    "\n",
    "    top_1_percent_accuracy = {i: 0 for i in range(num_classes)}\n",
    "    class_counts = {i: 0 for i in range(num_classes)}\n",
    "    for i in range(num_samples):\n",
    "        true_label = labels[i].item()\n",
    "        prob = probs[i].cpu().numpy()\n",
    "\n",
    "        top_indices = np.argsort(prob)[::-1]\n",
    "\n",
    "        if true_label == top_indices[0]:  # Check if the true class is the top-1 prediction\n",
    "            top_1_percent_accuracy[true_label] += 1\n",
    "\n",
    "        class_counts[true_label] += 1\n",
    "\n",
    "    for class_id in top_1_percent_accuracy:\n",
    "        if class_counts[class_id] > 0:  # Avoid division by zero\n",
    "            top_1_percent_accuracy[class_id] = (top_1_percent_accuracy[class_id] / class_counts[class_id]) * 100\n",
    "        else:\n",
    "            top_1_percent_accuracy[class_id] = 0\n",
    "\n",
    "    return top_1_percent_accuracy\n",
    "\n",
    "\n",
    "# Example usage after validation is complete\n",
    "def evaluate(model, val_loader, num_classes, classifier):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xi, _, labels) in enumerate(val_loader):\n",
    "            xi, labels = xi.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = model.backbone(xi)\n",
    "            logits = classifier(features)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "        # Concatenate logits and labels across all batches\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "        # Calculate Top 1% Accuracy per class\n",
    "        top_1_percent_acc_per_class = top_1_percent_accuracy_per_class(all_logits, all_labels, num_classes, top_percent=1)\n",
    "\n",
    "        print(\"\\nTop 1% Accuracy per class:\")\n",
    "        table = []\n",
    "        for class_id in range(num_classes):\n",
    "            class_name = class_names[class_id]  # Get the class name from the list\n",
    "            table.append([f\"Class {class_name}\", f\"{top_1_percent_acc_per_class[class_id]:.2f}%\"])\n",
    "\n",
    "        # Print the table using tabulate\n",
    "        headers = [\"Class\", \"Top 1% Accuracy\"]\n",
    "        print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "        # for class_id in range(num_classes):\n",
    "        #     print(f\"Class {class_id}: {top_1_percent_acc_per_class[class_id]:.2f}%\")\n",
    "\n",
    "\n",
    "# Assuming you have your validation loader and the number of classes\n",
    "num_classes = 13  # Set this according to your dataset\n",
    "\n",
    "\n",
    "# Initialize model components\n",
    "backbone = models.resnet50(pretrained=False)\n",
    "backbone.fc = torch.nn.Identity()\n",
    "\n",
    "checkpoint_path = './checkpoints/fold_10_checkpoint.pth'\n",
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cuda:0\")\n",
    "\n",
    "# Debug: print keys\n",
    "print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "\n",
    "\n",
    "simclr = SimCLR(backbone).to(\"cuda:0\")\n",
    "classifier = Classifier().to(\"cuda:0\")\n",
    "class_names = class_names\n",
    "\n",
    "simclr.load_state_dict(checkpoint['simclr'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "# Set to evaluation mode\n",
    "simclr.eval()\n",
    "classifier.eval()\n",
    "\n",
    "# Define transforms (similar to validation transforms)\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "roor_dir = './spectrograms'\n",
    "\n",
    "\n",
    "val_ds = UrbanSoundDataset(root_dir, [10], csv_file, transform)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "class_names = ['air_conditioner', 'ambulance', 'car_horn', 'children_playing',\n",
    "       'dog_bark', 'drilling', 'engine_idling', 'firetruck', 'gun_shot',\n",
    "       'jackhammer', 'police', 'street_music', 'traffic']\n",
    "evaluate(simclr, val_loader, num_classes, classifier = classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "985031ea-d337-4443-a574-b9b43d2669ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "985031ea-d337-4443-a574-b9b43d2669ea",
    "outputId": "f58c6011-3d1b-40cd-f71e-a76f9dc57a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
