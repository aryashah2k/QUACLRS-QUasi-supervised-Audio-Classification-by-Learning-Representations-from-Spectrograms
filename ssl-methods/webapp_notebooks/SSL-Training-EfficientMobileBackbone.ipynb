{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7d0774-b070-4b9c-a7ec-d42beec76056",
   "metadata": {},
   "source": [
    "# SimCLR Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec326472-5abf-4e6f-8303-e0e1eea36e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Before any torch imports\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 1\n",
    "num_folds = 1\n",
    "batch_size = 32\n",
    "feature_dim = 512\n",
    "num_classes = 13\n",
    "root_dir = './spectrograms/'\n",
    "csv_file = \"spectrograms_balanced_no_sirens.csv\"\n",
    "\n",
    "full_annotations = pd.read_csv(csv_file)\n",
    "class_names = sorted(full_annotations['class'].unique())\n",
    "\n",
    "# After loading full_annotations\n",
    "print(\"Unique classes:\", len(class_names))\n",
    "print(\"Class ID range:\", full_annotations['classID'].min(), full_annotations['classID'].max())\n",
    "assert full_annotations['classID'].between(0, num_classes-1).all(), \"Invalid class IDs detected\"\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, folds, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        \n",
    "        if isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        self.file_list = self.annotations[self.annotations['fold'].isin(folds)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.file_list.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, f'fold{row[\"fold\"]}', row['spec_file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = row['classID']\n",
    "        \n",
    "        if self.transform:\n",
    "            xi = self.transform(image)\n",
    "            xj = self.transform(image)\n",
    "            return xi, xj, label\n",
    "        return image, label\n",
    "\n",
    "# Model Components\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=512, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Loss Function\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "        \n",
    "        # Create labels: positives are the N off-diagonal elements\n",
    "        labels = torch.cat([\n",
    "            torch.arange(N, 2*N, device=z.device),\n",
    "            torch.arange(0, N, device=z.device)\n",
    "        ])\n",
    "        \n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
    "        sim = sim.masked_fill(mask, -1e9)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "# Training Function\n",
    "def train():\n",
    "    # Initialize backbone\n",
    "    backbone = models.resnet50(pretrained=True)\n",
    "    backbone.fc = torch.nn.Identity()\n",
    "    \n",
    "    # Data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # K-fold cross validation\n",
    "    for fold in range(1, num_folds+1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold}/{num_folds} {'='*20}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "        \n",
    "        # Data loaders\n",
    "        train_ds = UrbanSoundDataset(root_dir, [f for f in range(1,11) if f != fold], \n",
    "                                   csv_file, transform)\n",
    "        val_ds = UrbanSoundDataset(root_dir, [fold], csv_file, transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        # Model components\n",
    "        simclr = SimCLR(backbone).to(device)\n",
    "        classifier = Classifier().to(device)\n",
    "        optimizer = torch.optim.Adam(list(simclr.parameters()) + list(classifier.parameters()), lr=3e-4)\n",
    "        criterion = NTXentLoss()\n",
    "\n",
    "        # Metrics storage\n",
    "        train_losses, val_losses = [], []\n",
    "        val_accuracies, all_preds, all_labels = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training Phase\n",
    "            simclr.train()\n",
    "            classifier.train()\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch_idx, (xi, xj, labels) in enumerate(train_loader):\n",
    "                xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                zi, zj = simclr(xi), simclr(xj)\n",
    "                loss_contrastive = criterion(zi, zj)\n",
    "                \n",
    "                # Classification\n",
    "                features = simclr.backbone(xi)\n",
    "                logits = classifier(features)\n",
    "                loss_classification = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = loss_contrastive + 0.5 * loss_classification\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Progress tracking\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Print batch updates\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"  Batch {batch_idx + 1:03d}/{len(train_loader)} | \"\n",
    "                          f\"Loss: {loss.item():.4f} | \"\n",
    "                          f\"CLoss: {loss_contrastive.item():.4f} | \"\n",
    "                          f\"FLoss: {loss_classification.item():.4f} | \"\n",
    "                          f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "            # Epoch statistics\n",
    "            avg_train_loss = epoch_loss / batch_count\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"\\n  Training Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Last Batch Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Validation Phase\n",
    "            simclr.eval()\n",
    "            classifier.eval()\n",
    "            val_loss, correct, total = 0, 0, 0\n",
    "            \n",
    "            print(\"\\n  Validating...\")\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xi, _, labels) in enumerate(val_loader):\n",
    "                    xi, labels = xi.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    features = simclr.backbone(xi)\n",
    "                    logits = classifier(features)\n",
    "                    \n",
    "                    # Loss calculation\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Accuracy calculation\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                    \n",
    "                    # Store predictions\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                    # Validation batch updates\n",
    "                    if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                        acc = 100 * (preds == labels).sum().item() / labels.size(0)\n",
    "                        print(f\"    Val Batch {batch_idx + 1:03d}/{len(val_loader)} | \"\n",
    "                              f\"Loss: {loss.item():.4f} | \"\n",
    "                              f\"Batch Acc: {acc:.2f}%\")\n",
    "\n",
    "            # Validation statistics\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_acc = 100 * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f\"\\n  Validation Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
    "            print(f\"  Current Best Acc: {max(val_accuracies):.2f}%\")\n",
    "\n",
    "        # Fold Completion\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold} Completed ===\")\n",
    "        print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
    "        \n",
    "        # Generate plots\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title(f'Loss Curves - Fold {fold}')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.plot(val_accuracies)\n",
    "        plt.title(f'Validation Accuracy - Fold {fold}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fold_{fold}_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - Fold {fold}')\n",
    "        plt.savefig(f'fold_{fold}_confusion.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Classification Report\n",
    "        print(f\"\\nClassification Report - Fold {fold}:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "        \n",
    "        # Save model checkpoints\n",
    "        torch.save({\n",
    "            'simclr': simclr.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, f'fold_{fold}_checkpoint.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3dccb-8c73-45a1-b0ec-ebb96e69e284",
   "metadata": {},
   "source": [
    "# EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c896c2-f31e-4545-9825-c74ff426ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Debugging: sync CUDA errors\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Device and Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 3\n",
    "num_folds = 1\n",
    "batch_size = 32\n",
    "feature_dim = 512\n",
    "hidden_dim = 512\n",
    "projection_dim = 128\n",
    "num_classes = 13\n",
    "root_dir = './spectrograms/'\n",
    "csv_file = \"spectrograms_balanced_no_sirens.csv\"\n",
    "\n",
    "# Data/label validation\n",
    "full_annotations = pd.read_csv(csv_file)\n",
    "class_names = sorted(full_annotations['class'].unique())\n",
    "print(\"Unique classes:\", len(class_names))\n",
    "print(\"Class ID range:\", full_annotations['classID'].min(), full_annotations['classID'].max())\n",
    "assert full_annotations['classID'].between(0, num_classes-1).all(), \"Invalid class IDs detected\"\n",
    "\n",
    "# Dataset\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, folds, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        if isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        self.file_list = self.annotations[self.annotations['fold'].isin(folds)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.file_list.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, f'fold{row[\"fold\"]}', row['spec_file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = row['classID']\n",
    "        if self.transform:\n",
    "            xi = self.transform(image)\n",
    "            xj = self.transform(image)\n",
    "            return xi, xj, label\n",
    "        return image, label\n",
    "\n",
    "# Model components\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=feature_dim, hidden_dim=hidden_dim, output_dim=projection_dim):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=feature_dim, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Loss\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "        labels = torch.cat([\n",
    "            torch.arange(N, 2*N, device=z.device),\n",
    "            torch.arange(0, N, device=z.device)\n",
    "        ])\n",
    "        mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
    "        sim = sim.masked_fill(mask, -1e9)\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    from torchvision.models import ResNet50_Weights\n",
    "    backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    # Corrected backbone's fc layer to avoid dimension mismatch\n",
    "    backbone.fc = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(2048, 512)\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold}/{num_folds} {'='*20}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "        train_ds = UrbanSoundDataset(root_dir, [f for f in range(1,11) if f != fold], csv_file, transform)\n",
    "        val_ds = UrbanSoundDataset(root_dir, [fold], csv_file, transform)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        simclr = SimCLR(backbone).to(device)\n",
    "        classifier = Classifier().to(device)\n",
    "        optimizer = torch.optim.Adam(list(simclr.parameters()) + list(classifier.parameters()), lr=3e-4)\n",
    "        criterion = NTXentLoss()\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        val_accuracies, all_preds, all_labels = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            simclr.train()\n",
    "            classifier.train()\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_idx, (xi, xj, labels) in enumerate(train_loader):\n",
    "                xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
    "                zi, zj = simclr(xi), simclr(xj)\n",
    "                loss_contrastive = criterion(zi, zj)\n",
    "                features = simclr.backbone(xi)\n",
    "                logits = classifier(features)\n",
    "                loss_classification = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                loss = loss_contrastive + 0.5 * loss_classification\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"  Batch {batch_idx + 1:03d}/{len(train_loader)} | \"\n",
    "                          f\"Loss: {loss.item():.4f} | \"\n",
    "                          f\"CLoss: {loss_contrastive.item():.4f} | \"\n",
    "                          f\"FLoss: {loss_classification.item():.4f} | \"\n",
    "                          f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "            avg_train_loss = epoch_loss / batch_count\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"\\n  Training Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Last Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "            simclr.eval()\n",
    "            classifier.eval()\n",
    "            val_loss, correct, total = 0, 0, 0\n",
    "            print(\"\\n  Validating...\")\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xi, _, labels) in enumerate(val_loader):\n",
    "                    xi, labels = xi.to(device), labels.to(device)\n",
    "                    features = simclr.backbone(xi)\n",
    "                    logits = classifier(features)\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                        acc = 100 * (preds == labels).sum().item() / labels.size(0)\n",
    "                        print(f\"    Val Batch {batch_idx + 1:03d}/{len(val_loader)} | \"\n",
    "                              f\"Loss: {loss.item():.4f} | \"\n",
    "                              f\"Batch Acc: {acc:.2f}%\")\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_acc = 100 * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"\\n  Validation Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
    "            print(f\"  Current Best Acc: {max(val_accuracies):.2f}%\")\n",
    "\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold} Completed ===\")\n",
    "        print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title(f'Loss Curves - Fold {fold}')\n",
    "        plt.legend()\n",
    "        plt.subplot(122)\n",
    "        plt.plot(val_accuracies)\n",
    "        plt.title(f'Validation Accuracy - Fold {fold}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fold_{fold}_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - Fold {fold}')\n",
    "        plt.savefig(f'fold_{fold}_confusion.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Classification Report\n",
    "        print(f\"\\nClassification Report - Fold {fold}:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "        # Save model checkpoints\n",
    "        torch.save({\n",
    "            'simclr': simclr.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, f'fold_{fold}_checkpoint.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62387a-3145-4257-9413-abe9dae4c222",
   "metadata": {},
   "source": [
    "# MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2f07f-e2e0-4f1e-adf5-c11fb220ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Device and Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 3\n",
    "num_folds = 1\n",
    "batch_size = 128\n",
    "feature_dim = 512\n",
    "hidden_dim = 512\n",
    "projection_dim = 128\n",
    "num_classes = 13\n",
    "root_dir = './spectrograms/'\n",
    "csv_file = \"spectrograms_balanced_no_sirens.csv\"\n",
    "\n",
    "# Data validation\n",
    "full_annotations = pd.read_csv(csv_file)\n",
    "class_names = sorted(full_annotations['class'].unique())\n",
    "print(\"Unique classes:\", len(class_names))\n",
    "print(\"Class ID range:\", full_annotations['classID'].min(), full_annotations['classID'].max())\n",
    "assert full_annotations['classID'].between(0, num_classes-1).all(), \"Invalid class IDs detected\"\n",
    "\n",
    "# Dataset\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, folds, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        if isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        self.file_list = self.annotations[self.annotations['fold'].isin(folds)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.file_list.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, f'fold{row[\"fold\"]}', row['spec_file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = row['classID']\n",
    "        if self.transform:\n",
    "            xi = self.transform(image)\n",
    "            xj = self.transform(image)\n",
    "            return xi, xj, label\n",
    "        return image, label\n",
    "\n",
    "# Model components\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=feature_dim, hidden_dim=hidden_dim, output_dim=projection_dim):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=feature_dim, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Loss\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        sim = torch.mm(z, z.T) / self.temperature\n",
    "        labels = torch.cat([\n",
    "            torch.arange(N, 2*N, device=z.device),\n",
    "            torch.arange(0, N, device=z.device)\n",
    "        ])\n",
    "        mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
    "        sim = sim.masked_fill(mask, -1e9)\n",
    "        loss = self.criterion(sim, labels)\n",
    "        return loss\n",
    "\n",
    "# Training function with MobileNetV3\n",
    "def train():\n",
    "    from torchvision.models import MobileNet_V3_Small_Weights\n",
    "    \n",
    "    # 1. MobileNetV3-Small backbone\n",
    "    backbone = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "    \n",
    "    # 2. Modify classifier for feature extraction\n",
    "    backbone.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(576, feature_dim),  # Original input features: 576\n",
    "        torch.nn.Hardswish(inplace=True)\n",
    "    )\n",
    "\n",
    "    # 3. Input size for MobileNetV3\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold}/{num_folds} {'='*20}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "        \n",
    "        train_ds = UrbanSoundDataset(root_dir, [f for f in range(1,11) if f != fold], csv_file, transform)\n",
    "        val_ds = UrbanSoundDataset(root_dir, [fold], csv_file, transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        simclr = SimCLR(backbone).to(device)\n",
    "        classifier = Classifier().to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            list(simclr.parameters()) + list(classifier.parameters()), \n",
    "            lr=3e-4\n",
    "        )\n",
    "        criterion = NTXentLoss()\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        val_accuracies, all_preds, all_labels = [], [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            simclr.train()\n",
    "            classifier.train()\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_idx, (xi, xj, labels) in enumerate(train_loader):\n",
    "                xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                zi, zj = simclr(xi), simclr(xj)\n",
    "                loss_contrastive = criterion(zi, zj)\n",
    "                features = simclr.backbone(xi)\n",
    "                logits = classifier(features)\n",
    "                loss_classification = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                loss = loss_contrastive + 0.5 * loss_classification\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Metrics\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Progress reporting\n",
    "                if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"  Batch {batch_idx + 1:03d}/{len(train_loader)} | \"\n",
    "                          f\"Loss: {loss.item():.4f} | \"\n",
    "                          f\"CLoss: {loss_contrastive.item():.4f} | \"\n",
    "                          f\"FLoss: {loss_classification.item():.4f} | \"\n",
    "                          f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "            # Epoch summary\n",
    "            avg_train_loss = epoch_loss / batch_count\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"\\n  Training Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Last Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # Validation phase\n",
    "            simclr.eval()\n",
    "            classifier.eval()\n",
    "            val_loss, correct, total = 0, 0, 0\n",
    "            print(\"\\n  Validating...\")\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (xi, _, labels) in enumerate(val_loader):\n",
    "                    xi, labels = xi.to(device), labels.to(device)\n",
    "                    features = simclr.backbone(xi)\n",
    "                    logits = classifier(features)\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                    \n",
    "                    # Metrics\n",
    "                    val_loss += loss.item()\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                    # Validation progress\n",
    "                    if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                        acc = 100 * (preds == labels).sum().item() / labels.size(0)\n",
    "                        print(f\"    Val Batch {batch_idx + 1:03d}/{len(val_loader)} | \"\n",
    "                              f\"Loss: {loss.item():.4f} | \"\n",
    "                              f\"Batch Acc: {acc:.2f}%\")\n",
    "\n",
    "            # Validation summary\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_acc = 100 * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f\"\\n  Validation Summary | Epoch {epoch+1}\")\n",
    "            print(f\"  Avg Loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
    "            print(f\"  Current Best Acc: {max(val_accuracies):.2f}%\")\n",
    "\n",
    "        # Fold completion\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"=== Fold {fold} Completed ===\")\n",
    "        print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title(f'Loss Curves - Fold {fold}')\n",
    "        plt.legend()\n",
    "        plt.subplot(122)\n",
    "        plt.plot(val_accuracies)\n",
    "        plt.title(f'Validation Accuracy - Fold {fold}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fold_{fold}_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - Fold {fold}')\n",
    "        plt.savefig(f'fold_{fold}_confusion.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Classification report\n",
    "        print(f\"\\nClassification Report - Fold {fold}:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "        # Model checkpoint\n",
    "        torch.save({\n",
    "            'simclr': simclr.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, f'fold_{fold}_checkpoint.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7803d-34b3-4651-9252-110114d90b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b14b86-282e-4970-9549-1ac3a47986e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986303bb-d54a-4fa2-83c7-f940575f052e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c9d11-4ce6-41c4-8dee-5dded2647d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Model Components (updated for MobileNetV3)\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=512, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def process_audio(audio_path, sr=22050, duration=None, n_mels=224):\n",
    "    \"\"\"Process audio to spectrogram (matches training config)\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, \n",
    "                                      hop_length=512, n_mels=n_mels)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    temp_path = 'temp_spec.png'\n",
    "    plt.savefig(temp_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    img = Image.open(temp_path).convert('RGB')\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    return img, y, sr\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, checkpoint_path, class_names, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # MobileNetV3-Small backbone matching training\n",
    "        from torchvision.models import mobilenet_v3_small\n",
    "        backbone = mobilenet_v3_small(weights=None)\n",
    "        backbone.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(576, 512),  # Critical dimension match\n",
    "            torch.nn.Hardswish(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.simclr = SimCLR(backbone).to(self.device)\n",
    "        self.classifier = Classifier().to(self.device)\n",
    "        self.class_names = class_names\n",
    "\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.simclr.load_state_dict(checkpoint['simclr'], strict=True)  # Enforce exact match\n",
    "        self.classifier.load_state_dict(checkpoint['classifier'], strict=True)\n",
    "\n",
    "        self.simclr.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        # Input transforms matching training\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def predict(self, audio_path, verbose=True):\n",
    "        if verbose:\n",
    "            print(f\"Processing audio: {audio_path}\")\n",
    "        img, audio, sr = process_audio(audio_path)\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.simclr.backbone(img_tensor)\n",
    "            logits = self.classifier(features)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "        if verbose:\n",
    "            top_indices = np.argsort(probs)[::-1][:3]\n",
    "            print(\"\\nTop 3 predictions:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1}. {self.class_names[idx]}: {probs[idx]*100:.2f}%\")\n",
    "        return probs, audio, sr\n",
    "\n",
    "# Visualization function remains unchanged\n",
    "def visualize_prediction(probs, audio, sr, class_names):\n",
    "    print(\"Audio sample:\")\n",
    "    display(ipd.Audio(audio, rate=sr))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    indices = np.argsort(probs)[::-1]\n",
    "    plt.barh(range(len(class_names)), [probs[i] for i in indices])\n",
    "    plt.yticks(range(len(class_names)), [class_names[i] for i in indices])\n",
    "    plt.title(\"Class Probabilities\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class_names = ['air_conditioner', 'car_horn', 'children_playing', \n",
    "                  'dog_bark', 'drilling', 'engine_idling', \n",
    "                  'gun_shot', 'jackhammer', 'ambulance', 'firetruck', 'police', 'traffic', 'street_music']\n",
    "    classifier = AudioClassifier(\n",
    "        checkpoint_path='fold_1_checkpoint.pth',\n",
    "        class_names=class_names\n",
    "    )\n",
    "    audio_path = \"sound_601.wav\"\n",
    "    probs, audio, sr = classifier.predict(audio_path)\n",
    "    visualize_prediction(probs, audio, sr, class_names)\n",
    "    top_idx = np.argmax(probs)\n",
    "    print(f\"\\nFinal prediction: {class_names[top_idx]} with {probs[top_idx]*100:.2f}% confidence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985031ea-d337-4443-a574-b9b43d2669ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc95ae-e0ed-42bc-9878-2c09dbe338ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18a2cce-29a1-408c-98bb-602f3090a862",
   "metadata": {},
   "source": [
    "# Updated Inference For WebApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bcbbc-ea90-4083-ae63-65bc888a1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "import cv2\n",
    "\n",
    "# Model Components (updated for MobileNetV3)\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=512, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection = ProjectionHead()\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.projection(features)\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=512, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def process_audio(audio_path, sr=22050, duration=None, n_mels=224):\n",
    "    \"\"\"Process audio to spectrogram (matches training config)\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, \n",
    "                                      hop_length=512, n_mels=n_mels)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    temp_path = 'temp_spec.png'\n",
    "    plt.savefig(temp_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    img = Image.open(temp_path).convert('RGB')\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    return img, y, sr, S_dB  # Return S_dB for additional visualizations\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, checkpoint_path, class_names, device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # MobileNetV3-Small backbone matching training\n",
    "        from torchvision.models import mobilenet_v3_small\n",
    "        backbone = mobilenet_v3_small(weights=None)\n",
    "        \n",
    "        # Disable inplace operations for XAI compatibility\n",
    "        for module in backbone.modules():\n",
    "            if hasattr(module, 'inplace'):\n",
    "                module.inplace = False\n",
    "                \n",
    "        backbone.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(576, 512),\n",
    "            torch.nn.Hardswish(inplace=False)  # Explicitly disable inplace\n",
    "        )\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.simclr = SimCLR(backbone).to(self.device)\n",
    "        self.classifier = Classifier().to(self.device)\n",
    "        self.class_names = class_names\n",
    "\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.simclr.load_state_dict(checkpoint['simclr'], strict=False)\n",
    "        self.classifier.load_state_dict(checkpoint['classifier'], strict=False)\n",
    "\n",
    "        self.simclr.eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        # Input transforms matching training\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # Set up hooks for GradCAM\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self.setup_gradcam_hooks()\n",
    "        \n",
    "    def setup_gradcam_hooks(self):\n",
    "        \"\"\"Set up hooks for GradCAM visualization\"\"\"\n",
    "        # Target the last convolutional layer in MobileNetV3\n",
    "        target_layer = self.backbone.features[-1][-1]\n",
    "        \n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach().clone()\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach().clone()\n",
    "            \n",
    "        target_layer.register_forward_hook(forward_hook)\n",
    "        target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    def generate_gradcam(self, img_tensor, target_class=None):\n",
    "        \"\"\"Generate GradCAM visualization for the input image\"\"\"\n",
    "        # Forward pass\n",
    "        self.simclr.zero_grad()\n",
    "        features = self.simclr.backbone(img_tensor)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # If target class is not specified, use the predicted class\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Backward pass to get gradients\n",
    "        logits[0, target_class].backward(retain_graph=True)\n",
    "        \n",
    "        # Compute weights using gradients\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[2, 3], keepdim=True)\n",
    "        \n",
    "        # Weight the activations by the gradients\n",
    "        heatmap = torch.sum(pooled_gradients * self.activations, dim=1).squeeze()\n",
    "        heatmap = torch.relu(heatmap)  # ReLU to only show positive contributions\n",
    "        \n",
    "        # Normalize heatmap\n",
    "        if torch.max(heatmap) > 0:\n",
    "            heatmap /= torch.max(heatmap)\n",
    "            \n",
    "        return heatmap.cpu().numpy(), target_class\n",
    "    \n",
    "    def compute_integrated_gradients(self, img_tensor, target_class=None, steps=50):\n",
    "        \"\"\"Compute Integrated Gradients for the input image\"\"\"\n",
    "        # Create baseline (black image)\n",
    "        baseline = torch.zeros_like(img_tensor).to(self.device)\n",
    "        \n",
    "        # Forward pass to get prediction\n",
    "        self.simclr.zero_grad()\n",
    "        features = self.simclr.backbone(img_tensor)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # If target class is not specified, use the predicted class\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Compute integrated gradients\n",
    "        integrated_grads = torch.zeros_like(img_tensor).to(self.device)\n",
    "        \n",
    "        for step in range(1, steps + 1):\n",
    "            # Interpolate between baseline and input\n",
    "            interpolated = baseline + (img_tensor - baseline) * (step / steps)\n",
    "            interpolated.requires_grad = True\n",
    "            \n",
    "            # Forward pass\n",
    "            features = self.simclr.backbone(interpolated)\n",
    "            logits = self.classifier(features)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.simclr.zero_grad()\n",
    "            logits[0, target_class].backward(retain_graph=True)\n",
    "            \n",
    "            # Get gradients\n",
    "            gradients = interpolated.grad.detach()\n",
    "            \n",
    "            # Add to integrated gradients\n",
    "            integrated_grads += gradients / steps\n",
    "        \n",
    "        # Multiply by input - baseline\n",
    "        integrated_grads *= (img_tensor - baseline)\n",
    "        \n",
    "        # Sum across color channels for visualization\n",
    "        attribution_map = torch.sum(torch.abs(integrated_grads), dim=1).squeeze()\n",
    "        \n",
    "        # Normalize for visualization\n",
    "        if torch.max(attribution_map) > 0:\n",
    "            attribution_map /= torch.max(attribution_map)\n",
    "            \n",
    "        return attribution_map.cpu().numpy(), target_class\n",
    "    \n",
    "    def predict(self, audio_path, verbose=True, visualize_xai=True):\n",
    "        \"\"\"Predict class for audio file with XAI visualizations\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Processing audio: {audio_path}\")\n",
    "            \n",
    "        # Process audio to spectrogram\n",
    "        img, audio, sr, S_dB = process_audio(audio_path)\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            features = self.simclr.backbone(img_tensor)\n",
    "            logits = self.classifier(features)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Print top predictions\n",
    "        if verbose:\n",
    "            top_indices = np.argsort(probs)[::-1][:3]\n",
    "            print(\"\\nTop 3 predictions:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1}. {self.class_names[idx]}: {probs[idx]*100:.2f}%\")\n",
    "        \n",
    "        # Generate XAI visualizations\n",
    "        if visualize_xai:\n",
    "            self.visualize_xai(img, img_tensor, probs, S_dB)\n",
    "            \n",
    "        return probs, audio, sr\n",
    "\n",
    "    def visualize_xai(self, original_img, img_tensor, probs, spectrogram_db):\n",
    "        \"\"\"Generate and display multiple XAI visualizations\"\"\"\n",
    "        # Get top predicted class\n",
    "        top_class = np.argmax(probs)\n",
    "        \n",
    "        # 1. GradCAM visualization\n",
    "        gradcam_heatmap, _ = self.generate_gradcam(img_tensor, top_class)\n",
    "        \n",
    "        # 2. Integrated Gradients visualization\n",
    "        ig_heatmap, _ = self.compute_integrated_gradients(img_tensor, top_class)\n",
    "        \n",
    "        # Resize heatmaps to match original image\n",
    "        original_size = (original_img.width, original_img.height)\n",
    "        gradcam_heatmap_resized = cv2.resize(gradcam_heatmap, original_size)\n",
    "        ig_heatmap_resized = cv2.resize(ig_heatmap, original_size)\n",
    "        \n",
    "        # Convert heatmaps to RGB for visualization\n",
    "        gradcam_heatmap_rgb = cv2.applyColorMap(np.uint8(255 * gradcam_heatmap_resized), cv2.COLORMAP_JET)\n",
    "        gradcam_heatmap_rgb = cv2.cvtColor(gradcam_heatmap_rgb, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create overlay images\n",
    "        original_img_np = np.array(original_img)\n",
    "        gradcam_overlay = cv2.addWeighted(original_img_np, 0.6, gradcam_heatmap_rgb, 0.4, 0)\n",
    "        \n",
    "        # Create figure for visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Original spectrogram\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(\"Original Spectrogram\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 2. GradCAM heatmap\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(gradcam_heatmap_resized, cmap='jet')\n",
    "        plt.title(f\"GradCAM: {self.class_names[top_class]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 3. GradCAM overlay\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(gradcam_overlay)\n",
    "        plt.title(\"GradCAM Overlay\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 4. Integrated Gradients heatmap\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.imshow(ig_heatmap_resized, cmap='viridis')\n",
    "        plt.title(\"Integrated Gradients\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 5. Frequency-Time Analysis\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.imshow(spectrogram_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(\"Spectrogram (dB)\")\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Mel Frequency Bands')\n",
    "        \n",
    "        # 6. Class Probability Distribution\n",
    "        plt.subplot(2, 3, 6)\n",
    "        top_indices = np.argsort(probs)[::-1][:5]  # Show top 5 classes\n",
    "        plt.barh([self.class_names[i] for i in top_indices], \n",
    "                 [probs[i] for i in top_indices])\n",
    "        plt.title(\"Top 5 Class Probabilities\")\n",
    "        plt.xlabel(\"Probability\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualization function remains unchanged\n",
    "def visualize_prediction(probs, audio, sr, class_names):\n",
    "    print(\"Audio sample:\")\n",
    "    display(ipd.Audio(audio, rate=sr))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    indices = np.argsort(probs)[::-1]\n",
    "    plt.barh(range(len(class_names)), [probs[i] for i in indices])\n",
    "    plt.yticks(range(len(class_names)), [class_names[i] for i in indices])\n",
    "    plt.title(\"Class Probabilities\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class_names = ['air_conditioner', 'car_horn', 'children_playing', \n",
    "                  'dog_bark', 'drilling', 'engine_idling', \n",
    "                  'gun_shot', 'jackhammer', 'ambulance', 'firetruck', 'police', 'traffic', 'street_music']\n",
    "    classifier = AudioClassifier(\n",
    "        checkpoint_path='fold_1_checkpoint.pth',\n",
    "        class_names=class_names\n",
    "    )\n",
    "    audio_path = \"./TestAudioFiles/starwars.wav\"\n",
    "    probs, audio, sr = classifier.predict(audio_path)\n",
    "    visualize_prediction(probs, audio, sr, class_names)\n",
    "    top_idx = np.argmax(probs)\n",
    "    print(f\"\\nFinal prediction: {class_names[top_idx]} with {probs[top_idx]*100:.2f}% confidence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a02a92-56b0-4b1b-8722-3e352c4788f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
